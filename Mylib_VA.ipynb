{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SzChDO4B75nj",
        "s3vKn5GTWKyN",
        "yVuX-30tm-1j",
        "-jvWNP7NaMTE"
      ],
      "authorship_tag": "ABX9TyNXVpHuD0FWZTxBwZAbqdO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93b99bd93ecc47e9b45021436b6af370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ColorPickerModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ColorPickerModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ColorPickerView",
            "concise": false,
            "description": "Pick a color",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c92656fcc6844adfb90cbed51d650e96",
            "style": "IPY_MODEL_6659ee00fe4f4811b75c4cddd62ca7fc",
            "value": "#942eee"
          }
        },
        "c92656fcc6844adfb90cbed51d650e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6659ee00fe4f4811b75c4cddd62ca7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent-arese/Mylib/blob/main/Mylib_VA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5wBe75QkWMl"
      },
      "source": [
        "# <font color=\"00CED1\">  Config Env de travail </font>   <a id=\"EnvTravail\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version & Last update\n",
        "Version = 2\n",
        "MAJ = \"25/10/22 19:49\"\n",
        "print(f\"Mylib_VA.ipynb V{Version}  MAJ : {MAJ}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zmP_uTtVZy1",
        "outputId": "45fa26cf-80c3-418a-baac-02aa1870b824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mylib_VA.ipynb V2  MAJ : 25/10/22 19:49\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqvn1P4nkeub"
      },
      "source": [
        "### Collab Config  :  pip install , Locale , Classes ...."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # https://docs.python.org/fr/3/library/os.html\n",
        "path = \"/content/Exports/\"\n",
        "# os.mkdir(path)\n",
        "os.makedirs(path,exist_ok=True)\n",
        "print(f\"Creation repertoire{path}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uLU2NNcmJli",
        "outputId": "28de138d-3118-4e7b-c92c-bb77d5690f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creation repertoire/content/Exports/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-b8NdhLs6OM"
      },
      "outputs": [],
      "source": [
        "#_____________________________________________________________________________\n",
        "# Curent Enviroment  PIP  informations\n",
        "#_____________________________________________________________________________\n",
        "\n",
        "\n",
        "# Python Verions  Check \n",
        "# !python --version  # checks version from command line\n",
        "# !sudo apt-get update -y\n",
        "# !sudo apt-get upgrade -y\n",
        "\n",
        "# !sudo update-alternatives --config python3\n",
        "# #after running, enter the row number of the python version you want.\n",
        "# !python --version  # checks version from command line\n",
        "\n",
        "\n",
        "# Recherche des pip utlisés \n",
        "# ! pip list -v\n",
        "# ! pip list -v | grep [Pp]an \n",
        "# ! pip list -v | grep [Pp]lot\n",
        "# ! pip list -v | grep [Ss]ea\n",
        "# ! pip list -v | grep [Bb]l\n",
        "# ! pip list -v | grep [Aa]ut\n",
        "\n",
        "# ! pip list -v | grep pingouin\n",
        "\n",
        "# Versions\n",
        "# %watermark -v -p numpy,pandas,datetime,scipy,scipy.stats,statsmodels,statsmodels.api,statsmodels.formula.api,pingouin,matplotlib,seaborn,plotly,black,jupyterlab\n",
        "## %load_ext watermark\n",
        "\n",
        "\n",
        "#_____________________________________________________________________________\n",
        "# Add PIP to current Enviroment\n",
        "#_____________________________________________________________________________\n",
        "\n",
        "# librairie pour combiner Pyhton & SQL  \n",
        "#!pip install -U fugue[duckdb,sql]  # https://towardsdatascience.com/fugue-and-duckdb-fast-sql-code-in-python-e2e2dfc0f8eb\n",
        "# $ pip install plotly==5.10.0\n",
        "\n",
        "# librairie data science \n",
        "# ! pip install --upgrade pandas  # https://pandas.pydata.org/docs/index.html\n",
        "\n",
        "# ! pip install pingouin  # https://pingouin-stats.org/index.html\n",
        "\n",
        "\n",
        "# librairie data visualalisation \n",
        "# !pip install pandas-bokeh  # https://docs.bokeh.org/en/latest/\n",
        "\n",
        "\n",
        "#  TO DEL    !pip install -U dash \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install pinguin if it is not installed\n",
        "# ! pip list -v | grep pingouin\n",
        "\n",
        "try:\n",
        "    import pingouin\n",
        "    print(\"pinguoin is installed\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"module 'pinguoing' is not installed\")\n",
        "    ! pip install pingouin  # https://pingouin-stats.org/index.html\n",
        "    # install(\"mutagen\") # the install function from the question\n",
        "\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K9cS8RKdFE3U",
        "outputId": "d4c57e60-7209-44a5-b1a1-a19169ffdd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module 'pinguoing' is not installed\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pingouin\n",
            "  Downloading pingouin-0.5.2.tar.gz (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from pingouin) (3.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.11.2)\n",
            "Collecting statsmodels>=0.13\n",
            "  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.1.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.0.2)\n",
            "Collecting pandas_flavor>=0.2.0\n",
            "  Downloading pandas_flavor-0.3.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting outdated\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.8.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pingouin) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pingouin) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pingouin) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pingouin) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.2->pingouin) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->pingouin) (2022.5)\n",
            "Collecting pandas_flavor>=0.2.0\n",
            "  Downloading pandas_flavor-0.2.0-py2.py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (from pandas_flavor>=0.2.0->pingouin) (0.20.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.2->pingouin) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0->pingouin) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0->pingouin) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.13->pingouin) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.13->pingouin) (21.3)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.7/dist-packages (from outdated->pingouin) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated->pingouin) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->outdated->pingouin) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated->pingouin) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated->pingouin) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated->pingouin) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from xarray->pandas_flavor>=0.2.0->pingouin) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->xarray->pandas_flavor>=0.2.0->pingouin) (3.9.0)\n",
            "Building wheels for collected packages: pingouin, littleutils\n",
            "  Building wheel for pingouin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pingouin: filename=pingouin-0.5.2-py3-none-any.whl size=196206 sha256=ebf51a9a94fb2eabedbbe32b0e402c902f33d584748a3b34c3e0894e5f88956a\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/5a/63/a6d32fc26fa462c731f65480bfb98ff7bd39b8ebcb4bc6c2fe\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=be0730b092141bb5780dc9ab3302197f38b675f2376ca427afab2d9dbe5a71f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built pingouin littleutils\n",
            "Installing collected packages: littleutils, statsmodels, pandas-flavor, outdated, pingouin\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.12.2\n",
            "    Uninstalling statsmodels-0.12.2:\n",
            "      Successfully uninstalled statsmodels-0.12.2\n",
            "Successfully installed littleutils-0.2.2 outdated-0.2.2 pandas-flavor-0.2.0 pingouin-0.5.2 statsmodels-0.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "statsmodels"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 331 kB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8fpcnGUInir",
        "outputId": "55732607-52e4-4b6d-ddcc-3ea07a78aa0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m {pcolors.OK}\u001b[0m\n",
            "\u001b[93m{colors.WARNING}\u001b[0m\n",
            "\u001b[91m{pcolors.FAIL}\u001b[0m\n",
            "\u001b[94mpcolors.BLUE !\u001b[0m\n",
            "\u001b[104m pcolors.BLUEBG.\u001b[0m\n",
            " OK WARNING  FAIL  RESET  BLUE BLUEBG GREEN RED JAUNE\n"
          ]
        }
      ],
      "source": [
        "#_____________________________________________________________________________\n",
        "# Python print Color\n",
        "#_____________________________________________________________________________\n",
        "class pcolors:\n",
        "    OK = '\\033[92m' #GREEN\n",
        "    WARNING = '\\033[93m' #YELLOW\n",
        "    FAIL = '\\033[91m' #RED\n",
        "    RESET = '\\033[0m' #RESET COLOR\n",
        "    BLUE = '\\033[94m'\n",
        "    BLUEBG = '\\033[104m'\n",
        "    GREEN = '\\033[92m' \n",
        "    RED = '\\033[91m'\n",
        "    JAUNE = '\\033[93m'\n",
        "#Source : https://www.delftstack.com/fr/howto/python/python-print-colored-text/  AINSI colors\n",
        "\n",
        "#Exemple formatage code \n",
        "## Comment/Uncoment To Preview/hide print Color\n",
        "print(pcolors.OK + \" {pcolors.OK}\" + pcolors.RESET)\n",
        "print(pcolors.WARNING + \"{colors.WARNING}\" + pcolors.RESET)\n",
        "print(pcolors.FAIL + \"{pcolors.FAIL}\" + pcolors.RESET)\n",
        "print(f\"{pcolors.BLUE}pcolors.BLUE !{pcolors.RESET}\")\n",
        "print(f\"{pcolors.BLUEBG} pcolors.BLUEBG.{pcolors.RESET}\")\n",
        "\n",
        "print(\" OK WARNING  FAIL  RESET  BLUE BLUEBG GREEN RED JAUNE\")\n",
        "\n",
        "# https://html-color-codes.info/Codes-couleur-HTML/\n",
        "\n",
        "#-----------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMsAErRLI4dk"
      },
      "outputs": [],
      "source": [
        "#_____________________________________________________________________________\n",
        "# # Install language Fr \n",
        "#_____________________________________________________________________________\n",
        "\n",
        "# import os\n",
        "# ## --------------------------- ### \n",
        "# !/usr/share/locales/install-language-pack fr \n",
        "# !dpkg-reconfigure locales\n",
        "# # Restart Python process to pick up the new locales\n",
        "# os.kill(os.getpid(), 9)  \n",
        "# ## --------------------------- ###  ## Uncoment ONLY on the FIRST Collab session Run to import Fr lLocale confi and  clear error message \n",
        "\n",
        "# # !locale -a # Show list system availible local\n",
        "# # import datetime\n",
        "# # import locale\n",
        "\n",
        "# print(\"Curent sytem Locale : \",locale.getlocale())   # print curent locale (pervious)\n",
        "# locale.setlocale(locale.LC_ALL, \"fr_FR.utf8\")   ; print(\"Updated \",locale.getlocale()) \n",
        "# x = datetime.datetime.now(); x.strftime(\"%A %d  %B  %Y  %H:%M:%S %p\")  #test \n",
        "# print(\"TEST locale Dispaly :\",x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5h_4Ydd4Ml2"
      },
      "outputs": [],
      "source": [
        "# Pep8 Test \n",
        "\n",
        "# import os, sys, subprocess\n",
        "# if \"google.colab\" in sys.modules:\n",
        "#     cmd = \"pip install --upgrade watermark blackcellmagic\"\n",
        "#     process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
        "\n",
        "# # style Pep 8\n",
        "# %load_ext blackcellmagic\n",
        "\n",
        "# # Pour utiliser %%black\n",
        "\n",
        "# # Source  :  https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/04/style.ipynb#scrollTo=CR3HLFuW2VwJ\n",
        "#             https://stackoverflow.com/questions/63076002/code-formatter-like-nb-black-for-google-colab  or https://www.anycodings.com/1questions/1035115/code-formatter-like-nbblack-for-google-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99P3bdUgkwxz"
      },
      "source": [
        "### Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtW9d-qjkHZX"
      },
      "outputs": [],
      "source": [
        "####### librairies Pythons    #######                          \n",
        "import numpy as np\n",
        "import pandas as pd  #https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
        "from datetime import datetime # avoid future warning  use datetime instead of pd.datetime \n",
        "import os # https://docs.python.org/fr/3/library/os.html \n",
        "\n",
        "## Pandas Option setting \n",
        "from pandas.core.groupby.generic import DataFrameGroupBy\n",
        "pd.set_option('display.max_columns',None)  #pd.set_option('max_columns', 10) limiter le nbr de colonnes visualisé à 10 /none (=> scrollbar)\n",
        "pd.set_option('display.max_rows', 50)   # Reglage VA : 50 \n",
        "pd.set_option('display.max_colwidth', None)  # None ou -1 or  199\n",
        "pd.set_option('display.colheader_justify','left') #'left'/'right'\n",
        "pd.set_option(\"display.date_dayfirst\",True) # display.date_dayfirst / display.date_yearfirst\n",
        "pd.set_option(\"display.date_yearfirst\",False) \n",
        "pd.set_option(\"display.html.table_schema\", True) # test ?\n",
        "# pd.set_option(\"mode.sim_interactive\", True)  # Mode debogage ! ??????????\n",
        "# https://pandas.pydata.org/docs/user_guide/options.html\n",
        "# https://runebook.dev/fr/docs/pandas/user_guide/options\n",
        "# https://pandas.pydata.org/docs/user_guide/options.html\n",
        "\n",
        "import ipywidgets as widgets   #https://ipywidgets.readthedocs.io/en/8.0.2/examples/Widget%20Basics.html\n",
        "\n",
        "####### Statitics    #######\n",
        "import scipy.stats as stats   # https://github.com/scipy/scipy\n",
        "\n",
        "import statsmodels.api as sm   #https://www.statsmodels.org/stable/index.html  #https://www.statsmodels.org/dev/user-guide.html\n",
        "import statsmodels.formula.api as smf #https://www.statsmodels.org/stable/index.html\n",
        "\n",
        "import pingouin as pg    #https://pingouin-stats.org/api.html\n",
        "\n",
        "from patsy import dmatrices # https://patsy.readthedocs.io/en/latest/\n",
        "\n",
        "####### Data visualisation  #######\n",
        "import matplotlib.pyplot as plt    #https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf\n",
        "import seaborn as sns     #https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Seaborn_Cheat_Sheet.pdf\n",
        "import plotly.express as px     #https://plotly.com/python/ \n",
        "import plotly.graph_objects as go  #https://plotly.github.io/plotly.py-docs/search.html?q=hist&check_keywords=yes&area=default \n",
        "# import pandas_bokeh   # https://docs.bokeh.org/en/latest/docs/gallery.html\n",
        "\n",
        "#######  SQL  #######\n",
        "#import fugue_duckdb\n",
        "#from fugue_notebook import setup ;setup()\n",
        "\n",
        "\n",
        "####### Machine Learning  #######\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3tToYclGob"
      },
      "source": [
        "## Fonctions Perso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Aide():\n",
        " \"\"\"\n",
        " Aide fonctions Mylib \n",
        " \"\"\"\n",
        "\n",
        "  # Widget Tab Configuration\n",
        "\n",
        " import ipywidgets as widgets #https://ipywidgets.readthedocs.io/en/stable/ \n",
        " tab0 = widgets.Output()\n",
        " tab1 = widgets.Output()\n",
        " tab2 = widgets.Output()\n",
        " tab3 = widgets.Output()\n",
        " tab4 = widgets.Output()\n",
        " tab5 = widgets.Output()\n",
        " tab6 = widgets.Output()\n",
        " \n",
        " tab = widgets.Tab(children = [tab0,tab1,tab2,tab3,tab4,tab5,tab6])\n",
        " tab.set_title(0, 'Data Exploration')\n",
        " tab.set_title(1, 'Manip df')\n",
        " tab.set_title(2, 'Statistique')\n",
        " tab.set_title(3, 'Machine Learning')\n",
        " tab.set_title(4, 'Data Viz')\n",
        " tab.set_title(5, '....')\n",
        " tab.set_title(6, 'CheatSheets')\n",
        " display(tab)\n",
        "\n",
        " with tab0:\n",
        "  print(f\"{pcolors.OK}Data Exploration{pcolors.RESET}\")\n",
        "  print(f\"\\nINFOS(df) {pcolors.BLUE}&{pcolors.RESET} INFOSdf(df)\\n\")\n",
        "  print(\"Explore(df)\")\n",
        "  print(\"  PlotNaN(df)\")\n",
        "  print(\"  ExploreQuantitative(df)\")\n",
        "  print(\"  ExploreQualitative(df)\")\n",
        "  print(\"\\nOutlierBoxplot(df,col)\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nGraphiques plotly distribution {pcolors.RESET}\")\n",
        "  print(f\"DistributionHist(df,col {pcolors.BLUE}&{pcolors.RESET} DistributionBar(df,col)\")\n",
        "  print(\"AnalyseBivarHist(df,critereA,critereB)\")\n",
        "  \n",
        "  print(f\"{pcolors.WARNING}\\nGraphiques plotly Analyse bi-variÃƒÂ©e {pcolors.RESET}\")\n",
        "  print(\"AnalyseBivarOLS(df,critereX,critereY,critereC)    CritereC : Color  \" ) \n",
        "  print(\"AnalyseBivarHist(df,critereA,critereB)\")\n",
        "  print(\"AnalyseBivarOLS(df,critereX,critereY,critereC)    CritereC : Color\"  )\n",
        "  \n",
        " with tab1:\n",
        "  print(f\"{pcolors.OK}Manip DataFrame{pcolors.RESET}\")\n",
        "  print(\"\\nColMove(DataFrame,ColName,ColIndex)\")\n",
        "  print(\"get_df_name(df)                 get_np_array_name(obj, namespace) => get_np_array_name(my_numpy, globals()) \")\n",
        "  print(\"Rename DataFrame Columns with Pythonic Names  : RenameDfColPythonic(df)\")\n",
        "  print(\"FlatIndex(df)\")\n",
        "  print(f\"reduce_mem_usage(df) {pcolors.BLUE}&{pcolors.RESET} Change dtype Category to Object : dtypeCat2Obj(df)\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nMerge{pcolors.RESET}\")\n",
        "  print(\"MergeAudit(df)   Note gobal df names :  Merge_right_only Merge_left_only  Merge_both \")\n",
        "\n",
        "  print(\"Differences entre deux data frame  : get_different_rows(source_df, new_df) \")\n",
        "  print(\"SearchListe(df,liste):Recherche une liste dans un DataFrame et renvoie Df avec les Valeurs pour masque SearchListe(dx,[2,4,7] \")\n",
        "  print(\"SearchNaN(DataFrame,Option):    Option :   1: Total nombre de NaN par colonne   2: Afficher les lignes ayant au moins 1 NaN    3: Nomnbre de lignes ayant 1 NaN\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nDates {pcolors.RESET}\")\n",
        "  print(\"Age(df,birthdateCol)\")\n",
        "  print(\"DateTimeDetail(df,DateColSTR) )\")\n",
        "  print(\"DateTime2WeekFR(df,DateColSTR)\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nExports {pcolors.RESET}\")\n",
        "  print(f\"Export plotly graph in html : ExportPlotly(fig,file_name){pcolors.BLUE} & {pcolors.RESET} ExportPlotly_dfPATH(df,col,fig,file_name) {pcolors.BLUE} & {pcolors.RESET}  ExportPlotly_df_Name_StringPATH(df_Name_String,col_Name_String,fig,file_name)\")\n",
        "  print(f\"Export DataFrame Excel/CSV  : ExportDfColab(df,file_name  Export Format Excel : .xls & .xlsx   {pcolors.BLUE}or{pcolors.RESET}  Format CSV .csv (utF8 - separateur ;)  If required include path in the FileName '../data/my_new_file.csv' \\n\")\n",
        "\n",
        " with tab2:\n",
        "  print(f\"{pcolors.OK}Statistiques{pcolors.RESET}\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nStatistiques descriptive{pcolors.RESET}\")\n",
        "  print(\"QuartileIndicatorCol(df,col)\")\n",
        "  print(\"MoyPonderee(df, Colvalues, Colweights)\")\n",
        "  \n",
        "  \n",
        "  print(f\"{pcolors.WARNING}\\nIndicateurs Statistiques{pcolors.RESET}\")\n",
        "  print(\"PercentCumul(df,Col)\")\n",
        "  print(f\"PercentCumul(df,Col) {pcolors.BLUE}&{pcolors.RESET} PercentCumulRoundX(df,Col,RoundDecimalNumber {pcolors.BLUE}&{pcolors.RESET} PercentCumulNoRound(df,Col)\")\n",
        "  print(\"Pareto(data,percent)\")\n",
        "  print(\"mesure concentration : Indice Gini Gini(array)  & CourbeLorentz(data) (df.Colonne.to_numpy()  # convertir pandas.core.series.Series' to numpy.ndarray )\")\n",
        "\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nTest Statistiques{pcolors.RESET}\")\n",
        "  print(\"TestNorm(data,ÃŽÂ±)  & TestNormGlobal(data,ÃŽÂ±)\")\n",
        "  print(f\"\\n{pcolors.BLUE}Regression linÃƒÂ©aires Pearson & Spearman (correlation entre variables quantitatives) {pcolors.RESET}\")\n",
        "  print(\"Coef correalation Spearson & Spearman  : CoefPS(df,dataX,dataY,ÃŽÂ±)  (df: dataframe  - dataX : df.col or array \")\n",
        "\n",
        "  print(f\"\\n{pcolors.BLUE}Test Correlation Var Qualitative: Test de ÃÂ‡Ã‚Â² (khi2 - chi2){pcolors.RESET}\")\n",
        "  print(\"  TableauContingence(df,Col_I,Col_II) \")\n",
        "  print(\"  Khi2Test(df,Col_I,Col_II,ÃŽÂ±)\")\n",
        "\n",
        " with tab3:\n",
        "  print(f\"{pcolors.OK}Machine learning{pcolors.RESET}\")\n",
        "  print(\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\")\n",
        "  print(\"https://scikit-learn.org/stable/user_guide.html\")\n",
        "  print(\"https://scikit-learn.org/stable/modules/classes\")\n",
        "  \n",
        "  print(f\"\\n{pcolors.WARNING}ACP (Analyse Composantes Principales) fonctions{pcolors.RESET}\")\n",
        "  print(\"ACP_global(df,features_list,ColorCol,FC1x ,FC1y,FC2x ,FC2y,standardization=True)\")\n",
        "  print(\"    ACP_ML(df,features_list,standardization=True)\")\n",
        "  print(\"    BatonsBrises() ACP_EboulisPlot()\")\n",
        "  print(\"    ACP_CercleCorrelationsPlotVtest(PCx,PCy) ACP_projections(ColorCol,PCx,PCy) ACP_MatrixPlot_Projections(ColorCol,width,height,Nombre_Max_PC_graphique,)\")\n",
        "  print(\"    ACP_heatmapPC() FeatureCorrelationMatrix(df,Selected_Method_Name)\")\n",
        "  \n",
        "  print(f\"\\n{pcolors.WARNING}Cluster Kmean {pcolors.RESET}\")\n",
        "  print(\"Scaling_ML(df,features_list,standardization=True)\")\n",
        "  print(\"ClusterKmeanCoudeplot(nbr_Max_Cluster_envisage_Int)\")\n",
        "  print(\"KmeanCluster(nbr_de_Cluster)\")\n",
        "\n",
        "  print(f\"\\n{pcolors.WARNING}Cluster Hierachique {pcolors.RESET}\")\n",
        "  print(\"https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy\")\n",
        "  print(\"HierarchyClusterScipy(df,features_list,standardization=True):\")\n",
        "\n",
        " with tab4:\n",
        "  print(f\"{pcolors.OK}Data Viz{pcolors.RESET}\")\n",
        "  print(\"Galerie Exemple Data Viz : https://python-graph-gallery.com/\")\n",
        "  print()\n",
        "  print(\"https://plotly.com/python/\")\n",
        "  print(\"      Color Choice  color_continuous_scale='earth',  # _r : reverse color    viridis deep bluered inferno magma rainbow\")\n",
        "  print(\"      https://plotly.com/python/colorscales/\")\n",
        "\n",
        "  print()\n",
        "  print(\"Hist : https://plotly.com/python/histograms/  https://plotly.github.io/plotly.py-docs/generated/plotly.express.histogram.html  displot : https://plotly.com/python/distplot/\")\n",
        "  print(\"Bar : https://plotly.com/python/bar-charts/  https://plotly.github.io/plotly.py-docs/generated/plotly.express.bar.html \")\n",
        " \n",
        "  print()\n",
        "  print(\"Pie chart : https://plotly.com/python/pie-charts/   https://plotly.com/python-api-reference/generated/plotly.express.pie.html\")  \n",
        "  print(\"Boxplot : https://plotly.com/python/box-plots/  https://plotly.github.io/plotly.py-docs/generated/plotly.express.box.html\")\n",
        "  print(\"Violin :  https://plotly.com/python/violin/   https://plotly.com/python-api-reference/generated/plotly.express.violin.html       https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Violin.html \")\n",
        "  \n",
        "  print()\n",
        "  print(\"Scatter plot  : https://plotly.com/python/bubble-charts/ https://plotly.com/python-api-reference/generated/plotly.express.scatter.html https://plotly.com/python/error-bars/\")\n",
        "  print(\"3D Scatter    : https://plotly.com/python/3d-scatter-plots/ https://www.google.com/search?q=px.scatter_3d&rlz=1C1CHBD_frFR933FR933&oq=px.scatter_3d&aqs=chrome..69i57.1024j0j7&sourceid=chrome&ie=UTF-8\")\n",
        "  \n",
        "  print()\n",
        "  print(\"line : https://plotly.com/python/line-charts/  https://plotly.com/python-api-reference/generated/plotly.express.line\")\n",
        "  print('Time series  : https://plotly.com/python/time-series/    https://plotly.com/python/range-slider/ ')\n",
        "  print('Cumulative line plot  : https://plotly.com/python/ecdf-plots/')\n",
        "  \n",
        "  print()\n",
        "  print(\"Heatmap : https://plotly.com/python/heatmaps/ https://plotly.com/python/annotated-heatmap/ https://plotly.com/python-api-reference/generated/plotly.express.imshow.html\")\n",
        "  \n",
        " \n",
        "  print()\n",
        "  print(\"Dendogram  : https://plotly.com/python/dendrogram/ \")\n",
        "  print(\"network graph : https://plotly.com/python/network-graphs/\")\n",
        "   \n",
        "  print()\n",
        "  print(\"Machine learning / AI  : https://plotly.com/python/ai-ml/  https://plotly.com/python/roc-and-pr-curves/\")\n",
        "  print(\"ACP : ACP_ML(df,features_list,standardization=True)  BatonsBrises() ACP_EboulisPlot() ACP_CercleCorrelationsPlot(PCx,PCy) ACP_projections(ColorCol,PCx,PCy)  ACP_MatrixPlot_Projections(ColorCol,width,height,Nombre_Max_PC_graphique,) \")\n",
        "  print(\"Regression linÃƒÂ©aires / trendlines     :  https://plotly.com/python/ml-regression/  https://plotly.com/python/linear-fits/\")\n",
        "\n",
        "  with tab6:\n",
        "    print(f\"{pcolors.OK}CheatSheet{pcolors.RESET}\")\n",
        "    print(\"\\n https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf   \\n https://colab.research.google.com/drive/1-mfcGT9FvROM6pOL2_uNtx4ywrf6IlBn#scrollTo=icxtjOVUqN7F  \\n https://html-color-codes.info/Codes-couleur-HTML/     \\n https://docs.bokeh.org/en/latest/docs/gallery.html \")\n",
        "  \n",
        "#Aide()"
      ],
      "metadata": {
        "id": "w2UxPSJJ3pS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AideColor():\n",
        " return widgets.ColorPicker(\n",
        "    concise=False,\n",
        "    description='Pick a color',\n",
        "    value='blue',\n",
        "    disabled=False)"
      ],
      "metadata": {
        "id": "11gDdhh-r3xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AideColor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "93b99bd93ecc47e9b45021436b6af370",
            "c92656fcc6844adfb90cbed51d650e96",
            "6659ee00fe4f4811b75c4cddd62ca7fc"
          ]
        },
        "id": "dxEutKx6sdsh",
        "outputId": "0fbf8f82-9d0e-48b9-dd0a-32d2add4c9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ColorPicker(value='blue', description='Pick a color')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93b99bd93ecc47e9b45021436b6af370"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDZjGsrFiB2S"
      },
      "outputs": [],
      "source": [
        "# # a tester \n",
        "\n",
        "# https://pypi.org/project/pandas-profiling/\n",
        "\n",
        "# import sys\n",
        "# !{sys.executable} -m pip install -U pandas-profiling[notebook]\n",
        "# !jupyter nbextension enable --py widgetsnbextension\n",
        "# ! pip install ipywidgets\n",
        "\n",
        "# #\n",
        "# # from pandas_profiling import ProfileReport\n",
        "\n",
        "# # df = pd.DataFrame(np.random.rand(100, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DWFkD4-lRVr"
      },
      "source": [
        "\n",
        "* https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
        "\n",
        "* https://colab.research.google.com/drive/1-mfcGT9FvROM6pOL2_uNtx4ywrf6IlBn#scrollTo=icxtjOVUqN7F\n",
        "\n",
        "* https://html-color-codes.info/Codes-couleur-HTML/\n",
        "\n",
        "* https://docs.bokeh.org/en/latest/docs/gallery.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtiMnuY9lNO-"
      },
      "source": [
        "#### <font color=\"red\"> Cellules de bloc-notes template </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oxbSSfYl0Zj"
      },
      "outputs": [],
      "source": [
        "#########################################################################\n",
        "#  Aide Memoire!      Version 1.2                                     #\n",
        "#########################################################################\n",
        "# dt=BilanAlim\n",
        "# col=['index']\n",
        "\n",
        "# dt.info()\n",
        "# dt.describe(include='all')\n",
        "# dt.describe(include='category')\n",
        "\n",
        "#~~Search NaN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SearchNaN(dt,1)\n",
        "# dt.isnull().sum().sum()  # Is nul total df \n",
        "# SearchNaN(dt,2)\n",
        "# SearchNaN(dt,2).CodeZone.sort_values(na_position='first').unique()\n",
        "# len(SearchNaN(dt,2).CodeZone.sort_values(na_position='first').unique())\n",
        "# SearchNaN(dt,2).Zone.sort_values(na_position='first').unique()\n",
        "# SearchNaN(dt,2).Année.sort_values(na_position='first').unique()\n",
        "\n",
        "# dt.replace([np.inf, -np.inf], np.nan, inplace=True) # Remplacer les infini par des NaN (division ! )\n",
        "\n",
        "#~~Search values ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SearchListe(dt,[0])\n",
        "# SearchListe(dt,[0,np.nan,np.inf,-np.inf])\n",
        "# SearchListe(dt,[ 1, 249, 250, 273, 276, 351])\n",
        "# SearchListe(dt,[0]).CodeZone\n",
        "# dt.CodeZone.sort_values(ascending=True,na_position='first').unique()\n",
        "# dt.iloc[:,12].sort_values(ascending=True,na_position='first').unique()\n",
        "#  dt.iloc[:,2].unique()\n",
        "# dt.iloc[:,3:19]\n",
        "# dt.AlphaISO3.sort_values(ascending=True,na_position='first').unique()\n",
        "# len(dt.AlphaISO3.sort_values(ascending=True,na_position='first'))\n",
        "\n",
        "\n",
        "\n",
        "#__Liste valeur unique dans chaque colonne du df  _____________\n",
        "# for col in ListeCol: \n",
        "#   # print(col,':',len(dt[col].unique()),': \\n',dt[col].sort_values(ascending=True,na_position='first').unique(),'\\n______________\\n')\n",
        "#   print(col,':',len(dt[col].unique()),': \\n',dt[col].unique(),'\\n______________\\n') #Unsorted\n",
        "#-----------\n",
        "\n",
        "#~~ Différences liste ou df ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# LostElement(A_array,B_array)\n",
        "# get_different_rows(source_df, new_df)\n",
        "# ListeColUnique(dt)\n",
        "\n",
        "\n",
        "#~~ Différences liste ou df ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# df=dt\n",
        "# regex=\"^[Tt]h.*\"\n",
        "# df[df.Zone.str.contains(regex)]\n",
        "#-----------\n",
        "\n",
        "\n",
        "#~~ Dtype ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# 0.  # dtype Category => object (str)\n",
        "# reduce_mem_usage(dt) # Optisation dtype\n",
        "#__iloc dtype change ____________________\n",
        "# for i in range(2,5):\n",
        "#   dt.iloc[:,i]=dt.iloc[:,i].astype(float)\n",
        "#-----------\n",
        "\n",
        "#------ test Erreur dtype trop Petit: \n",
        "\n",
        "# print('Min',dt[col].min(),'Moy',dt[col].mean(),'Max',dt[col].max(),'Sum',dt[col].sum())  \n",
        "\n",
        "\n",
        "# dt.info()\n",
        "\n",
        "##### Df List\n",
        "# SecuAlimGeo\n",
        "\n",
        "##################### Aide memoire ##################\n",
        "# [i for i in range(2,12)]\n",
        "\n",
        "# dt.iloc[:, [2,3,12,14,15]]\n",
        "# dt.iloc[:,[i for i in range(18)]],2)\n",
        "\n",
        "# ColMove(df,ColName,ColIndex)\n",
        "# df= df.iloc[:,[0,1,2,3,7,12,16]]\n",
        "# o\n",
        "####################################################\n",
        "\n",
        "# !cat /proc/meminfo # Voir RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEr7mcvUpNEg"
      },
      "source": [
        "#### Liste Fonctions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8nJKvm997cJ"
      },
      "source": [
        "### Search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugv88iKGovoM"
      },
      "outputs": [],
      "source": [
        "def get_df_name(df):\n",
        "    name =[x for x in globals() if globals()[x] is df][0]\n",
        "    return name\n",
        "#source : https://stackoverflow.com/questions/31727333/get-the-name-of-a-pandas-dataframe   Pour les series utiliser .name  (ou .names )\n",
        "\n",
        "#Global.name = f\"{get_df_name(Global)}str\" ; Global.name    #df.name = get_df_name(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_np_array_name(obj, namespace):\n",
        "  \"\"\"\n",
        " get name of np array as a string  :  get_np_array_name(obj, namespace)\n",
        "     ==> most common namespace : globals()\n",
        " \n",
        " Usage example : \n",
        " my_numpy = np.zeros(2)\n",
        " get_np_array_namemy_numpy, globals())\n",
        "\n",
        "\"\"\"\n",
        "  return [name for name in namespace if namespace[name] is obj]\n",
        "\n",
        "# my_numpy = np.zeros(2)\n",
        "#get_np_array_name(my_numpy, globals())"
      ],
      "metadata": {
        "id": "OlMtsvaObpon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeRXBSMjl_o8"
      },
      "source": [
        "##### Search NaN  SearchNaN(DataFrame,Option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhZlTn2ZmEZk"
      },
      "outputs": [],
      "source": [
        "#Fonction ChercheNaN : Recherche des NaN\n",
        "def SearchNaN(DataFrame,Option):\n",
        "  \"\"\"ChercheNaN : Recherche des NaN\n",
        "  \n",
        "  Option : \n",
        "  1: Total nombre de NaN par colonne\n",
        "  2: Afficher les lignes ayant au moins 1 NaN\n",
        "  3: Nomnbre de lignes ayant 1 NaN\n",
        "  \"\"\"\n",
        "  df_name = get_df_name(DataFrame)\n",
        "\n",
        "  Nanrecap = pd.DataFrame(df.isnull().sum(axis=0),columns=['Nbr_NaN'])\n",
        "  Nanrecap[\"%-NaN\"] = (Nanrecap.Nbr_NaN/len(df))*100\n",
        "  Nanrecap.sort_values(by='Nbr_NaN', ascending=True, inplace=True)\n",
        "\n",
        "  if Option==1:\n",
        "   print(pcolors.OK+ df_name +pcolors.RESET,': Nombre de NaN par colonne',DataFrame.shape) \n",
        "   print(Nanrecap)\n",
        "   \n",
        "    #  Option1 : Total nombre de NaN par colonne\n",
        "  elif Option==2:\n",
        "    return DataFrame[DataFrame.isnull().any(axis=1)]  #Afficher les lignes ayant au moins 1 NaN\n",
        "  elif Option==3: \n",
        "    return print(pcolors.OK+ df_name +pcolors.RESET,': Nombre de ligne(s) ayant au moins 1 NaN :',\n",
        "                 len(DataFrame[DataFrame.isnull().any(axis=1)]),'\\n-----%------\\n',\n",
        "                 (len(DataFrame[DataFrame.isnull().any(axis=1)])/len(DataFrame))*100)\n",
        "  else:\n",
        "    print(\"SearchNaN(DataFrame,Option) Error :\",\"Choisir Option :\\n\",\n",
        "          \"\\n 1: Total nombre de NaN par colonne\",\n",
        "          \"\\n 2: Total nombre de NaN par colonne\",\n",
        "          \"\\n 3: Nomnbre de lignes ayant 1 NaN\")\n",
        "\n",
        "# DEBUG  recherche NaN\n",
        "# temp=geo\n",
        "# temp.isnull().sum(axis=0) #  Option1 : Total nombre de NaN par colonne\n",
        "# test=temp[temp.isnull().any(axis=1)] ; test #Afficher les lignes ayant au moins 1 NaN\n",
        "# len(test)\n",
        "# test.Zone.unique()\n",
        "\n",
        "# exemple code recherc NaN  : df.query(\"AlphaISO3.isna()\", engine=\"python\") # recherche des nan dans la colone AlphaISO3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1uh5_f5mIWb"
      },
      "source": [
        "##### SearchListe(df,liste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRsXPpgqoYFB"
      },
      "outputs": [],
      "source": [
        "def get_different_rows(source_df, new_df):\n",
        "    \"\"\"Returns just the rows (right_only) from the new dataframe that differ from the source dataframe\"\"\"\n",
        "    merged_df = source_df.merge(new_df, indicator=True, how='outer')\n",
        "    changed_rows_df = merged_df[merged_df['_merge'] == 'right_only']\n",
        "    return changed_rows_df.drop('_merge', axis=1)\n",
        "\n",
        "\n",
        "#Trouver les lignes peu communes entre deux DataFrames\n",
        "# pd.concat([SecuAlimGeo,SecuAlim]).drop_duplicates(keep=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBNjEd4RmMCq"
      },
      "outputs": [],
      "source": [
        "# Recherche une liste dans un DataFrame\n",
        "def SearchListe(df,liste):\n",
        "  \"\"\"Recherche une liste dans un DataFrame et renvoie Df avec les Valeurs pour masque SearchListe(dx,[2,4,7]) - liste=[2,4,7]/ SearchListe(dx,liste) accepte liste unique liste=[1]\"\"\"\n",
        "  return df[df.isin(liste)].dropna(thresh=1)\n",
        "\n",
        "# Exemple : \n",
        "# dx = pd.DataFrame(np.random.randint(10, size=(10, 3)),\n",
        "#                      columns=['A', 'B', 'C'])\n",
        "# dx\n",
        "# SearchListe(dx,[2,4,7])\n",
        "# liste=[2,4,7]\n",
        "# SearchListe(dx,liste)\n",
        "# SearchListe(dx,l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWAaJeqpoVeN"
      },
      "source": [
        "##### Différences entre deux df get_different_rows(source_df, new_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz0FVvnI-ODS"
      },
      "source": [
        "### Manip DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wniVx7NDntuB"
      },
      "source": [
        "##### ColMove(df,ColName,ColIndex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4UJxPfBnzy5"
      },
      "outputs": [],
      "source": [
        "# Deplacer une colonne dans un dataframe \n",
        "def ColMove(DataFrame,ColName,ColIndex):\n",
        "  \"\"\"Fonction  ColMove(DataFrame,ColName,ColIndex)  \n",
        "   Deplace un colonne dans un DataFrame  - ColName : nom colonne  - ColIndex : rang index souhaité (début df 0 !)\n",
        "  \"\"\" \n",
        "  DataFrame.insert(ColIndex,'Xcol',DataFrame[ColName])\n",
        "  DataFrame.drop(ColName, axis=1, inplace=True)\n",
        "  DataFrame.rename(columns={'Xcol' : ColName},inplace=True)\n",
        "  return DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9I5J4XHkun"
      },
      "source": [
        "##### MergeAudit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F92V-GRSHieA"
      },
      "outputs": [],
      "source": [
        "def MergeAudit(df): \n",
        "  print(\"\\n---------------------------------------------------------------------------\")\n",
        "  print(f\"{pcolors.BLUE}MergeAudit : {pcolors.RESET}{df._merge.unique()} {df.shape}\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}Merge_right_only :{pcolors.RESET} {df[df._merge=='right_only'].shape}\")\n",
        "  # Merge_right_only = df[df._merge=='right_only']\n",
        "\n",
        "  print(f\"{pcolors.WARNING}Merge_left_only :{pcolors.RESET} {df[df._merge=='left_only'].shape}\")\n",
        "  # Merge_left_only = df[df._merge=='left_only']\n",
        "\n",
        "  print(f\"{pcolors.WARNING}Merge_both :{pcolors.RESET} {df[df._merge=='both'].shape}\")\n",
        "  # Merge_both = df[df._merge=='both']\n",
        "\n",
        "  print(\"Note gobal df names :  Merge_right_only Merge_left_only  Merge_both\")\n",
        "\n",
        "  print(\"---------------------------------------------------------------------------\")\n",
        "  INFOS(df)\n",
        "  \n",
        "  global  Merge_right_only\n",
        "  global  Merge_left_only \n",
        "  global  Merge_both\n",
        "\n",
        "  Merge_right_only = df[df._merge=='right_only']\n",
        "  Merge_left_only = df[df._merge=='left_only']\n",
        "  Merge_both = df[df._merge=='both']\n",
        "  # return Merge_both.head(5) \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L5TnzxOA5_I"
      },
      "source": [
        "### Manip Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujRsJccaTMQd"
      },
      "outputs": [],
      "source": [
        "def DateTimeDetail(df,DateColSTR) : \n",
        "\n",
        "  # df[\"DateCourte\"]=pd.to_datetime(df[\"DateCourte\"],dayfirst=True)\n",
        " df['YearQuarter'] = df[DateColSTR].dt.to_period('Q')\n",
        "\n",
        " df[\"Year\"] = df[DateColSTR].dt.year.astype(np.int16)\n",
        " df[\"Month\"] = df[DateColSTR].dt.month.astype(np.int8)\n",
        " df[\"Day\"] = df[DateColSTR].dt.day.astype(np.int8)\n",
        " \n",
        " df['QuarterNum'] = (df[\"Month\"] - 1) // 3 + 1\n",
        " \n",
        " df.sort_values(by=[DateColSTR], inplace=True)\n",
        " return df\n",
        "\n",
        "  # https://pandas.pydata.org/docs/user_guide/timeseries.html\n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n",
        "  # https://docs.python.org/fr/3.7/library/datetime.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM41Y3x8TEU2"
      },
      "outputs": [],
      "source": [
        "def DateTime2WeekFR(df,DateColSTR) : \n",
        "\n",
        "  get_df_name(df)\n",
        "  # date = \"DateColSTR\"\n",
        "\n",
        "  #Conversion colonne date  au format datetime \n",
        "  df[DateColSTR] = pd.to_datetime(df[DateColSTR])\n",
        "  \n",
        "  df[\"weekdayNum\"]=df[DateColSTR].dt.weekday\n",
        "  #https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.weekday.html\n",
        "  #=> The day of the week with Monday=0=> Sunday=6 & translate in French (Numerotation FR (Lundi 1 dimanche 7)\n",
        "  conditionlist = [\n",
        "                 df[\"weekdayNum\"] == 0,\n",
        "                 df[\"weekdayNum\"] == 1,\n",
        "                 df[\"weekdayNum\"] == 2,\n",
        "                 df[\"weekdayNum\"] == 3,\n",
        "                 df[\"weekdayNum\"] == 4,\n",
        "                 df[\"weekdayNum\"] == 5,\n",
        "                 df[\"weekdayNum\"] == 6]\n",
        "                \n",
        "  choicelist = ['Lundi','Mardi','Mercredi',\"Jeudi\",\"Vendredi\",\"Samedi\",\"Dimanche\"]\n",
        "  df[\"weekday\"] = np.select(conditionlist, choicelist, default='?')\n",
        "  df[\"weekday\"] = df[\"weekday\"].astype('category') #Optimisation dtype \n",
        "\n",
        "  df[\"weekdayNum\"] = df[\"weekdayNum\"]+ 1 # Numerotation FR (Lundi 1 dimanche 7)\n",
        "\n",
        "  # df[\"weekdayEN\"] = df[\"date\"].dt.day_name() #Ok mais en Anglais  sur Colab! \n",
        "  # df[\"weekdayEN\"] = df[\"date\"].dt.day_name(locale='French') \n",
        "\n",
        "  df['weeknum'] = df[DateColSTR].apply(lambda x:x.isocalendar()[1]).astype(np.int8) #Numéro de Semaine\n",
        "  df.sort_values(by=[DateColSTR], inplace=True)\n",
        "  return df\n",
        "\n",
        "  # https://pandas.pydata.org/docs/user_guide/timeseries.html\n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n",
        " # https://docs.python.org/fr/3.7/library/datetime.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mJcn1KePuOZ"
      },
      "outputs": [],
      "source": [
        "def Age(df,birthdateCol):\n",
        " \"\"\" Calcul Age à partir de la colonne birthdateCol (au format datetime)  + Ajout colonne Age10 ( age decenie)\n",
        " \"\"\"\n",
        " from datetime import datetime\n",
        " get_df_name(df)\n",
        "  \n",
        " #  CurentYear = int(pd.to_datetime(\"today\").strftime(\"%Y\"))\n",
        " \n",
        " #  #Conversion colonne birthdateCol au format datetime \n",
        " #  df[birthdateCol] = pd.to_datetime(df[birthdateCol])\n",
        "\n",
        "\n",
        " # Calcul Age \n",
        " df[\"Age\"] = df[birthdateCol].apply(lambda x : (datetime.now().year - x.year))\n",
        "\n",
        " #  # Calcul Decenie Age \n",
        " df[\"Age10\"] = (df.Age // 10 )*10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfz68wcGqBiI"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/hamelg/python-for-data-17-dealing-with-dates/notebook\n",
        "# column_1 = dates.iloc[:,0]\n",
        "\n",
        "# pd.DataFrame({\"year\": column_1.dt.year,\n",
        "#               \"month\": column_1.dt.month,\n",
        "#               \"day\": column_1.dt.day,\n",
        "#               \"hour\": column_1.dt.hour,\n",
        "#               \"dayofyear\": column_1.dt.dayofyear,\n",
        "#               \"week\": column_1.dt.week,\n",
        "#               \"weekofyear\": column_1.dt.weekofyear,\n",
        "#               \"dayofweek\": column_1.dt.dayofweek,\n",
        "#               \"weekday\": column_1.dt.weekday,\n",
        "#               \"quarter\": column_1.dt.quarter,\n",
        "#              })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYVkMO_SD7Jx"
      },
      "source": [
        "### Data type Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MBrf4AlnGhK"
      },
      "source": [
        "##### Change dtype Category to Object : dtypeCat2Obj(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J6fmymnnJCb"
      },
      "outputs": [],
      "source": [
        "def dtypeCat2Obj(df):\n",
        "    \"\"\"  iterate through all the columns of a dataframe and \n",
        "    modify the Dtype Catergory to Object (Str)       \n",
        "    \"\"\"\n",
        "     \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type == 'category':\n",
        "             df[col] = df[col].astype(str)    \n",
        "           \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOrc2k05nTH5"
      },
      "source": [
        "##### ReduceMemUsage reduce_mem_usage(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSU1TrwynWXZ"
      },
      "outputs": [],
      "source": [
        "# Drastically reduce df RAM usage ! \n",
        "#I don't know who the original author of this function but many thanks to him ;) \n",
        "#source : https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" \n",
        "    iterate through all the columns of a dataframe and \n",
        "    modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(('Memory usage of dataframe is {:.2f}' \n",
        "                     'MB').format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "                \n",
        "        if col_type != object :\n",
        "        # if col_type != object | col_type != \"datetime64[ns]\" | col_type != \"period[Q-DEC]\":\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max <\\\n",
        "                  np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max <\\\n",
        "                   np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max <\\\n",
        "                   np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max <\\\n",
        "                   np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max <\\\n",
        "                   np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max <\\\n",
        "                   np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            if col_type == object : \n",
        "             df[col] = df[col].astype('category')\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(('Memory usage after optimization is: {:.2f}' \n",
        "                              'MB').format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n",
        "                                             / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "    # Attention ne fct pas si le df à optimizer contient un type 'category'\n",
        "  #Code ...........\n",
        "    # df.info(memory_usage=True)\n",
        "    # DataFrame.memory_usage(index=True, deep=False)\n",
        "\n",
        "    #Ajouter gestion des data types suivants : \n",
        "    # datetime64[ns]  a tester\n",
        "    # period[Q-DEC]  a tester"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drastically reduce df RAM usage ! \n",
        "#I don't know who the original author of this function but many thanks to him ;) \n",
        "#source : https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff\n",
        "# Remaque avec   Dicts can be used to emulate switch/case statement   : https://t.dripemail2.com/c/eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJkZXRvdXIiLCJpc3MiOiJtb25vbGl0aCIsInN1YiI6ImRldG91cl9saW5rIiwiaWF0IjoxNjY1NTQ0ODI5LCJuYmYiOjE2NjU1NDQ4MjksImFjY291bnRfaWQiOiI2MjE0NTAwIiwiZGVsaXZlcnlfaWQiOiJhNXNucmJmNmYyZmVjOHkzbjY2dCIsInVybCI6Imh0dHBzOi8vd3d3LmdldGRyaXAuY29tL2RlbGl2ZXJpZXMvYTVzbnJiZjZmMmZlYzh5M242NnQ_dmlld19pbl9icm93c2VyPXRydWUmX19zPXJsdWthN3lnbDFwc3NpcmUwdzZmIn0.9ACHImcHACCgkh-umWqxYZukGdSvwCSzpyvJOHzttrE\n",
        "\n",
        "\n",
        "def dispatch_dict(operator, x, y):\n",
        "    return {\n",
        "        'add': lambda: x + y,\n",
        "        'sub': lambda: x - y,\n",
        "        'mul': lambda: x * y,\n",
        "        'div': lambda: x / y,\n",
        "    }.get(operator, lambda: None)()\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "                \n",
        "        if col_type != object :\n",
        "        # if col_type != object | col_type != \"datetime64[ns]\" | col_type != \"period[Q-DEC]\":\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max <\\\n",
        "                  np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max <\\\n",
        "                   np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max <\\\n",
        "                   np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max <\\\n",
        "                   np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max <\\\n",
        "                   np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max <\\\n",
        "                   np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            if col_type == object : \n",
        "             df[col] = df[col].astype('category')\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(('Memory usage after optimization is: {:.2f}' \n",
        "                              'MB').format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n",
        "                                             / start_mem))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    return df\n",
        "\n",
        "    # Attention ne fct pas si le df à optimizer contient un type 'category'\n",
        "  #Code ...........\n",
        "    # df.info(memory_usage=True)\n",
        "    # DataFrame.memory_usage(index=True, deep=False)\n",
        "\n",
        "    #Ajouter gestion des data types suivants : \n",
        "    # datetime64[ns]  a tester\n",
        "    # period[Q-DEC]  a tester"
      ],
      "metadata": {
        "id": "O7oo7Nvto45Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOiZ4HjinZvi"
      },
      "source": [
        "###### dtype Min-Max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "dJHGt2kundVh",
        "outputId": "7bd93ed8-7fbf-4902-bd5b-0a1ab9de9aa8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   np.int8:  np.int16:  np.int32:   np.int64:            np.float16:  \\\n",
              "0 -128      -32768     -2147483648 -9223372036854775808 -65504.0       \n",
              "1  127       32767      2147483647  9223372036854775807  65504.0       \n",
              "\n",
              "   np.float32:   np.float64:    \n",
              "0 -3.402823e+38 -1.797693e+308  \n",
              "1  3.402823e+38  1.797693e+308  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61e5d87f-5457-4cfe-a8c5-12b1540d8341\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th></th>\n",
              "      <th>np.int8:</th>\n",
              "      <th>np.int16:</th>\n",
              "      <th>np.int32:</th>\n",
              "      <th>np.int64:</th>\n",
              "      <th>np.float16:</th>\n",
              "      <th>np.float32:</th>\n",
              "      <th>np.float64:</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-128</td>\n",
              "      <td>-32768</td>\n",
              "      <td>-2147483648</td>\n",
              "      <td>-9223372036854775808</td>\n",
              "      <td>-65504.0</td>\n",
              "      <td>-3.402823e+38</td>\n",
              "      <td>-1.797693e+308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>127</td>\n",
              "      <td>32767</td>\n",
              "      <td>2147483647</td>\n",
              "      <td>9223372036854775807</td>\n",
              "      <td>65504.0</td>\n",
              "      <td>3.402823e+38</td>\n",
              "      <td>1.797693e+308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61e5d87f-5457-4cfe-a8c5-12b1540d8341')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61e5d87f-5457-4cfe-a8c5-12b1540d8341 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61e5d87f-5457-4cfe-a8c5-12b1540d8341');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.dataresource+json": {
              "schema": {
                "fields": [
                  {
                    "name": "index",
                    "type": "integer"
                  },
                  {
                    "name": "np.int8:",
                    "type": "integer"
                  },
                  {
                    "name": "np.int16:",
                    "type": "integer"
                  },
                  {
                    "name": "np.int32:",
                    "type": "integer"
                  },
                  {
                    "name": "np.int64:",
                    "type": "integer"
                  },
                  {
                    "name": "np.float16:",
                    "type": "number"
                  },
                  {
                    "name": "np.float32:",
                    "type": "number"
                  },
                  {
                    "name": "np.float64:",
                    "type": "number"
                  }
                ],
                "primaryKey": [
                  "index"
                ],
                "pandas_version": "0.20.0"
              },
              "data": [
                {
                  "index": 0,
                  "np.int8:": -128,
                  "np.int16:": -32768,
                  "np.int32:": -2147483648,
                  "np.int64:": -9223372036854776000,
                  "np.float16:": -65504,
                  "np.float32:": -3.402823466e+38,
                  "np.float64:": "-inf"
                },
                {
                  "index": 1,
                  "np.int8:": 127,
                  "np.int16:": 32767,
                  "np.int32:": 2147483647,
                  "np.int64:": 9223372036854776000,
                  "np.float16:": 65504,
                  "np.float32:": 3.402823466e+38,
                  "np.float64:": "inf"
                }
              ]
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "pd.DataFrame.from_dict(\n",
        "{'np.int8:':[np.iinfo(np.int8).min, np.iinfo(np.int8).max],\n",
        "'np.int16:':[np.iinfo(np.int16).min, np.iinfo(np.int16).max],\n",
        "'np.int32:':[np.iinfo(np.int32).min, np.iinfo(np.int32).max],\n",
        "'np.int64:':[np.iinfo(np.int64).min, np.iinfo(np.int64).max],\n",
        "'np.float16:':[np.finfo(np.float16).min, np.finfo(np.float16).max],\n",
        "'np.float32:':[np.finfo(np.float32).min, np.finfo(np.float32).max],\n",
        "'np.float64:':[np.finfo(np.float64).min, np.finfo(np.float64).max]})\n",
        "\n",
        "## info complementaire https://towardsdatascience.com/reducing-memory-usage-in-pandas-with-smaller-datatypes-b527635830af"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnX8FACIn5ZS"
      },
      "source": [
        "##### FlatIndex(df) : Conversion multi-index  à index simple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RenameDfColPythonic(df): \n",
        " import re\n",
        " NewListCol=[]  # temp empty liste\n",
        " for ColName in  df.columns:   # simplify colum names \n",
        "  string = str(ColName)\n",
        "  string = re.sub(\"\\(|\\)|\\'\",\"\",string).replace('\"', '').replace(\" \", \"\").replace(\",\", \"_\").replace('É', 'E').replace('é', 'e').replace('è', 'e').replace('ê', 'e') # subtitute and replace specific caracters\n",
        "  # print(string)\n",
        "  NewListCol= np.append(NewListCol, string)\n",
        "\n",
        " # NewListCol \n",
        "\n",
        " df.columns = NewListCol  # alocate new columns names \n",
        " return df"
      ],
      "metadata": {
        "id": "rflWYYkxinCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FlatIndex(df):\n",
        " \"\"\" Flat  multi-index datatFrame & rename columns in one word pythonic name \"\"\"\n",
        " #  Step I : Convert muli-index  to simple index DataFrame\n",
        " df.columns = df.columns.to_flat_index() # colonnes => concatener le multi index en un index simple https://datascientyst.com/flatten-multiindex-in-pandas/\n",
        " df = df.reset_index() # Conversion Index en colonne simple\n",
        "\n",
        "\n",
        " #  Step II : renane DataFrame columns with one word name ! \n",
        " RenameDfColPythonic(df)  # Mylib_VA fonction\n",
        "\n",
        "  # NewListCol \n",
        "\n",
        " #df.columns = NewListCol  # alocate new columns names \n",
        "\n",
        " return df\n"
      ],
      "metadata": {
        "id": "hJW229bTitYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQq7ziN55Q3E"
      },
      "outputs": [],
      "source": [
        "def FlatIndexVO(df):\n",
        " \"\"\" Flat  multi-index datatFrame & rename columns in one word pythonic name \"\"\"\n",
        "\n",
        " #  Step I : Convert muli-index  to simple index DataFrame\n",
        " df.columns = df.columns.to_flat_index() # colonnes => concatener le multi index en un index simple https://datascientyst.com/flatten-multiindex-in-pandas/\n",
        " df = df.reset_index() # Conversion Index en colonne simple\n",
        "\n",
        "\n",
        " #  Step II : renane DataFrame columns with one word name ! \n",
        " import re\n",
        " NewListCol=[\"alphaTemp\"]  # temp new name columns list \n",
        "\n",
        " for ColName in  df.columns:   # simplify colum names \n",
        "  string = str(ColName)\n",
        "  string = re.sub(\"\\(|\\)|\\'\",\"\",string).replace('\"', '').replace(\" \", \"\").replace(\",\", \"_\") # subtitute and replace specific caracters\n",
        "  # print(string)\n",
        "  NewListCol= np.append(NewListCol, string)\n",
        "\n",
        " NewListCol = np.delete(NewListCol,0)   # delete  \"alpha\"  from NewListCol\n",
        " # NewListCol \n",
        "\n",
        " df.columns = NewListCol  # alocate new columns names \n",
        "\n",
        " return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TF_ApSU8YrP"
      },
      "source": [
        "### Fonction Satistiques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMJGjZbEG1QZ"
      },
      "source": [
        "* [Tableau recap Test Satistiques](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9c3ba3ff-43b7-42e3-9874-aff362e22c81/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221022%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221022T094752Z&X-Amz-Expires=86400&X-Amz-Signature=6afa116c3458a800302a221f2c2b6265a74a94edeaacf0d66730f9308e67ef2b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject) & [Tab2] (https://s3.us-west-2.amazonaws.com/secure.notion-static.com/475d5ed8-4eb5-4aec-8be2-121b145f8ef4/Untitled.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221022%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221022T094529Z&X-Amz-Expires=86400&X-Amz-Signature=10ea905a6bde0a9d4b5b2095150208ba3ea90a4279632f3731a576f82c1840af&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.pdf%22&x-id=GetObject)\n",
        "\n",
        "* [Guide choix test stat (conditions validité) ](https://help.xlstat.com/fr/6443-which-statistical-test-should-you-use)\n",
        " \n",
        "* https://pingouin-stats.org/guidelines.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPhqzlgqowQ4"
      },
      "source": [
        "##### PercentCumul(df,Col) Calcul % cumulé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUV0c1-7ol4B"
      },
      "outputs": [],
      "source": [
        "# Tri ordre decroissant puis ajoute Colonnne avec Calcul % cumulé pour identifier 20/80 ou 80/20 Arrondi 2\n",
        "def PercentCumul(df,Col):\n",
        "  \"\"\" Tri ordre decroissant puis ajoute Colonnne Col_cum% avec Calcul % cumulé ( arondi 2) pour identifier facilement 20/80 ou 80/20 \"\"\"\n",
        "  df=df.sort_values(by=[Col],ascending=False)\n",
        "  df[Col+'_pourcent']=(df[Col]/df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent_Cumul']=(df[Col].cumsum() / df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent']=df[Col+'_pourcent'].round(2)\n",
        "  df[Col+'_pourcent_Cumul']=df[Col+'_pourcent_Cumul'].round(2)\n",
        " \n",
        "  # df=df.sort_values(by=[Col+'cum_%'],ascending=False)\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "# Cf cumsum() pour  SommeCumulée\n",
        " #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTDbvoTC9zBv"
      },
      "outputs": [],
      "source": [
        "# Tri ordre decroissant puis ajoute Colonnne avec Calcul % cumulé pour identifier 20/80 ou 80/20  arondi RoundDecimalNumber\n",
        "def PercentCumulRoundX(df,Col,RoundDecimalNumber):\n",
        "  \"\"\" Tri ordre decroissant puis ajoute Colonnne Col_cum% avec Calcul % cumulé (avec arondi RoundDecimalNumber) pour identifier facilement 20/80 ou 80/20 \"\"\"\n",
        "  df=df.sort_values(by=[Col],ascending=False)\n",
        "  df[Col+'_pourcent']=(df[Col]/df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent_Cumul']=(df[Col].cumsum() / df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent']=df[Col+'_pourcent'].round(RoundDecimalNumber)\n",
        "  df[Col+'_pourcent_Cumul']=df[Col+'_pourcent_Cumul'].round(RoundDecimalNumber)\n",
        " \n",
        "  # df=df.sort_values(by=[Col+'cum_%'],ascending=False)\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "# Cf cumsum() pour  SommeCumulée\n",
        " #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4e-1wAl9ruz"
      },
      "outputs": [],
      "source": [
        "# Tri ordre decroissant puis ajoute Colonnne avec Calcul % cumulé pour identifier 20/80 ou 80/20  SANS arondi \n",
        "def PercentCumulNoRound(df,Col):\n",
        "  \"\"\" Tri ordre decroissant puis ajoute Colonnne Col_cum% avec Calcul % cumulé ' sans arondi pour identifier facilement 20/80 ou 80/20 \"\"\"\n",
        "  df=df.sort_values(by=[Col],ascending=False)\n",
        "  df[Col+'_pourcent']=(df[Col]/df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent_Cumul']=(df[Col].cumsum() / df[Col].sum()) * 100\n",
        "  df[Col+'_pourcent']=df[Col+'_pourcent']\n",
        "  df[Col+'_pourcent_Cumul']=df[Col+'_pourcent_Cumul']\n",
        " \n",
        "  # df=df.sort_values(by=[Col+'cum_%'],ascending=False)\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "# Cf cumsum() pour  SommeCumulée\n",
        " #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39dfdCIk2gVW"
      },
      "source": [
        "##### Pareto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5oXeIN22c7x"
      },
      "outputs": [],
      "source": [
        "def Pareto(data,percent):\n",
        "  \"\"\" Calcul CA Pareto  \n",
        "  data=df.Col  (df.Colonne)\n",
        "  percent = 80   (seuil float % cumulé)\n",
        "  \"\"\" \n",
        "  # data=df.CAcum_p \n",
        "  # percent = 80\n",
        "  dataListe = data[data <= percent]\n",
        "  liste = dataListe.index.unique().sort_values(ascending=True)\n",
        "  # print(percent ,\"% du\", data.name , \"est réalisé avec\", len (dfListe ),\"des\",liste.name,\":\")\n",
        "  print(pcolors.OK + \"\",len(dataListe),liste.name,\"(\",round(((len(dataListe)/len(data))*100),2),\"%) totalisent\",percent ,\"% du\", data.name ,\":\"+ pcolors.RESET)\n",
        "  print(liste)\n",
        "  return dataListe\n",
        "\n",
        " # Aller plus loing  Pareto :  https://commentprogresser.com/outil-pareto.html\n",
        " # ABC  \n",
        " #  s'agit d'une variante des 20/80 qui propose cette fois-ci un découpage en 3 segments.\n",
        " #   Classe A : 20% des causes représentent 80% des effets.\n",
        " #   Classe B : 30% des causes représentent 15% des effets.\n",
        " #   Classe C: 50% des causes représentent 5% des effets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQklXV_n-g6"
      },
      "source": [
        "#### Moyenne Pondérée "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUlLQG9pef1T"
      },
      "outputs": [],
      "source": [
        "# Calcul Moyenne Pondérée / Weighted Average\n",
        "\n",
        "def MoyPonderee(df, Colvalues, Colweights):\n",
        "  \"\"\" Moyenne Pondérée / Weighted Average\n",
        "    df: DataFrame  - Colvalues : Col Valeurs  - Colweights : Col. Poids relatif \n",
        "  Peut être utlisée avec un Groupby : df.groupby('Year').apply(MoyPonderee, 'Grades', 'NumCourses')\n",
        "  Source :  Rework of https://datagy.io/pandas-weighted-average/ \n",
        "  \"\"\"\n",
        "  return sum(df[Colweights] * df[Colvalues]) / df[Colweights].sum()\n",
        "\n",
        "\n",
        "# Source Rework of https://datagy.io/pandas-weighted-average/ \n",
        "# autre methode : https://numpy.org/doc/stable/reference/generated/numpy.average.html?highlight=average#numpy.average \n",
        "# autre source :  https://datagy.io/pandas-weighted-average/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0rklT4VlYMo"
      },
      "source": [
        "#### QuartileIndicatorCol(df,col) Ajout Colonne quartile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za2KZttuS-Es"
      },
      "outputs": [],
      "source": [
        "# Quartile (Ajout colonne conditionnelle quartile) & Outliers methode Interquartiles V2 \n",
        "\n",
        "def QuartileIndicatorCol(df,col): \n",
        "  \"\"\" Ajout d'un colonne Conditionelle avec le quartile au df courant  & Recherce Ouliers Methode IQ ( InterQuartiles)\n",
        " QartileIndicatorCol(df,col) => df[col+\"Quartile\"] \"\"\"\n",
        "\n",
        "  # Caculs Seuils des quartiles \n",
        "  q25=df[col].quantile(q=0.25, interpolation='linear')  #Q1\n",
        "  q50=df[col].quantile(q=0.50, interpolation='linear')  #Q2\n",
        "  q75=df[col].quantile(q=0.75, interpolation='linear')  #Q3\n",
        "\n",
        "  Q1=q25 ;   Q2=q50 ;   Q3=q75 # Quartiles simplifiés\n",
        "\n",
        "  # Quartiles Conditions & label \n",
        "  conditionlist = [\n",
        "    (df[col] <= q25) ,\n",
        "    (df[col] > q25) & (df[col] <q75),\n",
        "    (df[col] >= q75)]\n",
        "   \n",
        "  choicelist = ['quartile1','quartile2','quartile3']\n",
        "\n",
        "  df[col+\"_Quartile\"] = np.select(conditionlist, choicelist, default='Not Specified')\n",
        "  df[col+\"_Quartile\"] = df[col+\"_Quartile\"].astype('category')\n",
        "\n",
        "  # écart interquartile\n",
        "  EcartInterquartile  = Q3 - Q1  # IQ = Q3 - Q1\n",
        "  TauxIQoutliers = 1.5\n",
        "  IQoutliersInf = Q1 -  TauxIQoutliers *  EcartInterquartile # Outliers Inférieurs ?\n",
        "  IQoutliersSup = Q3 +  TauxIQoutliers *  EcartInterquartile # Outliers ISuperieur ? \n",
        "\n",
        " # IQ Conditions & label \n",
        "  conditionlist = [\n",
        "   (df[col] <  IQoutliersInf) ,\n",
        "   (df[col] >=  IQoutliersInf) & (df[col] <= IQoutliersSup),\n",
        "   (df[col] > IQoutliersSup)    ]\n",
        "  \n",
        "  choicelist = ['IQ_Ouliers_Inf','-','IQ_Ouliers_Sup'] \n",
        "\n",
        "  df[col+\"_IQ_Ouliers\"] = np.select(conditionlist, choicelist, default='Not Specified')\n",
        "  df[col+\"_IQ_Ouliers\"] = df[col+\"_IQ_Ouliers\"].astype('category') \n",
        "  \n",
        "  print( pcolors.OK + \"Methode IQ \"+ col+ \":\"+ pcolors.RESET ,\n",
        "        \"Ecart Interquartile= \",EcartInterquartile,\n",
        "        \" -Q1 : \",Q1,\" -Q2 : \",Q2, \" -Q3 : \",Q3)\n",
        "     \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzChDO4B75nj"
      },
      "source": [
        "####  Zscore :  Zscore(df,Col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTdYGglH8AfN"
      },
      "outputs": [],
      "source": [
        "#Zscore\n",
        "def Zscore(df,Col): \n",
        "  \"\"\"Zscore(df,Col) :  Ajout colonnes Zscore_Col &  NivConf_Zscore_col\n",
        "                      'ZSscore Omlit NaN  & Interepretion Niv confiance\n",
        "  Parametres  Df: DataFrame & col : Colonne\n",
        "  \"\"\"\n",
        "  import scipy.stats as stats \n",
        "  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html\n",
        "  df[\"Zscore_\"+Col] = stats.zscore(df[Col], axis=0, nan_policy='omit')\n",
        "\n",
        "  #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html \n",
        "  # df[\"ZscoreManuel\"+Col] = (df[Col] - df[Col].mean())/df[Col].std(ddof=0) # Calcul Zscore Manuel\n",
        "  \n",
        "  \n",
        "  # Definition Seuil IF Zscore\n",
        "  Za=2.58 ;ZaConf=\"99%\" # Seuil Intervale  confiance 99%  \n",
        "  Zb=1.96 ;ZbConf=\"95%\"# seuil Intervale  confiance95%    \n",
        "  Zc=1.65 ;ZcConf=\"90%\"# seuil Intervale confiance 90%   \n",
        "  # Plus l'intervale est grand plus on s'éloigne de la moyenne et plus la proba de contenir un outlier est élevée. \n",
        "  \n",
        "  \"\"\"Calcul de l’intervalle de confiance : Le principe général d’un intervalle de confiance consiste à déterminer, \n",
        "  à partir de ce qui a été observé dans un sous-échantillon, un intervalle dans lequel la grandeur que l’on étudie,\n",
        "  au sein de la population dont est extrait l’échantillon, a de fortes chances de se situer. En l’occurrence, \n",
        "  il s’agit de déterminer un intervalle, connaissant la proportion p observée dans l’échantillon, \n",
        "  au sein duquel la proportion π réelle de la population étudiée se situe avec une probabilité égale à une valeur fixée à l’avance, \n",
        "  usuellement 95 %, et notée 1-α.\"\"\"\n",
        "\n",
        " \n",
        "  # https://www.math.u-bordeaux.fr/~mchabano/Tab0.pdf\n",
        "  # Zd=1.65 ;ZcConf=\"99,9%\"# seuil confiance 99%   risque erreur α = 0.1%\n",
        "  # source : https://pro.arcgis.com/fr/pro-app/2.7/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm\n",
        "  # source2 : https://joseph.larmarange.net/?Intervalle-de-confiance-bilateral#:~:text=Le%20plus%20souvent%2C%20les%20intervalles,z%3D%201%2C95996398454%20%E2%89%88%201%2C960\n",
        "  # Tables loi normale :  https://blog.univ-reunion.fr/alessioguarino/files/2016/08/Tables-Loi-Normale-test-Z-Khi2-Student.pdf\n",
        "  #  np.abs(z_score) > threshold:\n",
        "\n",
        "\n",
        "  # Colonne Interpration Seuil IF Zscore \n",
        "  ColRefName=\"Zscore_\"+Col # Nom Colonne de référence pour la segmentation\n",
        "  ColCatName=\"Seuil_IConF_Zscore\" # Nom Colonne Categorie\n",
        "  df[ColCatName]=\"?\"\n",
        "  df[ColRefName].fillna(0,inplace=True)\n",
        "\n",
        "  df.loc[np.abs(df[ColRefName]) > Za , ColCatName] = \"Superieur à \"+ZaConf \n",
        "  df.loc[np.abs(df[ColRefName]) <= Za , ColCatName] = ZaConf    # utilisation de la Valeur Absolue (-za <= Seuil confiance <= Za )\n",
        "  df.loc[np.abs(df[ColRefName]) <= Zb , ColCatName] = ZbConf\n",
        "  df.loc[np.abs(df[ColRefName]) <= Zc , ColCatName] = ZcConf\n",
        "\n",
        "  df.sort_values(ColRefName, inplace=True)\n",
        "\n",
        "\n",
        "  # Colonne Interpretation Zscore \n",
        "   \n",
        "  zscoreI=3  # Outliers probable ? \n",
        "  zscoreII=2 # Outliers possible ?\n",
        "\n",
        "  # Colonne Interpration  Zscore  : Outliers\n",
        "  ColRefName=\"Zscore_\"+Col # Nom Colonne de référence pour la segmentation\n",
        "  ColCatName=\"Outiers_Zscore\" # Nom Colonne Categorie\n",
        "  df[ColCatName]=\"?\"\n",
        "  df[ColRefName].fillna(0,inplace=True)\n",
        "\n",
        "  df.loc[np.abs(df[ColRefName]) >= zscoreI , ColCatName] = \"Outlier probable  : Zscore >= à +/-\"+str(zscoreI)   # utilisation de la Valeur Absolue (-za <= Seuil confiance <= Za )\n",
        "  df.loc[np.abs(df[ColRefName]) >= zscoreII , ColCatName] = \"Outlier possible : Zscore >= à +/-\"+str(zscoreII) \n",
        "  \n",
        "\n",
        "  df.sort_values(ColRefName, inplace=True)\n",
        "\n",
        "\n",
        "  print(ColRefName,ColCatName)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3vKn5GTWKyN"
      },
      "source": [
        "####  Standardisation loi Normale :  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1d00EySXRdi"
      },
      "outputs": [],
      "source": [
        "# Standadisation d'une loi normale N(µ,σ)  :  Ramener un loi normale  à la la loi normale centrée réduite : N (0, 1)\n",
        "\n",
        "def Standardisation(X,µ,σ):\n",
        "  # Standardisation Formula :   \n",
        "  Z=(X-µ)/σ  # Z : Zscore  X:observation   µ: moyenne   σ : Ecart type\n",
        "\n",
        "  #https://www.youtube.com/watch?v=2tuBREK_mgE\n",
        "  #https://www.youtube.com/watch?v=mtbJbDwqWLE  : 68-95-99.7 Rule (5.2)\n",
        "  # Table Loi Normale Centrale reduite "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOSElyVgigUg"
      },
      "source": [
        "####  Description loi Normale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys7NgLMZzuc6"
      },
      "outputs": [],
      "source": [
        "def DescribeLoiNormale(df,Col): \n",
        " df_name = get_df_name(df)\n",
        "\n",
        " sns.histplot(df[Col], kde=True)\n",
        " \n",
        " print(pcolors.OK +\"Moyenne :\"+pcolors.RESET,df[Col].mean())\n",
        " print(pcolors.OK +\"Mediane :\"+pcolors.RESET,df[Col].median())\n",
        " print(pcolors.OK +\"Mode :\"+pcolors.RESET,df[Col].mode())\n",
        " print(pcolors.OK +\"Variance Corrigée :\"+pcolors.RESET,df[Col].var(ddof=0))\n",
        " print(pcolors.OK +\"Ecart type Corrigée :\"+pcolors.RESET,df[Col].std(ddof=0))\n",
        " print(pcolors.OK +\"Skew :\"+pcolors.RESET,df[Col].skew())\n",
        " print(pcolors.OK +\"Kurtosis :\"+pcolors.RESET,df[Col].kurtosis())\n",
        " df[Col].hist() ; plt.show()\n",
        "\n",
        " # Skewness empirique  : Mesure asymétrie Distribution\n",
        " γs = df[Col].skew()\n",
        "  #  γs== 0 #alors la distribution est symétrique.\n",
        "  #  γs>0 #alors la distribution est étalée à droite.\n",
        "  #  γs<0 #alors la distribution est étalée à gauche.\n",
        "  # Interpration Skewness  \n",
        " conditionlist = [\n",
        "    γs== 0, #alors la distribution est symétrique.\n",
        "    γs>0, #alors la distribution est étalée à droite.\n",
        "    γs<0 #alors la distribution est étalée à gauche \n",
        "    ]\n",
        " choicelist = ['la distribution est symétrique par rapport à la moyenne','Symétrie :  la distribution est étalée à droite','Symétrie :la distribution est étalée à gauche']\n",
        "\n",
        " Skewness_empiriqueResultat = np.select(conditionlist, choicelist, default='?')\n",
        " print(Skewness_empiriqueResultat)\n",
        "\n",
        "\n",
        " # Kurtosis empirique : Mesure Applatissement\n",
        " # L’aplatissement peut s’interpréter à la condition que la distribution soit symétrique !\n",
        " γk = df[Col].kurtosis() \n",
        " #  γk==0 # alors la distribution a le même aplatissement que la distribution normale.\n",
        " #  γk>0 # alors elle est moins aplatie que la distribution normale : les observations sont plus concentrées.\n",
        " #  γk<0 # alors les observations sont moins concentrées : la distribution est plus aplati\n",
        " \n",
        " conditionlist = [\n",
        "    γk == 0, # alors la distribution a le même aplatissement que la distribution normale.\n",
        "    γk>0, # alors elle est moins aplatie que la distribution normale : les observations sont plus concentrées.\n",
        "    γk<0 # alors les observations sont moins concentrées : la distribution est plus aplatie\n",
        "    ]\n",
        "\n",
        " choicelist = [\"Coefficient d'aplatissement (kurtosis): La distribution a le même aplatissement que la distribution normale centrée reduite.\",\n",
        "               \"La distribution est moins aplatie que la normale centrée reduite: les observations sont plus concentrées autour de la moyenne\",\n",
        "               \"Coefficient d'aplatissement (kurtosis) La distribution est la distribution est plus aplatie que la loi normale centrée reduite: les observations sont moins concentrées\"]\n",
        " \n",
        "\n",
        "\n",
        " Kurtosis_empiriqueResultat = np.select(conditionlist, choicelist, default='?')\n",
        " print(Kurtosis_empiriqueResultat)\n",
        "\n",
        " # Updgrade : Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll7rPM_NnUMf"
      },
      "source": [
        "#### Tests Normalité :  TestNorm(data,α)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78yHlQMRPA5v"
      },
      "outputs": [],
      "source": [
        "def QuantileQuantilePlot(data):\n",
        " \"\"\" X= df.Col  or X = pandas series\n",
        " Tracé des graphiques : line=\"s\" permet de comparer à une distribution normale avec la même moyenne et le même écart-type  que la variable \"\"\"\n",
        " \n",
        " #  Version 1 avec statsmodel\n",
        " # Tracé des graphiques : line=\"s\" permet de comparer à une distribution normale avec la même moyenne et le même écart-type  que la variable\n",
        " #  fig = sm.qqplot(data, line=\"s\",fit=True)\n",
        " #  plt.show()\n",
        " # source :  https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html\n",
        "\n",
        " # Version 2 avc pinguin-stats\n",
        " fig = pg.qqplot(data, dist='norm')\n",
        " plt.show()\n",
        " \n",
        " # https://pingouin-stats.org/generated/pingouin.qqplot.html#pingouin.qqplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test normalité d'un échantillon \n",
        "def TestNorm(data,α=0.05):\n",
        "\n",
        " \"\"\" Test Normalité d'un echantillon  data array ou df[Col] \n",
        "  Kolmogorov-Smirnov, Agostino’s K-squared,  Shapiro-Wilk,  test Khi carré our Chi square\"\"\"\n",
        "\n",
        " N = len(data) # Taille échantillon \n",
        " # α=0.050 # niveau alpha 0.05 / 5% par defaut \n",
        "\n",
        " # Plots \n",
        " sns.histplot(data, kde=True)\n",
        " # print(\"\\n\")\n",
        " QuantileQuantilePlot(data)\n",
        "\n",
        " # Choix du test Statistique \n",
        "\n",
        "\n",
        " # Hypotheses test Satistique\n",
        " print(f\"{pcolors.WARNING}TEST alpa {round(α*100,3)}%{pcolors.RESET}\")\n",
        " print() \n",
        "\n",
        " print(f\"{pcolors.WARNING}Hypothèse test normalité{pcolors.RESET}\")\n",
        " print( \"H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha\")    \n",
        " print(\"H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalelement distribué\")\n",
        " print()\n",
        "\n",
        " if N < 50 :\n",
        "   #__________________________________________________________ \n",
        "   #The Shapiro-Wilk test is a test of normality. It is used to determine whether or not a sample comes from a normal distribution.\n",
        "   #https://fr.wikipedia.org/wiki/Test_de_Shapiro-Wilk\n",
        "   #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
        "\n",
        "   TestSW = stats.shapiro(data)\n",
        "   # Test Non parametrique sur des variables quantitatives\n",
        "   # Plus pertinant sur les petits echantillon que Kolmogorov-Smirnov\n",
        "   # PValue  noZ fiable si N>5000 \n",
        "   # H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     \n",
        "   # H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalement distribué\n",
        "   #Interpretation du test :  si la p-value <= α H0 validé au risque α  |  #  si la p-value > α H0 rejeté \n",
        "  \n",
        "   print(f\"{pcolors.OK}Test normalité Shapiro-Wilk:{pcolors.RESET}(petit échantillon <50 observations) W: {TestSW[0]} p-value:{TestSW[1]}N:{N}\")\n",
        "   if TestSW[1]<= α: \n",
        "    print(f\"H0 validée ({round(TestSW[1],4)} <= {α}): L'échantillon est issu d'une population normalement distribuée au risque alpha (α:{α*100}%) N:{N}\")\n",
        "   else:\n",
        "    print(f\"H0 rejetée , H1  retenue : La valeur de la p-value obtenue ({round(TestSW[1],4)} > {α}) ne présupose en rien de la nature de la distribution des données.\")\n",
        "\n",
        "\n",
        "   print(\"\\n\")\n",
        " \n",
        " else :   \n",
        "   #__________________________________________________________ \n",
        "   # Test normalité Spicy Stat : Agostino’s K-squared \n",
        "   # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html\n",
        "   # D’Agostino’s K-squared test check’s normality of a variable based on skewness and kurtosis. It was named by Ralph D’Agostino\n",
        "   # Moins sensible aux  ex aequo que Kolmogorov-Smirnov\n",
        "   # ne fonctionne pas bien avec échantillons trop petits  N< 20   ( Ok car toust les N<50 sont testé avec Shapiro-Wilk )\n",
        "   TestN = stats.normaltest(data, axis=0, nan_policy='propagate') \n",
        "   print(f\"{pcolors.OK}Test normalité Agostino’s K-squared :{pcolors.RESET} {TestN} Echantillon de taile moyenne (>20) à grande) N:{N}\")\n",
        "   if TestN[1]<= α: \n",
        "      print(f\"H0 validée ({round(TestN[1],4)} <= {α}): L'échantillon est issu d'une population normalement distribuée au risque alpha (α:{α*100}%) N:{N}\")\n",
        "   else:\n",
        "     print(f\"H0 rejetée , H1  retenue : La valeur de la p-value obtenue ({round(TestN[1],4)} > {α}) ne présupose en rien de la nature de la distribution des données.\")\n",
        " print(\"\\n\") "
      ],
      "metadata": {
        "id": "LzBtvedi71fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcbyLxwCJzg6"
      },
      "outputs": [],
      "source": [
        "# Test normalité d'un échantillon \n",
        "def TestNormGlobal(data,α):\n",
        " \"\"\" Test Normalité d'un echantillon  data array ou df[Col] \n",
        "  Kolmogorov-Smirnov, Agostino’s K-squared,  Shapiro-Wilk,  test Khi carré our Chi square\"\"\"\n",
        "\n",
        " N = len(data) # Taille échantillon \n",
        " # α=0.050 # niveau alpha 0.05 / 5% par defaut \n",
        "\n",
        "\n",
        "\n",
        " # Plots \n",
        " sns.histplot(data, kde=True)\n",
        " # print(\"\\n\")\n",
        " QuantileQuantilePlot(data)\n",
        "\n",
        "\n",
        " # Hypotheses test Satistique\n",
        " print(pcolors.WARNING +\"TEST alpa \",round(α*100,3),\"%\"+pcolors.RESET)\n",
        " print() \n",
        "\n",
        " print(pcolors.WARNING+\"Hypothèse test normalité\"+pcolors.RESET)\n",
        " print( \"H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha\")    \n",
        " print(\"H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalelement distribué\")\n",
        " print()\n",
        "\n",
        " #__________________________________________________________   \n",
        " #Test de Kolmogorov-Smirnov\n",
        " #https://fr.wikipedia.org/wiki/Test_de_Kolmogorov-Smirnov?tableofcontents=0\n",
        " # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html\n",
        "\n",
        " # Test Non parametrique sur des variables quantitatives\n",
        " #Attention test peut efficae dans les queues d e distribution  \n",
        " # Plus pertinant sur les petit echantillon\n",
        " # H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     \n",
        " # H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalelement distribué\n",
        " #Interpretation du test : \n",
        " #  si la p-value est inférieure à un niveau alpha choisi (par exemple 0.05), alors l'hypothèse nulle est validé au risque alpha .\n",
        " #  si la p-value est supérieure au niveau alpha choisi (par exemple 0.05), alors on ne doit pas rejeter l'hypothèse nulle. La valeur de la p-value alors obtenue ne présuppose en rien de la nature de la distribution des données.\n",
        "\n",
        " TestKS = stats.kstest(data, 'norm', alternative='two-sided')  #alternative {‘two-sided’, ‘less’, ‘greater’},\n",
        "\n",
        " # print(TestKS ,TestKS[0],TestKS[1])\n",
        " # print(TestKS)\n",
        " print(pcolors.OK +\"Test normalité Kolmogorov-Smirnov:\"+pcolors.RESET,TestKS ,\"N:\",N)\n",
        " if TestKS[1]<= α: \n",
        "   print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%) N:\",N)\n",
        " else:\n",
        "    print(\"La valeur de la p-value obtenue (\",round(TestKS[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\")\n",
        " print(\"\\n\") \n",
        " \n",
        " #__________________________________________________________ \n",
        " # Test normalité Spicy Stat : Agostino’s K-squared \n",
        " # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html\n",
        " # D’Agostino’s K-squared test check’s normality of a variable based on skewness and kurtosis. It was named by Ralph D’Agostino\n",
        " # N > 20 de préférence\n",
        "\n",
        " if N > 20 :\n",
        "  TestN = stats.normaltest(data, axis=0, nan_policy='propagate')\n",
        "\n",
        "  print(pcolors.OK +\"Test normalité Agostino’s K-squared :\"+pcolors.RESET,TestKS ,\" Echantillon de taile moyenne (>20) à grande) N:\",N)\n",
        "  if TestN[1]<= α: \n",
        "    print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%) N:\",N)\n",
        "  else:\n",
        "    print(\"La valeur de la p-value obtenue (\",round(TestN[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\")\n",
        " else : \n",
        "   print(\"Echantillon N trop petit (\",N,\"<20), \",pcolors.WARNING + \"Le test d'Agostino’s K-squared  n'est pas applicable\"+pcolors.RESET)\n",
        "\n",
        " print(\"\\n\") \n",
        " \n",
        " #__________________________________________________________ \n",
        "  #The Shapiro-Wilk test is a test of normality. It is used to determine whether or not a sample comes from a normal distribution.\n",
        "  #https://fr.wikipedia.org/wiki/Test_de_Shapiro-Wilk\n",
        "  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
        "\n",
        " TestSW = stats.shapiro(data)\n",
        " # Test Non parametrique sur des variables quantitatives\n",
        " # Plus pertinant sur les petits echantillon que Kolmogorov-Smirnov\n",
        " # PValue  non fiable si N>5000 \n",
        " # H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     \n",
        " # H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalement distribué\n",
        " #Interpretation du test :  si la p-value <= α H0 validé au risque α  |  #  si la p-value > α H0 rejeté \n",
        " if N < 50 :\n",
        "  print(pcolors.OK +\"Test normalité Shapiro-Wilk:\"+pcolors.RESET,\"(petit échantillon <50 observations) W:\", TestSW[0],\" p-value:\",TestSW[1],\"N:\",N)\n",
        "  if TestSW[1]<= α: \n",
        "   print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%)\")    \n",
        "  else:\n",
        "   print(\"La valeur de la p-value obtenue (\",round(TestSW[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\")\n",
        " else :\n",
        "   print(pcolors.OK +\"Test normalité Shapiro-Wilk:\"+pcolors.RESET,pcolors.WARNING +\"(\",N, \">50 observations !)\"+pcolors.RESET,\" W:\", TestSW[0],\" p-value:\",TestSW[1],\"N:\",N)\n",
        "   if TestSW[1]<= α: \n",
        "    print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%)\",pcolors.WARNING +\" Attention fiabilité ? : Trop d'observations\"+pcolors.RESET,\"(\",N, \">50 )\")    \n",
        "   else:\n",
        "    print(\"La valeur de la p-value obtenue (\",round(TestSW[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\",pcolors.WARNING +\" Attention fiabilité ? : Trop d'observations\"+pcolors.RESET,\"(\",N, \">50 )\")\n",
        "\n",
        " print(\"\\n\")\n",
        "\n",
        " #  #__________________________________________________________  \n",
        " #  # test Khi carré our Chi square     ANNULE car peut performant \n",
        " #  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html\n",
        " #  # https://oraprdnt.uqtr.uquebec.ca/Gscdepot/paf1010/18/M12.pdf\n",
        "\n",
        " #  TestChiSquare = stats.chisquare(data, axis=None)\n",
        "\n",
        " #  print(pcolors.OK +\"Test normalité Chi square:\"+pcolors.RESET,\" ( Observations >0 & Max 20% <5)\", \"res.:\", TestChiSquare[0],\" p-value:\",TestChiSquare[1],\"N:\",N)\n",
        " #  if TestSW[1]>= α: \n",
        " #    print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%)\")    \n",
        " #  else:\n",
        " #   print(\"La valeur de la p-value obtenue (\",round(TestChiSquare[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\")\n",
        "\n",
        " #  print(\"\\n\")\n",
        " \n",
        "\n",
        "\n",
        "#  #__________________________________________________________  \n",
        "#  #Le test de Lilliefors:  #https://lemakistatheux.wordpress.com/2013/05/09/le-test-de-kolmogorov-smirnov/\n",
        "#  # A utiliser si la moyenne et l'ecart type ne sont pas connus\n",
        "\n",
        "#  from statsmodels.stats.diagnostic import lilliefors\n",
        "\n",
        "#  # H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     \n",
        "#  # H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population normalement distribué\n",
        "#  # If the pvalue is lower than some threshold, e.g. 0.05, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
        "#  # p-value may not be accurate for N > 5000.\n",
        "  \n",
        "#  if N < 5000 :\n",
        "#    # https://docs.python.org/fr/3.5/tutorial/errors.html\n",
        "#    # https://www.delftstack.com/fr/howto/python/manually-raise-exceptions-in-python/\n",
        "\n",
        "#    print(\"test lilliefor possible \")\n",
        "#    TestLF = lilliefors(data, dist='norm', pvalmethod='table')     #{‘norm’, ‘exp’}, optional\n",
        "#    # https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.lilliefors.html\n",
        "#    # https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.lilliefors.html?highlight=lilliefors\n",
        "   \n",
        "#    print(pcolors.OK +\"Test normalité Lilliefors:\"+pcolors.RESET,\" (ni espérance μ ni l'écart type σ ne sont connus) ksstat:\", TestLF[0],\" p-value:\",TestLF[1],\"N:\",N)\n",
        "#    if TestLF[1]<= α: \n",
        "#       print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%)\")\n",
        "#    else:\n",
        "#       print(\"La valeur de la p-value obtenue (\",round(TestLF[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\") \n",
        "\n",
        "#    #  try:\n",
        "#    #   TestLF = lilliefors(data, dist='norm', pvalmethod='table')\n",
        "#    #   pass  \n",
        "#    #   print(TestLF)\n",
        "#    #  except Exception as e:\n",
        "#    #   print(pcolors.WARNING + \"Exception Test normalité Lilliefors : \" + repr(e)) + pcolors.RESET; TestLF=[0,1] \n",
        "\n",
        "#    #   # Fin gestiion exceptin \n",
        "\n",
        "#    #   #  TestLF = lilliefors(data, dist='norm', pvalmethod='table')     #{‘norm’, ‘exp’}, optional\n",
        "#    #   # https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.lilliefors.html\n",
        "#    #   # https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.lilliefors.html?highlight=lilliefors\n",
        "\n",
        "#    #   print(pcolors.OK +\"Test normalité Lilliefors:\"+pcolors.RESET,\" (ni espérance μ ni l'écart type σ ne sont connus) ksstat:\", TestLF[0],\" p-value:\",TestLF[1],\"N:\",N)\n",
        "#    #   if TestLF[1]<= α: \n",
        "#    #     print(\"L'échantillon est issu d'une population normalement distribuée au risque alpha (α\",α*100,\"%)\")\n",
        "#    #   else:\n",
        "#    #     print(\"La valeur de la p-value obtenue (\",round(TestLF[1],4) ,\">\",α,\"') ne présupose en rien de la nature de la distribution des données.\")     \n",
        "#  else: \n",
        "#     print(pcolors.WARNING +\"Echantillon N trop grand\"+pcolors.RESET, \"(\",N,\">5000), \",pcolors.OK + \"Le test de Lilliefors n'est pas applicable\"+pcolors.RESET)\n",
        "   \n",
        "   \n",
        "\n",
        " print(\"\\n\")\n",
        " \n",
        "  #__________________________________________________________  \n",
        "  # #Le test de Kuiper ( Non parametrique, plus sensible que le Kolmogorov-Smirnov sur les queue de distributions)\n",
        "  # # Ne fonctionne pas avec les loi de poisson \n",
        "  # # https://docs.astropy.org/en/stable/api/astropy.stats.kuiper.html\n",
        " \n",
        " \n",
        "\n",
        " #__________________________________________________________  \n",
        " #https://fr.wikipedia.org/wiki/Test_statistique\n",
        " # https://lemakistatheux.wordpress.com/2013/05/09/le-test-de-kolmogorov-smirnov/\n",
        " # https://towardsdatascience.com/normality-tests-in-python-31e04aa4f411"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaWRfT1pJh7A"
      },
      "source": [
        "###### Test distributions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ek3bYUOTvlM"
      },
      "outputs": [],
      "source": [
        "# DEBUG data distributions test  Simul  Variable quantitatives\n",
        "\n",
        "# Choisir une distrubution Test \n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#Loi Normale \n",
        "#np.random.normal(mu, sigma, 1000) https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n",
        "# data= np.random.normal(0, 1, 100) ; label=\"Loi Normale Centrée Réduite\" \n",
        "# data= np.random.normal(80, 9, 6000) ; label=\"Loi Normale N6000\"\n",
        "from random import gauss\n",
        "# data = [gauss(100,15) for i in range(300)] ; label=\"Loi Gauss N300\"\n",
        "# data = [gauss(100,15) for i in range(100)]  ; label=\"Loi Normale N100\"\n",
        "\n",
        "#Loi Binomiale\n",
        "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.binomial.html\n",
        "# n, p = 1000, .5  # number of trials, probability of each trial ;\n",
        "# data = np.random.binomial(n, p, 5) ; label=\"Loi Binon\"  \n",
        "\n",
        "#Loi de Poisson\n",
        "# https://fr.wikipedia.org/wiki/Loi_de_Poisson  #https://blog.minitab.com/fr/que-sont-les-tests-de-poisson-a-un-ou-deux-echantillons#:~:text=Ces%20tests%20permettent%20d'effectuer,d'une%20loi%20de%20Poisson.\n",
        "# data = np.random.poisson(30, 100) ; label=\"Loi Poisson N13\"\n",
        "# data = np.random.poisson(30,1000); label=\"Loi Poisson N1000\"\n",
        "\n",
        "\n",
        "#Loi de Student\n",
        "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_t.html\n",
        "# data = np.random.standard_t(10, size=40)\n",
        "\n",
        "\n",
        "#Distibution aléatoire \n",
        "# random.randint(low, high=None, size=None, dtype=int)  https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html\n",
        "# data = np.random.randint(0, high=1, size=500, dtype=int) ;label=\"Radom 0-1 int\" \n",
        "# data = np.random.randint(-50, high=1000000, size=10, dtype=int) ;label=\"Radom  int\" \n",
        "data =np.random.random(1000) ;label=\"Radom  int\"\n",
        "# data = np.random.randn(100) ;label=\"Radom  int\" \n",
        "\n",
        "#Distribution Exponentielle \n",
        "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.exponential.html\n",
        "# data = np.random.exponential(scale=3.0, size=50) ;label=\"Exponentielle\"\n",
        "\n",
        "#Distribution Pareto \n",
        "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.pareto.html\n",
        "# a, m = 3., 2.  # shape and mode\n",
        "# data = (np.random.pareto(a, 100) + 1) * m ;label=\"Pareto\"\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# print(pcolors.FAIL+\"Distrution Test :\"+pcolors.RESET,label)\n",
        "# sns.histplot(data, kde=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################    Uncoment  for  testing   #############################################\n",
        "# Test\n",
        "# αListe = (0.0001,0.05,0.1,0.9999)\n",
        "# for α in αListe :\n",
        "#   print(pcolors.FAIL+\"Distrution Test :\"+pcolors.RESET,label)\n",
        "#   TestNorm(data,α)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00I-_jUO1c9"
      },
      "source": [
        "#### Regression linéaires Pearson & Spearman (correlation entre variables quantitatives)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6mPhRMiPHKz"
      },
      "outputs": [],
      "source": [
        "def CoefPS(df,dataA,dataB,α):\n",
        "  \"\"\" Coef correlation linéaire Pearson & Spearman from spicy stats\n",
        "    df: dataframe  - dataX : df.col  or array   - dataY: df.col or array \n",
        "  \"\"\"\n",
        "  A = df[dataA]\n",
        "  B = df[dataB]\n",
        "  alpha = α \n",
        "\n",
        "  # Prerequis pearson  : A & B de disutirbution normale \n",
        "\n",
        "  # Prerequis Spearman   : Non parametrique  & N >= 500 \n",
        "   \n",
        "\n",
        "\n",
        "  # Hypothèses Test Correlation\n",
        "  print(pcolors.WARNING + \"H₀: Il n'y pas de correlation entre les deux variables quantitatives\")\n",
        "  print(\"H₁: Les deux variables quantitatives sont corrélées\"+pcolors.RESET)\n",
        "  conclusionH1 = \"L'hypthèse nulle (H₀)  est rejettée au risque α \"+str(α*100)+\"%, l'hypothèse alternative (H1) est validée  : les deux variables sont corrélées.\" # Null Hypothesis is rejected.\"\n",
        "  conclusionH0 = \"L'hypthèse nulle (H₀)  est validée au risque α \"+str(α*100)+\"% : les deux variables ne sont pas corrélées.\" # Failed to reject the null hypothesis\n",
        "  print()\n",
        "\n",
        "  # Coefficient de corrélation linéaire de Pearson\n",
        "  coef_Pearson = stats.pearsonr(A,B)\n",
        "  p = coef_Pearson[1] # p-value coef_Pearson\n",
        "  print(f\"Le coefficient de corrélation linéaire de Pearson est de {coef_Pearson}\")\n",
        "  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
        "  conclusion = \"p-value <\"+str(alpha*100)+\"% :\"+conclusionH0 # Failed to reject the null hypothesis\n",
        "  if p >= alpha:\n",
        "   conclusion =  \"p-value >=\"+str(alpha*100)+\"% :\"+conclusionH1 # Null Hypothesis is rejected.\"\n",
        "  print(pcolors.OK +conclusion+pcolors.RESET)\n",
        "\n",
        "  print()\n",
        "  # Coefficient de corrélation linéaire de Spearman\n",
        "  coef_Spearman = stats.spearmanr(A,B)\n",
        "  p = coef_Spearman[0] # P-value coef_Spearman\n",
        "  print(f\"Le coefficient de corrélation linéaire de Spearman est de {coef_Spearman}\")\n",
        "  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
        "  conclusion = \"p-value <\"+str(alpha*100)+\"% :\"+conclusionH0 # Failed to reject the null hypothesis\n",
        "  if p >= alpha:\n",
        "   conclusion =  \"p-value >=\"+str(alpha*100)+\"% :\"+conclusionH1 # Null Hypothesis is rejected.\"\n",
        "  print(pcolors.OK +conclusion+pcolors.RESET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sugqr1WuYI2d"
      },
      "source": [
        "#### Test Correlation  Var Qualitative:  Test de χ² (khi2 - chi2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRiZRJcjUK0d"
      },
      "outputs": [],
      "source": [
        "def TableauContingence(df,Col_I,Col_II):\n",
        " \"\"\"Afficher un tableau de contigence pour etudier la relation entre deux variabes qualitatives TableauContingence(df,Col_I,Col_II)   \n",
        " df : DataFrame   \"Col_I\" : Colonne1  \"Col_II\" : Colonne2  \"\"\"\n",
        " df_name = get_df_name(df)\n",
        " X = Col_I ;  Y = Col_II\n",
        " df[Col_I]\n",
        "\n",
        " ContTable = df[[X,Y]].pivot_table(index=X,columns=Y,aggfunc=len,margins=True,margins_name=\"Total\").fillna(0)\n",
        " print(pcolors.OK +\"Tableau contigence : \"+pcolors.RESET,df_name,\"(\",Col_I,\"&\",Col_II,\")\")\n",
        " print(\"_________________________________________________________________\")\n",
        "\n",
        " \n",
        " df = ContTable.copy()\n",
        " #  df.columns=df.columns.to_flat_index() # colonnes => concatener le multi index en un index simple https://datascientyst.com/flatten-multiindex-in-pandas/\n",
        " df = FlatIndex(df)\n",
        " df=df.reset_index() # Conversion Index en colonnes simples   #4074 rows × 7 columns\n",
        " \n",
        " return ContTable\n",
        " \n",
        " #  sns.heatmap(df, annot=True, fmt=\"d\", linewidths=.5, ax=ax, cmap=\"YlGnBu\")\n",
        " # https://seaborn.pydata.org/generated/seaborn.heatmap.html # https://seaborn.pydata.org/tutorial/color_palettes.html\n",
        " #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZWxdiRyfVul"
      },
      "outputs": [],
      "source": [
        "#Test de χ² (khi2) a été réalisé dans le cas d’une corrélation entre deux variables qualitatives ;\n",
        "# Test du χ2 d'indépendance \n",
        "def  Khi2Test(df,Col_I,Col_II,α):\n",
        " print(pcolors.OK + \" Test Khi 2 : test indépandances deux variables qualitatives\"+pcolors.RESET)\n",
        " print(\" ----------------------------------------------------------------------\")\n",
        " \n",
        " # significance level\n",
        " alpha = α #0.05 \n",
        " X = Col_I ;  Y = Col_II\n",
        " #  TabContingence = TableauContingence(df,Col_I,Col_II)\n",
        " #  print(TabContingence)\n",
        " \n",
        " print()  \n",
        " print(\"Conditions d'utilisation :\")\n",
        " print(\" observations <= 5  limité  à  20% des cases  du tableau de contingence (Dans ce cas Utiliser un test non-parametrique : test exact de Fisher\")\n",
        " print() \n",
        "\n",
        " # create contingency table\n",
        " print(pcolors.OK +\"Tableau de Contingence : Effectifs observés\" +pcolors.RESET)\n",
        " print(\"\\n--------------------------------------------\")\n",
        " table_chi2View = pd.crosstab(df[X], df[Y],margins=True, margins_name=\"Total\") # Visualisation\n",
        " table_chi2 = pd.crosstab(df[X], df[Y]) #Calcul\n",
        "\n",
        "\n",
        " fig = px.imshow(table_chi2,\n",
        "                 y= df[X].unique(),\n",
        "                 title = \"Tableau de contingence Effectifs observés\",\n",
        "                 text_auto=True,\n",
        "                 width=800,height= 600)\n",
        " fig.update_xaxes(type='category')\n",
        " fig.update_yaxes(type='category')               \n",
        " #  fig.update_xaxes(side=\"top\")\n",
        " fig.show()  # https://plotly.com/python-api-reference/generated/plotly.express.imshow.html\n",
        "\n",
        "#  print(table_chi2View)\n",
        "\n",
        " print(\"\\n--------------------------------------------\")\n",
        " #  print(table_chi2View[Col_I].min(),table_chi2View[Col_II].min() )\n",
        " \n",
        " # test Conditons utilisation : \n",
        " n =0 ; liste = df[Y].unique() # ; print(liste)\n",
        " for l in liste:\n",
        "   #  print(table_chi2View[l].min() > 5)\n",
        "   test = table_chi2View[l].min() > 0 # DEbug ! \n",
        "   if test == False:\n",
        "     n = n+1\n",
        "     #  print(n)\n",
        " #  print(\"Valeur final n :\",n)\n",
        "\n",
        " if n == 0: \n",
        "   print(\"Il y a moins de 20%  des cases du tableau de contingence avec moisn de 5 observations, le test du χ² est donc possible.\")\n",
        "\n",
        "\n",
        "   print()\n",
        "   # Hypothèses Test χ²\n",
        "   print(pcolors.WARNING + \"H₀: les deux variables qualitatives(\",X,\"&\",Y,\") sont indépandantes\")\n",
        "   print(\"H₁: les deux variables qualitatives (\",X,\"&\",Y,\") sont liées de maniere sinificative\"+pcolors.RESET)\n",
        "   conclusionH1 = \"L'hypthèse nulle (H₀)  est rejettée au risque α \"+str(α*100)+\"%, les deux variables sont liées de manière significative.\" # Null Hypothesis is rejected.\"\n",
        "   conclusionH0 = \"L'hypthèse nulle (H₀)  est validée au risque α \"+str(α*100)+\"% : les deux variables sont indépandantes.\" # Failed to reject the null hypothesis\n",
        "  \n",
        "   \n",
        "  \n",
        "   \n",
        "   ## Chi2 Scpicy Stats --------------------------------------------------------------------\n",
        "   print()\n",
        "   chi2, p, dof, expected = stats.chi2_contingency(table_chi2.values)\n",
        "   #  https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html\n",
        "   print(\"chisquare-score (Valeur critique) : \", chi2)\n",
        "   print(\"p-value: \", p,\" α :\",alpha)\n",
        "   print(\"Degré de liberté :\",dof)\n",
        "   \n",
        "   conclusion = \"p-value >\"+str(alpha*100)+\"% :\"+conclusionH0 # Failed to reject the null hypothesis\n",
        "   if p <= alpha:\n",
        "     conclusion =  \"p-value <=\"+str(alpha*100)+\"% :\"+conclusionH1 # Null Hypothesis is rejected.\"\n",
        "   print(pcolors.OK +conclusion+pcolors.RESET)\n",
        "\n",
        "   # On compte le nombre de tuples du tableau\n",
        "   len_Col_I= table_chi2.shape[0]\n",
        "   len_Col_II= table_chi2.shape[1]\n",
        "   \n",
        "   # Calcul ecart entre les  observations et la distribution théorique\n",
        "   cs = np.zeros((len_Col_I, len_Col_II))\n",
        "   for ci in range(len_Col_I):\n",
        "     for cii in range(len_Col_II):\n",
        "        cs[ci ,cii] = (table_chi2.values[ci ,cii] - expected[ci ,cii])**2 / expected[ci ,cii]\n",
        "   \n",
        "   #  print(cs)\n",
        "   #  fig = px.imshow(cs, text_auto=True)\n",
        "   #  fig.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "   ## -------------------------------------------------------------------------------------\n",
        "   print()\n",
        "\n",
        "  \n",
        "   ## Chi2 manuel --------------------------------------------------------------------\n",
        "   print()\n",
        "\n",
        "  #  # Calcualtion of Chisquare\n",
        "  #  data_crosstab = table_chi2View\n",
        "  #  chi_square = 0\n",
        "  #  rows = df[Col_I].unique()\n",
        "  #  columns = df[Col_II].unique()\n",
        "  #  for i in columns:\n",
        "  #    for j in rows:\n",
        "  #       O = data_crosstab[i][j]\n",
        "  #       E = data_crosstab[i]['Total'] * data_crosstab['Total'][j] / data_crosstab['Total']['Total']\n",
        "  #       chi_square += (O-E)**2/E\n",
        "    \n",
        "      \n",
        "  #  # The p-value approach\n",
        "  #  print(pcolors.OK +\"Approche 1: Interprétation à partir de la  p-value\"+pcolors.RESET) # The p-value approach to hypothesis testing in the decision rule\n",
        "  #  p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "  #  conclusion =  conclusionH0 # Failed to reject the null hypothesis\n",
        "  #  if p_value <= alpha:\n",
        "  #    conclusion =  conclusionH1 # Null Hypothesis is rejected.\"\n",
        "  #  print(\"chisquare-score :\", chi_square, \" & p-value is:\", p_value)\n",
        "  #  print(conclusion)\n",
        "\n",
        "  #  # The critical value approach\n",
        "  #  print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "  #  print(pcolors.OK +\"Approche 2: Interprétation à partir de la valeur critique\"+pcolors.RESET) # The critical value approach to hypothesis testing in the decision rule\n",
        "  #  critical_value = stats.chi2.ppf(1-alpha, (len(rows)-1)*(len(columns)-1))\n",
        "  #  conclusion = conclusionH0  # Failed to reject the null hypothesis\n",
        "  #  if chi_square > critical_value:\n",
        "  #    conclusion =  conclusionH1 # Null Hypothesis is rejected.\"\n",
        "  #  print(\"chisquare-score :\", chi_square, \" & critical value :\", critical_value)\n",
        "  #  print(conclusion)\n",
        "  #  ## -------------------------------------------------------------------------------------\n",
        "   \n",
        "  #  # Confusion Matrix\n",
        "  #  print()\n",
        "  #  print(\"Confusion Matrix\")\n",
        "  #  confusion_matrix = pd.crosstab(df[Col_I], df[Col_I])\n",
        "  #  print(confusion_matrix)\n",
        "   \n",
        "\n",
        "  #  # first chose your category columns of interest\n",
        "  #  df = df[[Col_I, Col_II]]\n",
        "\n",
        "  #  # now change this to dummy variables, one-hot encoded:\n",
        "  #  DataMatrix = pd.get_dummies(df)\n",
        "\n",
        "  #   # plot as simply as:\n",
        "  #  plt.figure(figsize=(15,12))  # for large datasets\n",
        "  #  plt.title('Cramer\\'s V Comparaison '+Col_I+' et '+Col_II)\n",
        "  #  sns.heatmap(DataMatrix.corr('pearson'), cmap='coolwarm', center=0)\n",
        "  #  # https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
        "  \n",
        "  #  z = [[.1, .3, .5, .7, .9],\n",
        "  #    [1, .8, .6, .4, .2],\n",
        "  #    [.2, 0, .5, .7, .9],\n",
        "  #    [.9, .8, .4, .2, 0],\n",
        "  #    [.3, .4, .5, .7, 1]]\n",
        "\n",
        "  #  fig = px.imshow(z, text_auto=True)\n",
        "  #  fig.show()\n",
        "\n",
        "\n",
        "\n",
        " else: # Cas ou le χ² n'est pas applicable\n",
        "  print(\"Il y a plus de 20% des cases du tableau de contingence avec moins de 5 onservations, Utiliser un test non-parametrique : test exact de Fisher\")\n",
        "  # N2/k ≥ 10, où N est l'effectif total et k est toujours le nombre de catégories\n",
        "  \n",
        "  # Valeur théorique de chqye classe >=1 & 80% mini supérieur à 5 \n",
        "  # https://fr.wikipedia.org/wiki/Test_du_%CF%87%C2%B2\n",
        "\n",
        " #### SOUCRCE inspiration code https://towardsdatascience.com/chi-square-test-with-python-d8ba98117626 \n",
        " #https://fr.wikipedia.org/wiki/Test_du_%CF%87%C2%B2\n",
        " #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html?highlight=chi#scipy.stats.chi2_contingency\n",
        " \n",
        " # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html\n",
        " # https://www.ibm.com/docs/fr/spss-statistics/SaaS?topic=statistics-tests-independence-chi-square\n",
        " # https://lms.fun-mooc.fr/asset-v1:grenoblealpes+92001+session01+type@asset+block/mod6-cap2.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVuX-30tm-1j"
      },
      "source": [
        "#### Test Comparaison Plusieurs observations ANOVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgv1krgnndtf"
      },
      "outputs": [],
      "source": [
        "# Test Comparaison Plusieurs observations ANOVA (Quantitative & Qualitatif)\n",
        "def AnovaTest(df,Col_I,Col_II,α):\n",
        " \"\"\"\n",
        " Test Comparaison Plusieurs observations ANOVA (Quantitative & Qualitatif)\n",
        "  df : Dataframe*\n",
        "   Col_I : \"Colonne1VariableQuantitative\")\n",
        "   Col_II : \"Colonne1VariableQualitative\")\n",
        "   verifier nb >20 ou normalité\n",
        " \"\"\"\n",
        " print(pcolors.OK +\"Test ANOVA ( observation Quantitatives & Qualitatives)\"+pcolors.RESET)  \n",
        " print()\n",
        "\n",
        " #Conditions d'utilisation \n",
        " print(pcolors.WARNING +\"Conditions d'utilisation :\"+pcolors.RESET)\n",
        "\n",
        " print(\"* Les categories observées sont indépendantes (Un individu ne peut appartenir qu'a un seul groupe\")\n",
        "\n",
        " print(\"* Les échantillons ont des variances égales\") \n",
        "#  liste = df[Col_II].unique() # print(liste)\n",
        "#  for l in liste :\n",
        "#    result = df.loc[:,[Col_II]][ (df[Col_II] == l)] # print(result)\n",
        "#    print(\" -> Variance groupe \",l,\": \",np.var(result, ddof=1)[0])\n",
        "\n",
        " print(\"* Au moins 20 individus par échantillon, ou normalité des populations de chaque échantillon supposée ou vérifiée\")\n",
        "#  for l in liste :\n",
        "#    result = df.loc[:,[Col_II]][ (df[Col_II] == l)] # print(result)\n",
        "#    print(\" -> Nombre individus par groupe \",l,\": \",len(result))\n",
        "\n",
        " print(\" La variable qualitative contient plus de 3 groupes (nb groupe =2  utiliser Pair T-test) : \")\n",
        " nbgroupe = df[Col_II].unique()\n",
        " print(\" -> Nombre de groupes \",len(nbgroupe),\":\",nbgroupe)\n",
        " print()\n",
        "\n",
        " # Hypothèses Test ANOVA Oneway \n",
        " print(pcolors.WARNING + \"H₀: Les moyennes des groupes sont égales (pas de variation de la moyenne de chaque groupe\")\n",
        " print(\"H₁: Au moins , un des groupe à une moyenne différentes des autres groupes\"+pcolors.RESET)\n",
        " conclusionH1 = \"L'hypthèse nulle (H₀)  est rejettée au risque α \"+str(α*100)+\"%, Au moins , un des groupe à une moyenne différentes des autres groupes\" # Null Hypothesis is rejected.\"\n",
        " conclusionH0 = \"L'hypthèse nulle (H₀)  est validée au risque α \"+str(α*100)+\"% : Les moyennes des groupes sont égales.\" # Failed to reject the null hypothesis\n",
        "\n",
        "\n",
        "#  ## stats f_oneway functions takes the groups as input and returns ANOVA F and p value -------------------------------------------------------\n",
        "#  print(liste)\n",
        "#  listeGroupe = []\n",
        "#  for l in liste :\n",
        "#    result = \"G_\"+ str(l) # print(result)\n",
        "#    listeGroupe.append(result)\n",
        "#    print(result)\n",
        "#  print(listeGroupe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwHQaTBM7Ott"
      },
      "outputs": [],
      "source": [
        "def AnovaTestNormalité(df,Col_I,Col_II,α):\n",
        " liste = df[Col_II].unique() # print(liste)\n",
        " for l in liste :\n",
        "   result = df.loc[:,[Col_II]][ (df[Col_II] == l)] # print(result)\n",
        "   return TestNorm(result,α)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhc0-fK1PIGQ"
      },
      "outputs": [],
      "source": [
        "# test de Student \n",
        "# https://www.investopedia.com/terms/z/z-test.asp\n",
        "# https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/#:~:text=The%20number%20of%20degrees%20of,in%20both%20samples%2C%20minus%20two.&text=The%20critical%20value%20can%20be%20calculated%20using%20the%20percent%20point,0.05%20(95%25%20confidence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq-iSPEwYIaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a877939d-8ee8-4d60-b5fb-bcab2c2b53f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/__init__.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing\n"
          ]
        }
      ],
      "source": [
        "# https://kanoki.org/2019/11/18/how-to-create-dataframe-for-testing/\n",
        "df= pd.util.testing.makeMixedDataFrame()\n",
        "\n",
        "\n",
        "# print(df)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# dz=TableauContingence(df,\"C\",\"A\")\n",
        "# px.imshow(dz, text_auto=True)\n",
        "# # Khi2Test(df,\"C\",\"A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHltth4Mqqe"
      },
      "source": [
        "liste test a voir \n",
        "\n",
        "\n",
        "*   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "*  https://fr.wikipedia.org/wiki/Test_de_Bartlett\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bartlett.html\n",
        "\n",
        "* Statistical Power \n",
        "  * https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/\n",
        "  * https://www.geeksforgeeks.org/introduction-to-power-analysis-in-python/\n",
        "  * https://statsthinking21.github.io/statsthinking21-python/09-StatisticalPower.html\n",
        "  * https://towardsdatascience.com/introduction-to-power-analysis-in-python-e7b748dfa26\n",
        "\n",
        "* Plotly \n",
        "  * https://plotly.com/python/bar-charts/\n",
        "  * https://plotly.com/python-api-reference/generated/plotly.express.imshow.html\n",
        "\n",
        "*  Test Statistiques \n",
        " * https://help.xlstat.com/fr/6443-which-statistical-test-should-you-use\n",
        " * https://fr.wikipedia.org/wiki/Table_d%27utilisation_des_tests_statistiques\n",
        " * https://fr.wikipedia.org/wiki/Test_du_%CF%87%C2%B2\n",
        " * https://rplusplus.com/comment-choisir-le-bon-test-statistique/\n",
        " * Khi deux \n",
        "    * Test du χ2 d'adéquation\n",
        "    * Test du χ2 d'homogénéité\n",
        "      * https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html?highlight=chi#scipy.stats.chi2_contingency\n",
        "    * Test du χ2 d'indépendance\n",
        "    * https://fr.wikipedia.org/wiki/Test_du_%CF%87%C2%B2\n",
        "\n",
        "  * https://www.soft-concept.com/surveymag/principaux-test-statistiques-etudes.html\n",
        "  * https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-l-inf-tests.pdf\n",
        "  * https://bioinfo-fr.net/tests-statistiques-suivez-lguide \n",
        "\n",
        "\n",
        "\n",
        "* Moyenne mobile \n",
        "  * https://stackoverflow.com/questions/56911611/python-pandas-create-cumulative-average-while-grouping-by-other-column\n",
        " * https://www.statology.org/cumulative-average-python/\n",
        "\n",
        " * Rolling :\n",
        "   * https://www.geeksforgeeks.org/python-pandas-dataframe-rolling/  \n",
        "   * https://www.geeksforgeeks.org/python-pandas-dataframe-rolling/ \n",
        "   * https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n",
        "   * https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n",
        " * https://stackoverflow.com/questions/56911611/python-pandas-create-cumulative-average-while-grouping-by-other-column\n",
        "\n",
        "\n",
        "* Power analisis :  https://www.geeksforgeeks.org/introduction-to-power-analysis-in-python/df\n",
        "\n",
        "*   https://www.geeksforgeeks.org/introduction-to-power-analysis-in-python/df\n",
        "*   https://statsthinking21.github.io/statsthinking21-python/09-StatisticalPower.html\n",
        "* https://towardsdatascience.com/introduction-to-power-analysis-in-python-e7b748dfa26\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMJcOZ6unpxr"
      },
      "source": [
        "####  Mesures Concentration : Courbe Lorentz et indice de gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urWT5YoorIo2"
      },
      "outputs": [],
      "source": [
        "def Gini(array):\n",
        "    \"\"\"Calculate the Gini coefficient of a numpy array. \n",
        "    \"\"\"\n",
        "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
        "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
        "    # source : https://github.com/oliviaguest/gini\n",
        "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
        "    if np.amin(array) < 0:\n",
        "        array -= np.amin(array) #values cannot be negative\n",
        "    array += 0.0000001 #values cannot be 0\n",
        "    array = np.sort(array) #values must be sorted\n",
        "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
        "    n = array.shape[0]#number of array elements\n",
        "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient\n",
        "# https://www.insee.fr/fr/metadonnees/definition/c1551#:~:text=L'indice%20(ou%20coefficient),indice%20de%20Gini%20est%20%C3%A9lev%C3%A9. \n",
        "# Coef Gini indicateur synthétique permettant du niveau d'inégalité d'une variable sur pop. donnée  [ 0 (égalité parfaite) - 1   situation la plus inégalitaire possible, la variable vaut 0 sur toute la pop à l’exception d’un seul individu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfuX4j-r5nJa"
      },
      "outputs": [],
      "source": [
        "# Courbe de Lorentz\n",
        "def CourbeLorentz(data):\n",
        " ''' df.Colonne.to_numpy()  # convertir pandas.core.series.Series' to numpy.ndarray '''\n",
        " #  data = np.random.uniform(-1,0,10)\n",
        " array = data\n",
        "\n",
        " # Filtrer les valeurs > 0 \n",
        " filter_array = []  # Create an empty list\n",
        "\n",
        " for element in array: #   go through each element in array :  if the element > 0, set the value to True, otherwise False:\n",
        "   if element > 0:\n",
        "    filter_array.append(True)\n",
        "   else:\n",
        "    filter_array.append(False)\n",
        "\n",
        " data = array[filter_array] # Data is cleaned (all > 0 )\n",
        " #  print(data,len(data))\n",
        "\n",
        " n=len(data) \n",
        " #  print(n)\n",
        " lorenz = np.cumsum(np.sort(data)) / data.sum() # tri ordre croissant et division par total pour normaliser (0-1)\n",
        " lorenz = np.append([0],lorenz) # La courbe de Lorenz commence à 0\n",
        " \n",
        "\n",
        " # Plot avec style personalisé \n",
        " # print(plt.style.available)\n",
        " with plt.style.context(\"fivethirtyeight\"): \n",
        "  fig, ax = plt.subplots() \n",
        "   \n",
        "  xaxis = np.linspace(0-1/n,1+1/n,n+1) #Il y a un segment de taille n pour chaque individu, plus 1 segment supplémentaire d'ordonnée 0. Le premier segment commence à 0-1/n, et le dernier termine à 1+1/n.\n",
        "  x=np.linspace(0, 1, 10) ;  y=x # première bissectrice\n",
        "\n",
        "  plt.plot(xaxis,lorenz,drawstyle='steps-post',color=\"green\", label=\"Courbe Concentration\")  # Courbe de Lorentz\n",
        "  plt.plot(x, y, color=\"grey\",label=\"Equirépartition\" ) # Droite équirépartition\n",
        "  \n",
        "  # plt.axvline(x=1, ymin=0, ymax=1, color=\"grey\", ls='--' ,  linewidth=1)  # Barre verticale\n",
        "  # plt.axhline(y=1, xmin=0, xmax=1, color=\"grey\",ls='--',  linewidth=1)  # Barre horizontale \n",
        "\n",
        "  plt.axvline(x=1, ymin=0, ymax=1, color=\"grey\", ls='--' ,  linewidth=1)  # Barre verticale\n",
        "  plt.axhline(y=1, xmin=0, xmax=1, color=\"grey\",ls='--',  linewidth=1)  # Barre horizontale \n",
        "  \n",
        "  plt.vlines(x=.5, ymin=0, ymax=.5, color='blue', linestyle='--', linewidth=1, ) #label='Medial'\n",
        "  plt.hlines(xmin=1, xmax=0, y=.5, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "  plt.scatter(0,0.5 ) # Ajout d'un point  label=\"0.5\"\n",
        "  plt.scatter(0.5,0) # Ajout d'un point ,label=\"50% population\"\n",
        "\n",
        "  GiniTxt = \"Coef Gini : \" + round(Gini(array),3).astype(str)\n",
        "  plt.annotate(GiniTxt,(0.1,0.7)) # Ajout d'une annotation coef Gini\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  plt.ylabel('Ratio')  # Titre de l'axe y\n",
        "\n",
        "  label=\"population (n=\"+str(n)+\")\"\n",
        "  plt.xlabel(label)  # Titre de l'axe x\n",
        "  \n",
        "  \n",
        "  plt.xlim(0,1.1) # on limite l'axe des abscisses\n",
        "  plt.ylim(0,1.1) # On limites l'axe des ordonnées\n",
        "  # plt.axes().axis('equal')\n",
        "  # # plt.axes().axis('equal', 'auto-adjusted data limits')\n",
        "\n",
        "  from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
        " \n",
        "  ax.xaxis.set_major_locator(MultipleLocator(0.1))\n",
        "  # ax.xaxis.set_major_formatter(FormatStrFormatter('%'))\n",
        "  # ax.axis.set_minor_locator(MultipleLocator(0.05))\n",
        "  # https://matplotlib.org/3.1.1/gallery/ticks_and_spines/major_minor_demo.html\n",
        "  # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.tick_params.html\n",
        "  ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
        "\n",
        "  \n",
        "  plt.legend()\n",
        "  # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "  plt.title('Courbe Lorentz')\n",
        "  \n",
        "  plt.savefig(\"Courbe Lorentz.png\", dpi=300)\n",
        "\n",
        " \n",
        "  plt.show()\n",
        "  \n",
        "  AUC = (lorenz.sum() -lorenz[-1]/2 -lorenz[0]/2)/n # Surface sous la courbe de Lorenz. Le premier segment (lorenz[0]) est à moitié en dessous de 0, on le coupe donc en 2, on fait de même pour le dernier segment lorenz[-1] qui est à moitié au dessus de 1.\n",
        "  S = 0.5 - AUC # surface entre la première bissectrice et le courbe de Lorenz\n",
        "  GiniCoef = 2*S\n",
        "  print(pcolors.OK +\"Coef Gini :\"+pcolors.RESET,Gini(data))\n",
        "\n",
        "# data = np.random.uniform(0,10,10)\n",
        "# CourbeLorentz(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "186kuJ5lso5-"
      },
      "source": [
        "##### Test Lorentz & Gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS8gxH9rrglm"
      },
      "outputs": [],
      "source": [
        "# a = np.zeros((1000))   # dervait tendre vers 1 \n",
        "# a[0] = 1.0\n",
        "# print(a[:5],\"...\",a[-1:],\",Gini \",Gini(a),\"dervait tendre vers 1 \")\n",
        "\n",
        "# s = np.random.uniform(-1,0,1000)  # devrait être proche 0.33\n",
        "# print(s[:5],\"...\",s[-1:],\",Gini \",Gini(s),\"devrait être proche 0.33\")\n",
        "\n",
        "# b = np.ones((10)) ;b # # devrait tendre vers 0\n",
        "# print(b[:5],\"...\",b[-1:],\",Gini \",Gini(b),\"devrait tendre vers 0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9YURU8A6ck0"
      },
      "source": [
        "###  Exports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPMoKlT6vVd"
      },
      "source": [
        "#### Export DataFrame Excel /CSV :   ExportDF(df,file_name) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ExportDF(df,file_name):\n",
        " \"\"\" ExportXLS(df,file_name)   DataFrame et Nom du fichier  \n",
        "  Format Excel : .xls & .xlsx     ou Format CSV .csv (utF8 - separateur ;)\n",
        " le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   \"\"\"\n",
        " from google.colab import files\n",
        "\n",
        "\n",
        " ExportPath = ExportdfPATH(df)\n",
        " finalPath=f\"{ExportPath}{file_name}\"\n",
        " \n",
        " import re\n",
        " df_name = get_df_name(df)  # DataFrame Name \n",
        " testCSV=re.search('.csv', file_name) != None # True si file_name contient re.search # print(test)\n",
        " testXLS=re.search('.xls', file_name) != None # True si file_name contient re.search # print(test)\n",
        "\n",
        " # Specify the name of the file ( .xlx for Excel or .csv for text format)\n",
        " # file_name = 'Output.xlsx'\n",
        " if testCSV == True :\n",
        "  df.to_csv(finalPath , encoding='utf-8') # saving CSV\n",
        "  files.download(finalPath)\n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",finalPath)\n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
        "  \n",
        " elif testXLS == True :\n",
        "  df.to_excel(finalPath) # saving the excelsheet \n",
        "  # df.to_excel(file_name,sheet_name='Sheet1') # saving the excelsheet \n",
        "  files.download(finalPath)\n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",finalPath) \n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n",
        "  \n",
        " else :\n",
        "   print(\"Erreur convertion : \", df_name,'en :',file_name,\" vérifier le format du fichier .xls .xlsx  ou .csv\")\n",
        "\n",
        "\n",
        "# Ajouter Fonction  to html : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html?highlight=to_#pandas-dataframe-to-html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html\n",
        "\n",
        "# df.to_csv('FAO_PoliticalSabilityIndexMonde.csv', encoding = 'utf-8-sig') \n",
        "# files.download('FAO_PoliticalSabilityIndexMonde.csv')\n"
      ],
      "metadata": {
        "id": "lX-x3DCR8P90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZFxMmEDYWY6"
      },
      "outputs": [],
      "source": [
        "def ExportDfColab(df,file_name):\n",
        " \"\"\" ExportXLS(df,file_name)   DataFrame et Nom du fichier  \n",
        "  Format Excel : .xls & .xlsx     ou Format CSV .csv (utF8 - separateur ;)\n",
        " le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   \n",
        "  To save in specific folder :  include path in the FileName '../data/my_new_file.csv'\n",
        " \"\"\"\n",
        " from google.colab import files\n",
        " \n",
        " import re\n",
        " df_name = get_df_name(df)  # DataFrame Name \n",
        " testCSV=re.search('.csv', file_name) != None # True si file_name contient re.search # print(test)\n",
        " testXLS=re.search('.xls', file_name) != None # True si file_name contient re.search # print(test)\n",
        "\n",
        " # Specify the name of the file ( .xlx for Excel or .csv for text format)\n",
        " # file_name = 'Output.xlsx'\n",
        " if testCSV == True :\n",
        "  df.to_csv(file_name, encoding='utf-8') # saving CSV\n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",file_name)\n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
        "  \n",
        " elif testXLS == True :\n",
        "  df.to_excel(file_name) # saving the excelsheet \n",
        "  # df.to_excel(file_name,sheet_name='Sheet1') # saving the excelsheet \n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",file_name) \n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n",
        "  \n",
        " else :\n",
        "   print(\"Erreur convertion : \", df_name,'en :',file_name,\" vérifier le format du fichier .xls .xlsx  ou .csv\")\n",
        "\n",
        "\n",
        "# Ajouter Fonction  to html : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html?highlight=to_#pandas-dataframe-to-html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html\n",
        "# https://dataindependent.com/pandas/pandas-write-to-csv-pd-dataframe-to_csv/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcSdg9Nu6fAE"
      },
      "outputs": [],
      "source": [
        "def ExportDFV0(df,file_name):\n",
        " \"\"\" ExportXLS(df,file_name)   DataFrame et Nom du fichier  \n",
        "  Format Excel : .xls & .xlsx     ou Format CSV .csv (utF8 - separateur ;)\n",
        " le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   \"\"\"\n",
        " from google.colab import files\n",
        " \n",
        " import re\n",
        " df_name = get_df_name(df)  # DataFrame Name \n",
        " testCSV=re.search('.csv', file_name) != None # True si file_name contient re.search # print(test)\n",
        " testXLS=re.search('.xls', file_name) != None # True si file_name contient re.search # print(test)\n",
        "\n",
        " # Specify the name of the file ( .xlx for Excel or .csv for text format)\n",
        " # file_name = 'Output.xlsx'\n",
        " if testCSV == True :\n",
        "  df.to_csv(file_name, encoding='utf-8') # saving CSV\n",
        "  files.download(file_name)\n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",file_name)\n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
        "  \n",
        " elif testXLS == True :\n",
        "  df.to_excel(file_name) # saving the excelsheet \n",
        "  # df.to_excel(file_name,sheet_name='Sheet1') # saving the excelsheet \n",
        "  files.download(file_name)\n",
        "  print(\"Confirmation : \", df_name,\"exporté sous :\",file_name) \n",
        "  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n",
        "  \n",
        " else :\n",
        "   print(\"Erreur convertion : \", df_name,'en :',file_name,\" vérifier le format du fichier .xls .xlsx  ou .csv\")\n",
        "\n",
        "\n",
        "# Ajouter Fonction  to html : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html?highlight=to_#pandas-dataframe-to-html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html\n",
        "\n",
        "# df.to_csv('FAO_PoliticalSabilityIndexMonde.csv', encoding = 'utf-8-sig') \n",
        "# files.download('FAO_PoliticalSabilityIndexMonde.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liO4moBFHfvV"
      },
      "outputs": [],
      "source": [
        "#test \n",
        "# ExportDF(df,\"test.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQB4FC17Gj_2"
      },
      "source": [
        "###### Export Foction en cours de dev "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GegMonVPG6O"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# df = pd.DataFrame(np.random.random((10,3)), columns = (\"col 1\", \"col 2\", \"col 3\"))\n",
        "\n",
        "# #https://stackoverflow.com/questions/32137396/how-do-i-plot-only-a-table-in-matplotlib\n",
        "# fig, ax =plt.subplots(figsize=(12,4))\n",
        "# ax.axis('tight')\n",
        "# ax.axis('off')\n",
        "# the_table = ax.table(cellText=df.values,colLabels=df.columns,loc='center')\n",
        "\n",
        "# #https://stackoverflow.com/questions/4042192/reduce-left-and-right-margins-in-matplotlib-plot\n",
        "# pp = PdfPages(\"foo.pdf\")\n",
        "# pp.savefig(fig, bbox_inches='tight')\n",
        "# pp.close()\n",
        "# Complement : https://stackoverflow.com/questions/33155776/export-pandas-dataframe-into-a-pdf-file-using-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FCyi1QOP6A1"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import pdfkit as pdf\n",
        "# import sqlite3\n",
        "\n",
        "# con=sqlite3.connect(\"baza.db\")\n",
        "\n",
        "# df=pd.read_sql_query(\"select * from dobit\", con)\n",
        "# df.to_html('/home/linux/izvestaj.html')\n",
        "# nazivFajla='/home/linux/pdfPrintOut.pdf'\n",
        "# pdf.from_file('/home/linux/izvestaj.html', nazivFajla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC2bbEXlQ6_O"
      },
      "outputs": [],
      "source": [
        "# !pip install weasyprint\n",
        "# #  Create a pandas dataframe with demo data:\n",
        "# import pandas as pd\n",
        "# demodata_csv = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
        "# df = pd.read_csv(demodata_csv)\n",
        "\n",
        "# # Pretty print the dataframe as an html table to a file\n",
        "# intermediate_html = '/tmp/intermediate.html'\n",
        "# to_html_pretty(df,intermediate_html,'Iris Data')\n",
        "# # if you do not want pretty printing, just use pandas:\n",
        "# # df.to_html(intermediate_html)\n",
        "\n",
        "# # Convert the html file to a pdf file using weasyprint\n",
        "# import weasyprint\n",
        "# out_pdf= '/tmp/demo.pdf'\n",
        "# weasyprint.HTML(intermediate_html).write_pdf(out_pdf)\n",
        "\n",
        "# # This is the table pretty printer used above:\n",
        "\n",
        "# def to_html_pretty(df, filename='/tmp/out.html', title=''):\n",
        "#     '''\n",
        "#     Write an entire dataframe to an HTML file\n",
        "#     with nice formatting.\n",
        "#     Thanks to @stackoverflowuser2010 for the\n",
        "#     pretty printer see https://stackoverflow.com/a/47723330/362951\n",
        "#     '''\n",
        "#     ht = ''\n",
        "#     if title != '':\n",
        "#         ht += '<h2> %s </h2>\\n' % title\n",
        "#     ht += df.to_html(classes='wide', escape=False)\n",
        "\n",
        "#     with open(filename, 'w') as f:\n",
        "#          f.write(HTML_TEMPLATE1 + ht + HTML_TEMPLATE2)\n",
        "\n",
        "# HTML_TEMPLATE1 = '''\n",
        "# <html>\n",
        "# <head>\n",
        "# <style>\n",
        "#   h2 {\n",
        "#     text-align: center;\n",
        "#     font-family: Helvetica, Arial, sans-serif;\n",
        "#   }\n",
        "#   table { \n",
        "#     margin-left: auto;\n",
        "#     margin-right: auto;\n",
        "#   }\n",
        "#   table, th, td {\n",
        "#     border: 1px solid black;\n",
        "#     border-collapse: collapse;\n",
        "#   }\n",
        "#   th, td {\n",
        "#     padding: 5px;\n",
        "#     text-align: center;\n",
        "#     font-family: Helvetica, Arial, sans-serif;\n",
        "#     font-size: 90%;\n",
        "#   }\n",
        "#   table tbody tr:hover {\n",
        "#     background-color: #dddddd;\n",
        "#   }\n",
        "#   .wide {\n",
        "#     width: 90%; \n",
        "#   }\n",
        "# </style>\n",
        "# </head>\n",
        "# <body>\n",
        "# '''\n",
        "\n",
        "# HTML_TEMPLATE2 = '''\n",
        "# </body>\n",
        "# </html>\n",
        "# '''\n",
        "\n",
        "# https://stackoverflow.com/questions/33155776/export-pandas-dataframe-into-a-pdf-file-using-python?newreg=2926175bf7ab431a87ec344c0fb5e967"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uX82pruJMxc"
      },
      "source": [
        "#### Export Plolty to HTML :   ExportPlotly(fig,file_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-IEAwXpJcka"
      },
      "outputs": [],
      "source": [
        "# Exporte une figure plotly (fig) au format HTML :  file_name.html\n",
        "def ExportPlotly(fig,file_name):\n",
        " \"\"\"ExportPlotly(fig,file_name)\n",
        " Exporte une figure plotly (fig) au format HTML (file_name.html)\n",
        " le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   \"\"\"\n",
        "  \n",
        " #  path = f\"/content/Exports/\"\n",
        " #  os.mkdir(path)\n",
        "\n",
        " file_name = f\"{file_name}\"\n",
        " fig.write_html(file_name)\n",
        " print(f\">>>{file_name} Exporté\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ExportdfPATH(df):\n",
        " #  path = f\"/content/Exports/\"\n",
        " #  os.mkdir(path)\n",
        " import os # https://docs.python.org/fr/3/library/os.html\n",
        " ExportdfPATH = f\"/content/Exports/{get_df_name(df)}/\"\n",
        " os.makedirs(ExportdfPATH,exist_ok=True)\n",
        " print(f\"Repertoire : {ExportdfPATH}\")\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "gGW7k_oIRJIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ExportPloltyColab(df,col_Name_String,fig,file_name):\n",
        " import os # https://docs.python.org/fr/3/library/os.html\n",
        " path = f\"/content/Exports/{get_df_name(df)}\"\n",
        " os.makedirs( path , exist_ok=True)\n",
        "\n",
        " dfNameString = f\"{get_df_name(df)}\" ; print(f\"dfNameString :{dfNameString}\")\n",
        " finalPathLEBON = f\"{path}/{dfNameString}_{col_Name_String}_{file_name}\" ;print(f\"finalpath : {finalPathLEBON}\")\n",
        " ExportPlotly(fig,finalPathLEBON)"
      ],
      "metadata": {
        "id": "XBe3uuOyjhEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsOJ-ScGVwfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporte une figure plotly (fig) au format HTML :  /content/Exports/df/df_col_file_name.html\n",
        "def ExportPlotly_df_Name_StringPATH(df_Name_String,col_Name_String,fig,file_name):\n",
        " \"\"\"\n",
        " ExportPlotly_dfPATH(df,col,fig,file_name)\n",
        " Export Plolty graph in html in the folowing directoty (dynamic names  {})\n",
        "  \n",
        "  for Colab :\n",
        "    fig : plotly figure name \n",
        "    exemple : /content/Exports/df/df_col_file_name.html\n",
        "    /content/Exports/{df}/{df}_{col}{file_name}.html}\n",
        "\n",
        " WARNING : fonctionne avec fonction ExportPlotly(fig,file_name)\n",
        " \"\"\"\n",
        "  \n",
        " #  path = f\"/content/Exports/\"\n",
        " #  os.mkdir(path)\n",
        "\n",
        " col = col_Name_String\n",
        "\n",
        " import os # https://docs.python.org/fr/3/library/os.html\n",
        " path = f\"/content/Exports/{df_Name_String}\"\n",
        " os.makedirs(path,exist_ok=True)\n",
        " print(f\"Repertoire : {path}\")\n",
        " \n",
        " Final_file_name = f\"{path}/{df_Name_String}_{col}_{file_name}\"\n",
        " #file_name = f\"{get_df_name(df)}_{col}OutlierBoxplot\"\n",
        " #print(file_name)\n",
        " ExportPlotly(fig,Final_file_name)\n",
        "\n",
        "#ExportPlotly_dfPATH(df,col,fig,file_name)"
      ],
      "metadata": {
        "id": "0avBz0CKRk_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q74HdR4jexxP"
      },
      "outputs": [],
      "source": [
        "# Push to gihub en Test : https://gist.github.com/avullo/b8153522f015a8b908072833b95c3408"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jvWNP7NaMTE"
      },
      "source": [
        "#### Wordcloud  A completer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NtIWd4LaXIj"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
        "\n",
        "# # Create a list of word\n",
        "# text=\"BottleNeck BottleNeck BottleNeck  BottleNeck  BottleNeck   vin champagne AOC Bordeaux Bordeaux Bourgogne Bourgogne Gewurztraminer Grand Cru Château Turcaud Bordeaux Rouge Domaine La Croix    Beaune Vieilles Vignes Champagne Mailly Grand Cru  Mailly Grand Cru Brut Réserve Alscace Jurançon Pinot Gris Vendanges Tardives Sec Pinot Gris Champagne Gosset  Pinot Noir Domaine Huet Vouvray Saumur Blanc Clos RomanDomaine Saint-Nicolas Château de Villeneuve Saumur-Champigny Domaine Labranche Pinot Noir Sous La Tour Chambolle-Musigny 1er Cru  Wemyss Malts Single Cask Scotch Whisky Domaine de La Tour Cognac Frapin\" #+Global.query(\"post_title != 0\").post_title.values\n",
        "# text\n",
        "# # text\n",
        "\n",
        "# # # Create the wordcloud object\n",
        "# wordcloud = WordCloud(width=480, height=480, margin=0).generate(text)\n",
        "# wordcloudWhite = WordCloud(width=480, height=480, margin=0, background_color=\"white\").generate(text)\n",
        "\n",
        "\n",
        "\n",
        "# # Display the generated image:\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis(\"off\")\n",
        "# plt.margins(x=0, y=0)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# plt.imshow(wordcloudWhite, interpolation='bilinear')\n",
        "# plt.axis(\"off\")\n",
        "# plt.margins(x=0, y=0)\n",
        "# plt.show()\n",
        "\n",
        "#WordCloud Tuto\n",
        "#https://re-thought.com/creating-wordclouds-in-python/\n",
        "#https://www.datacamp.com/tutorial/wordcloud-python \n",
        "# https://www.datacamp.com/tutorial/wordcloud-python\n",
        "# https://thecleverprogrammer.com/2021/11/11/word-cloud-from-a-pandas-dataframe-in-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU93NRopNszB"
      },
      "source": [
        "### Graphes Plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou5y2rz_Nwfi"
      },
      "source": [
        "#### DistributionHist(df,col) & DistributionBar(df,col)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def FeatureCorrelationMatrix(df,Selected_Method_Name):\n",
        " \"\"\"\n",
        " Heatmap : Matrice de corrélation entres les differentes variables (features) d'unh dataframe \n",
        "  FeatureCorrelationMatrix(df,Selected_Method_Name)\n",
        "\n",
        "  df : datatframe \n",
        "  Selected_Method_Name : String  metode coef correlation [\"pearson\", \"kendall\", \"spearman\"] => en cas d'ereeur la methode de pearson est appliquée par défault\n",
        "  Note : la correlation n'est calculé que pour les données quantitatives ! ( les autres colonnes sont ignorées)\n",
        "\n",
        " \"\"\"\n",
        " # heatmap Correlation of a data frame  features Version Plotly \n",
        " #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
        " print(f\"\\n{pcolors.WARNING}Heatmap : Matrice correlations variables (features) {pcolors.RESET}\")\n",
        "  \n",
        " #pcs_heatmap = pcs.copy()\n",
        " #pcs_heatmap.set_index('features', inplace=True)\n",
        " #pcs_heatmap\n",
        " \n",
        " df_Name_String = f\"{get_df_name(df)}\"\n",
        " list_Of_features = df.columns\n",
        " Selected_Method = Selected_Method_Name   # choice  {\"pearson\", \"kendall\", \"spearman\"}\n",
        " Liste_methodes = [\"pearson\", \"kendall\", \"spearman\"]\n",
        "\n",
        " if Selected_Method not in Liste_methodes:\n",
        "  print(f\"{pcolors.FAIL}Erreur de saisie Selected_Method :{pcolors.RESET} choisir un des méthode de cette liste {Liste_methodes}\")\n",
        "  print(\"                                  ..... pearson appliquée par défaut\" )\n",
        "  Selected_Method = \"pearson\" \n",
        "    \n",
        "\n",
        " corr_matrix = df.corr(method=Selected_Method) # Dataframe Matrice de Correlation\n",
        " \n",
        " # graphique de Corrélations Plotly \n",
        "\n",
        " titre = f\"Heatmap Martrice Corrélations {Selected_Method} des variables {df_Name_String}\"\n",
        " fig = px.imshow(corr_matrix, \n",
        "                 text_auto='.3f',\n",
        "                 labels=dict(color=f'Coef Corrélation {Selected_Method}'),\n",
        "                 color_continuous_scale=\"prgn\",   # \"tropic_r\",\n",
        "                 color_continuous_midpoint=0, zmin=-1, zmax=1,\n",
        "                 title= titre,\n",
        "                 width=1000 ,height=700)\n",
        " fig.show()\n",
        "\n",
        " \n",
        " # Export Plotly graph to .html\n",
        " # ExportPlotly(fig,file_name)\n",
        " col_Name_String =  \"_\"\n",
        " file_name = f\"Features_CorrelationMatrix_{Selected_Method}.html\"\n",
        " ExportPloltyColab(df,col_Name_String,fig,file_name)\n",
        " \n",
        " print(titre)\n",
        " return corr_matrix"
      ],
      "metadata": {
        "id": "1bZkNb57zBFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PlotNaN(df):\n",
        "  # print(\"Heatmap : Plot NaN\")\n",
        "  # plt.figure(figsize=(20,10))\n",
        "  # sns.heatmap(df.isna(), cbar=False)\n",
        "\n",
        " IsNan = df.isnull().values.any()\n",
        " if IsNan == True :\n",
        "   print(f\"{pcolors.WARNING}Heatmap des NaN{pcolors.RESET} {get_df_name(df)} {df.shape}\")\n",
        "   Ploty_title_comment = f\"{df.isnull().sum().sum()}\"\n",
        " else :\n",
        "    print(f\"{pcolors.OK}Heatmap des Nan :{pcolors.RESET} Aucun NaN dans {get_df_name(df)} {df.shape}\")\n",
        "    Ploty_title_comment = \" : Aucun NaN dans le dataframe \"\n",
        "\n",
        "\n",
        " fig = px.imshow(df.isna(),\n",
        "                  text_auto=True, \n",
        "                  title=f\"Heatmap des NaN {get_df_name(df)} {Ploty_title_comment}\",\n",
        "                  color_continuous_scale=\"greys\",\n",
        "                  color_continuous_midpoint=1, \n",
        "                  zmin=0.5, #zmax=1,\n",
        "                  )\n",
        " \n",
        " fig.update_layout(\n",
        "    yaxis_title=\"index\",)\n",
        " fig.show()\n",
        "  \n",
        " # Export Plotly graph to .html\n",
        " # ExportPlotly(fig,file_name)\n",
        " col_Name_String =  \"_\"\n",
        " file_name =f\"PlotNaN.html\"\n",
        " ExportPloltyColab(df,col_Name_String,fig,file_name)"
      ],
      "metadata": {
        "id": "ts7qSVzt8yp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wvw9qCHNwfj"
      },
      "outputs": [],
      "source": [
        "def DistributionHist(df,col): \n",
        " import plotly.express as px\n",
        " df.sort_values(col, ascending=True, inplace=True)\n",
        " fig = px.histogram(df, x=col,\n",
        "            marginal='box',     #'rug', 'box', 'violin', or 'histogram'\n",
        "            hover_data=df.columns ,text_auto=True,\n",
        "            title=\"Distribution \"+ col,\n",
        "            width=1000 ,height=600)\n",
        " \n",
        " #file_name =\"DistributionHist\"\n",
        " #ExportPlotly_dfPATH(df,col,fig,file_name)\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " # ExportPlotly(fig,file_name)\n",
        " #col_Name_String =  \"_\"\n",
        " #file_name = f\"DistributionHist.html\"\n",
        " #ExportPloltyColab(df,col_Name_String,fig,file_name)\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " # ExportPlotly(fig,file_name)\n",
        " #col_Name_String =  \"_\"\n",
        " #file_name = f\"DistribHist.html\"\n",
        " #ExportPloltyColab(df,col_Name_String,fig,file_name)\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " #   file_name =\"_OutlierBoxplot\"\n",
        " #   ExportPlotly(fig,file_name)\n",
        " df.name = get_df_name(df)\n",
        " #import os # https://docs.python.org/fr/3/library/os.html\n",
        " path = f\"/content/Exports/{df.name}/\"\n",
        " os.makedirs( path , exist_ok=True)\n",
        " file_name = f\"{path}{df.name}_{col}_distribHist.html\"\n",
        " ExportPlotly(fig,file_name) \n",
        "\n",
        "  \n",
        " return fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PwfHe6GceU7"
      },
      "outputs": [],
      "source": [
        "def DistributionBar(df,col): \n",
        " import plotly.express as px\n",
        " df.sort_values(col, ascending=True, inplace=True)\n",
        " fig = px.bar(df, x=col,\n",
        "            barmode='group', # 'group', 'overlay' or 'relative'\n",
        "             hover_data=df.columns ,\n",
        "              # text_auto=True,\n",
        "            title=\"Distribution \"+ col,\n",
        "             width=1000 ,height=600)\n",
        " \n",
        " #file_name =\"_DistributionBar\"\n",
        " #ExportPlotly_dfPATH(df,col,fig,file_name)\n",
        "\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " #   file_name =\"_OutlierBoxplot\"\n",
        " #   ExportPlotly(fig,file_name)\n",
        " df.name = get_df_name(df)\n",
        " #import os # https://docs.python.org/fr/3/library/os.html\n",
        " path = f\"/content/Exports/{df.name}/\"\n",
        " os.makedirs( path , exist_ok=True)\n",
        " file_name = f\"{path}{df.name}_{col}_DistribBar.html\"\n",
        " ExportPlotly(fig,file_name) \n",
        "\n",
        " return fig.show()\n",
        "\n",
        "\n",
        "  \n",
        " return fig.show()\n",
        "  #https://plotly.com/python/bar-charts/ \n",
        "  #https://plotly.com/python-api-reference/generated/plotly.express.bar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG-JlZwRPQZY"
      },
      "source": [
        "#### OutlierBoxplot(df,col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HWLkVptObdI"
      },
      "outputs": [],
      "source": [
        "def OutlierBoxplot(df,col): \n",
        " import plotly.express as px\n",
        " fig = px.box(data_frame=df, x=col, y=None, \n",
        "          color=None, \n",
        "          facet_row=None, facet_col=None, facet_col_wrap=0, facet_row_spacing=None, facet_col_spacing=None, \n",
        "          hover_name=None, hover_data=df.columns, \n",
        "          custom_data=None, \n",
        "          animation_frame=None, animation_group=None, \n",
        "          category_orders=None, labels=None, \n",
        "          color_discrete_sequence=None, \n",
        "          color_discrete_map=None, \n",
        "          orientation=\"h\", \n",
        "          boxmode=\"overlay\", #['group', 'overlay']\n",
        "          log_x=False, log_y=False, range_x=None, \n",
        "          range_y=None, points='suspectedoutliers', # ['all', 'outliers', 'suspectedoutliers', False]\n",
        "          notched=True,\n",
        "          title=\"Répartition \"+col+\" et visualisation des Outliers potentiels\", template=None,\n",
        "          width=1000 ,height=600)\n",
        " \n",
        "\n",
        " # Export Plotly graph to .html\n",
        " #   file_name =\"_OutlierBoxplot\"\n",
        " #   ExportPlotly(fig,file_name)\n",
        " df.name = get_df_name(df)\n",
        " #import os # https://docs.python.org/fr/3/library/os.html\n",
        " path = f\"/content/Exports/{df.name}/\"\n",
        " os.makedirs( path , exist_ok=True)\n",
        " file_name = f\"{path}{df.name}_{col}_OutlierBoxplot.html\"\n",
        " ExportPlotly(fig,file_name) \n",
        "\n",
        " return fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C0Xwr6hIeDG"
      },
      "source": [
        "#### AnalyseBivarHist(df,critereA,critereB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDa2ZlIKIaK4"
      },
      "outputs": [],
      "source": [
        "def AnalyseBivarHist(df,critereA,critereB) : \n",
        " \"\"\"  AnalyseBivarHist : Analyse Bivarié  tableau contingence & histograme nomrmalisé percent )\n",
        " \"\"\"\n",
        " #  df = Global.copy()\n",
        " #  critereA = \"Sexe\"\n",
        " #  critereB = \"Work_accident\"\n",
        "\n",
        " #Tableaux de contingence \n",
        " ContTabbleNorm = pd.crosstab(df[critereA], df[critereB] , margins=True, margins_name=\"Total\" ,normalize=True)\n",
        " ContTable = pd.crosstab(df[critereA], df[critereB] , margins=True, margins_name=\"Total\" ,normalize=False)\n",
        "\n",
        " print(pcolors.OK+\"\",critereB,\" par \",critereA,\"\"+pcolors.RESET)\n",
        " #  print()\n",
        " #  print(ContTabbleNorm.round(2).to_markdown())\n",
        " print()\n",
        " print(ContTable.round(2).to_markdown())\n",
        "\n",
        " # Histograme CritereB par CritereA\n",
        "\n",
        " fig = px.histogram(df, x = critereB, \n",
        "            color = critereA, color_discrete_map = {\"H\":'blue',\"F\":'pink'},\n",
        "            marginal = None,     #'rug', 'box', 'violin', or 'histogram'\n",
        "            histnorm = None,    # None  'percent', 'probability', 'density', or 'probability density'           \n",
        "            labels = {\"H\":'Homme',\"F\":'Femme'},\n",
        "            hover_data = df.columns ,\n",
        "            barnorm ='percent' , # None 'fraction' or 'percent'\n",
        "            barmode ='group'  , # 'relative'  'group', 'overlay'\n",
        "            text_auto = '.2f', #  True / False  or format '.2f'   \n",
        "            title = \"Reparition \"+ critereB+\" par \"+critereA,\n",
        "            width=1000 ,height=600)\n",
        "\n",
        " # fig.update_traces(textposition='inside', textinfo='value+percent+label')  # \"label\", \"text\", \"value\", \"percent\"\n",
        " fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
        "\n",
        "\n",
        " fig.add_shape( # add a horizontal \"target\" line\n",
        "    type=\"line\", line_color=\"grey\", line_width=3, opacity=0.5, line_dash=\"dot\",\n",
        "    x0=0, x1=1, xref=\"paper\", y0=50, y1=50, yref=\"y\")\n",
        "\n",
        " # fig.add_annotation( # add a text callout with arrow\n",
        " #     text=\"below target!\", x=\"Fri\", y=400, arrowhead=1, showarrow=True)\n",
        "\n",
        " # custum axes\n",
        " fig.update_yaxes(ticksuffix=\"%\", showgrid=True)   # the y-axis is in €uro ticksuffix=\"%\"    presuffix=\"€\"\n",
        " # fig.update_xaxes( dtick=\"M1\", tickformat=\"%b\\n%Y\", ticklabelmode=\"period\") # the x-axis Mois / annnée\n",
        " # fig.update_xaxes( dtick=\"M1\", ticklabelmode=\"period\") # the x-axis Mois / annnée\n",
        "\n",
        " fig.show()\n",
        "\n",
        " file_name =f\"AnalyseBivarHist\"\n",
        " col = f\"{critereA}_{critereB}\"\n",
        " ExportPlotly_dfPATH(df,col,fig,file_name)\n",
        " # https://plotly.github.io/plotly.py-docs/generated/plotly.express.histogram.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI8PuDF_InK2"
      },
      "source": [
        "#### AnalyseBivarOLS(df,critereA,critereB,critereC)  (Scatterplot & OLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHqWO3RyI8_F"
      },
      "outputs": [],
      "source": [
        "def AnalyseBivarOLS(df,critereX,critereY,critereColor) : \n",
        " \"\"\"  AnalyseBivarOLS : Analyse Bivarié  Scatter plot + OLS)\n",
        "         df\n",
        "         critereA  \"Colname\"  X\n",
        "         critereB  \"Colname\"  Y \n",
        "         critereC \"Colname\"  Color \n",
        "         OLS = Y = a X + b  // Coef correlation R² \n",
        " \"\"\"\n",
        " #  df = Global.copy()\n",
        " #  critereA = \"Sexe\"\n",
        " #  critereB = \"Work_accident\"\n",
        "\n",
        "  #  #Tableaux de contingence \n",
        " #  ContTabbleNorm = pd.crosstab(df[critereA], df[critereB] , margins=True, margins_name=\"Total\" ,normalize=True)\n",
        " #  ContTable = pd.crosstab(df[critereA], df[critereB] , margins=True, margins_name=\"Total\" ,normalize=False)\n",
        "\n",
        " #  print(pcolors.OK+\"\",critereB,\" par \",critereA,\"\"+pcolors.RESET)\n",
        " #  #  print()\n",
        " #  #  print(ContTabbleNorm.round(2).to_markdown())\n",
        " #  print()\n",
        " #  print(ContTable.round(2).to_markdown())\n",
        "\n",
        " # Scatterplot & t OLS  CritereB par CritereA\n",
        "\n",
        " \n",
        "\n",
        " fig = px.scatter(df, x=critereX, y=critereY, \n",
        "                  color = critereColor, color_discrete_map = {\"H\":'blue',\"F\":'pink'},\n",
        "                  opacity=0.80, color_continuous_scale='twilight',\n",
        "                  marginal_x= None,     #'rug', 'box', 'violin', or 'histogram'\n",
        "                  marginal_y= None,     #'rug', 'box', 'violin', or 'histogram'\n",
        "                  trendline = 'ols'     , #'ols', 'lowess', 'rolling', 'expanding' or 'ewm'. \n",
        "                  trendline_scope = 'trace' ,  # 'trace' or 'overall'\n",
        "                  # trendline_color_override = 'grey', \n",
        "                  # labels={\"price_sum\":\"CA quotidien\"},\n",
        "                 title= critereX+\" en fonction de \"+critereY) \n",
        " \n",
        " # fig0 = px.scatter(df, x = critereA, y = critereB, color = critereC,  opacity=0.0, trendline='ols',trendline_scope = 'trace',trendline_color_override = 'red')\n",
        " # fig.add_trace(fig0.data[1])\n",
        " \n",
        " \n",
        " # legend \n",
        " fig.update_traces(showlegend=True)\n",
        " # fig.update_layout(legend=dict( yanchor=\"top\"))\n",
        " # fig.update_layout(legend_title_text='Moyenne Mobile')\n",
        "\n",
        " # fig.update_traces(textposition='inside', textinfo='value+percent+label')  # \"label\", \"text\", \"value\", \"percent\"\n",
        " fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
        "\n",
        " # custum axes\n",
        " # fig.update_yaxes(ticksuffix=\"%\", showgrid=True)   # the y-axis is in €uro ticksuffix=\"%\"    presuffix=\"€\"\n",
        " # fig.update_yaxes(tickprefix=\"€\", showgrid=True)   # the y-axis is in €uro\n",
        " # fig.update_xaxes( dtick=\"M1\", tickformat=\"%b\\n%Y\", ticklabelmode=\"period\") # the x-axis Mois / annnée\n",
        " #fig.update_xaxes( dtick=\"M1\", ticklabelmode=\"period\") # the x-axis Mois / annnée\n",
        "\n",
        " # Annotations \n",
        " # fig.add_shape( type=\"line\", line_color=\"grey\", line_width=3, opacity=0.5, line_dash=\"dot\", x0=0, x1=1, xref=\"paper\", y0=50, y1=50, yref=\"y\") # add a horizontal \"target\" line \n",
        " #  fig.add_annotation(text=\"below target!\", x=\"Fri\", y=400, arrowhead=1, showarrow=True) # add a text callout with arrow\n",
        "\n",
        "\n",
        " fig.show()\n",
        "\n",
        " print()\n",
        " #Coef Regfression linéaire Pearson & Coef Spearman\n",
        " print(\"Correlation \"+critereX+\" & \"+critereY)\n",
        " CoefPS(df,critereX,critereY,0.05)\n",
        " print()\n",
        " # ols Table\n",
        " results = px.get_trendline_results(fig)\n",
        " return results.px_fit_results.iloc[1].summary()\n",
        "\n",
        " results = px.get_trendline_results(fig)\n",
        " return results.px_fit_results.iloc[0].summary()\n",
        "\n",
        " #  results = px.get_trendline_results(fig0)\n",
        " #  results.px_fit_results.iloc[0].summary()\n",
        " # OLS intepretation : https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/ \n",
        " \n",
        " # https://plotly.com/python/line-and-scatter/\n",
        " # https://plotly.com/python-api-reference/generated/plotly.express.scatter.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rUtv0K4xm1u"
      },
      "source": [
        "### Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GeWhJKhQYMAr"
      },
      "outputs": [],
      "source": [
        "def INFOS(df):\n",
        "  \"\"\" Info sur un  DataFrame df \"\"\"\n",
        "  dtypeCat2Obj(df)\n",
        "  ListeCol=df.columns\n",
        "  df_name = get_df_name(df)\n",
        "\n",
        "  INFOdf = pd.DataFrame(columns = ['Colonne','Nb_Unique' , 'Nb_Doublons', 'Doublons_%', 'DataType', 'nbr_NaN', 'NaN_%', 'nbr_Zero', 'Zero_%','Commentaire',\"Var_Type\",\"display\",\"Min/first\",\"Max/last\",\"Uniqueliste\"])\n",
        "\n",
        "  lineSeparator0 = '\\n_________________________________________________________________________________________________________________________________________________________________\\n'\n",
        "  lineSeparator1 = '\\n------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n'\n",
        "\n",
        "  # print('Liste colonnes :',pcolors.OK +ListeCol+ pcolors.RESET)\n",
        "  print(lineSeparator0,\n",
        "        \" Info DataFrame  : \",\n",
        "        f\"{pcolors.WARNING} {df_name} {pcolors.RESET} ( {len(df)} Lignes {len(ListeCol)} Colonnes)\",\n",
        "        lineSeparator0)\n",
        "  \n",
        "  for col in ListeCol: \n",
        "    NbUnique = df[col].nunique()\n",
        "    Nbdoublons = len(df[col])-NbUnique\n",
        "    DoublonsP = np.round(((len(df[col])-NbUnique)/len(df[col]))*100,0) # doublons %\n",
        "    if Nbdoublons == 0:\n",
        "     commentaire = \"clé_Candidate\"\n",
        "    else:\n",
        "      commentaire = \"-\"\n",
        " \n",
        "    NaNsum = df[col].isnull().sum(axis=0) # Nomnbre de Nan par colonne\n",
        "    NanPP = np.round((NaNsum /len(df[col]))*100,3) # % de Nan par colonne\n",
        "    Zerosum = len(df[df[col] == 0])\n",
        "    ZeroPP = np.round((Zerosum /len(df[col]))*100,3) \n",
        "    Unique = df[col].unique()\n",
        "\n",
        "    if len(Unique)> 5 : \n",
        "      Uniqueliste = f\"n={len(Unique)} : {Unique[0:2]} ... {Unique[-2:]}\"\n",
        "    else : \n",
        "     Uniqueliste = Unique[0:4]\n",
        "\n",
        "    if df[col].dtype != 'object':\n",
        "     new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Quantitative\",\"Min_Max\",f\"{df[col].min()}\",df[col].max(),Uniqueliste )] # Nouvelle ligne par colonne \n",
        "    else: \n",
        "      new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Qualitative\",\"First_Last\",df[col].iloc[0],df[col].iloc[-1],Uniqueliste )] # Nouvelle ligne par colonne\n",
        "\n",
        "\n",
        "    New = pd.DataFrame( new_list, columns = INFOdf.columns) # DataFrame Colonne    \n",
        "    INFOdf = INFOdf.append(New,ignore_index=True) # Ajout de la ligne correspondant à la colonne\n",
        "      \n",
        "  #return INFOdf \n",
        "  print(INFOdf.to_markdown())\n",
        "  print(lineSeparator1) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def INFOS_V0(df):\n",
        "  \"\"\" Info sur un  DataFrame df \"\"\"\n",
        "  dtypeCat2Obj(df)\n",
        "  ListeCol=df.columns\n",
        "  df_name = get_df_name(df)\n",
        "\n",
        "  INFOdf = pd.DataFrame(columns = ['Colonne','Nb_Unique' , 'Nb_Doublons', 'Doublons_%', 'DataType', 'nbr_NaN', 'NaN_%', 'nbr_Zero', 'Zero_%','Commentaire',\"Var_Type\",\"display\",\"Min/first\",\"Max/last\",\"Uniqueliste\"])\n",
        "\n",
        "  lineSeparator0 = '\\n_________________________________________________________________________________________________________________________________________________________________\\n'\n",
        "  lineSeparator1 = '\\n------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n'\n",
        "\n",
        "  # print('Liste colonnes :',pcolors.OK +ListeCol+ pcolors.RESET)\n",
        "  print(lineSeparator0,\n",
        "        \" Info DataFrame  : \",\n",
        "        f\"{pcolors.WARNING} {df_name} {pcolors.RESET} ( {len(df)} Lignes {len(ListeCol)} Colonnes)\",\n",
        "        lineSeparator0)\n",
        "  \n",
        "  for col in ListeCol: \n",
        "    NbUnique = df[col].nunique()\n",
        "    Nbdoublons = len(df[col])-NbUnique\n",
        "    DoublonsP = np.round(((len(df[col])-NbUnique)/len(df[col]))*100,0) # doublons %\n",
        "    if Nbdoublons == 0:\n",
        "     commentaire = \"clé_Candidate\"\n",
        "    else:\n",
        "      commentaire = \"-\"\n",
        " \n",
        "    NaNsum = df[col].isnull().sum(axis=0) # Nomnbre de Nan par colonne\n",
        "    NanPP = np.round((NaNsum /len(df[col]))*100,3) # % de Nan par colonne\n",
        "    Zerosum = len(df[df[col] == 0])\n",
        "    ZeroPP = np.round((Zerosum /len(df[col]))*100,3) \n",
        "    Unique = df[col].unique()\n",
        "\n",
        "    if len(Unique)> 5 : \n",
        "      Uniqueliste = f\"n={len(Unique)} : {Unique[0:2]} ... {Unique[-2:]}\"\n",
        "    else : \n",
        "     Uniqueliste = Unique[0:4]\n",
        "\n",
        "    if df[col].dtype != 'object':\n",
        "     new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Quantitative\",\"Min_Max\",f\"{df[col].min()}\",df[col].max(),Uniqueliste )] # Nouvelle ligne par colonne \n",
        "    else: \n",
        "      new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Qualitative\",\"First_Last\",df[col].iloc[0],df[col].iloc[-1],Uniqueliste )] # Nouvelle ligne par colonne\n",
        "\n",
        "\n",
        "    New = pd.DataFrame( new_list, columns = INFOdf.columns) # DataFrame Colonne    \n",
        "    INFOdf = INFOdf.append(New,ignore_index=True) # Ajout de la ligne correspondant à la colonne\n",
        "      \n",
        "  #return INFOdf \n",
        "  print(INFOdf.to_markdown())\n",
        "  print(lineSeparator1) "
      ],
      "metadata": {
        "id": "Qb66tLuZCy0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmwFmONrzCFs"
      },
      "outputs": [],
      "source": [
        "def INFOSdf(df):\n",
        "  \"\"\" Info sur un  DataFrame df \"\"\"\n",
        "  dtypeCat2Obj(df)\n",
        "  ListeCol=df.columns\n",
        "  df_name = get_df_name(df)\n",
        "\n",
        "  INFOdf = pd.DataFrame(columns = ['Colonne','Nb_Unique' , 'Nb_Doublons', 'Doublons_%', 'DataType', 'nbr_NaN', 'NaN_%', 'nbr_Zero', 'Zero_%','Commentaire',\"Var_Type\",\"display\",\"Min/first\",\"Max/last\",\"Uniqueliste\"])\n",
        "\n",
        "  lineSeparator0 = '\\n_________________________________________________________________________________________________________________________________________________________________\\n'\n",
        "  lineSeparator1 = '\\n------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n'\n",
        "\n",
        "  # print('Liste colonnes :',pcolors.OK +ListeCol+ pcolors.RESET)\n",
        "  print(lineSeparator0,\n",
        "        \" Info DataFrame  : \",\n",
        "        f\"{pcolors.WARNING} {df_name} {pcolors.RESET} ( {len(df)} Lignes {len(ListeCol)} Colonnes)\",\n",
        "        lineSeparator0)\n",
        "  \n",
        "  for col in ListeCol: \n",
        "    NbUnique = df[col].nunique()\n",
        "    Nbdoublons = len(df[col])-NbUnique\n",
        "    DoublonsP = np.round(((len(df[col])-NbUnique)/len(df[col]))*100,0) # doublons %\n",
        "    if Nbdoublons == 0:\n",
        "     commentaire = \"clé_Candidate\"\n",
        "    else:\n",
        "      commentaire = \"-\"\n",
        " \n",
        "    NaNsum = df[col].isnull().sum(axis=0) # Nomnbre de Nan par colonne\n",
        "    NanPP = np.round((NaNsum /len(df[col]))*100,3) # % de Nan par colonne\n",
        "    Zerosum = len(df[df[col] == 0])\n",
        "    ZeroPP = np.round((Zerosum /len(df[col]))*100,3) \n",
        "    Unique = df[col].unique()\n",
        "\n",
        "    if len(Unique)> 5 : \n",
        "      Uniqueliste = f\"n={len(Unique)} : {Unique[0:2]} ... {Unique[-2:]}\"\n",
        "    else : \n",
        "     Uniqueliste = Unique[0:4]\n",
        "\n",
        "    if df[col].dtype != 'object':\n",
        "     new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Quantitative\",\"Min_Max\",df[col].min(),df[col].max(),Uniqueliste )] # Nouvelle ligne par colonne \n",
        "    else: \n",
        "      new_list = [ ( col, NbUnique, Nbdoublons, f\"{DoublonsP}%\", df[col].dtype , NaNsum, f\"{NanPP}%\" ,Zerosum ,f\"{ZeroPP}%\", commentaire,\"Qualitative\",\"First_Last\",df[col].iloc[0],df[col].iloc[-1],Uniqueliste )] # Nouvelle ligne par colonne\n",
        "\n",
        "\n",
        "    New = pd.DataFrame( new_list, columns = INFOdf.columns) # DataFrame Colonne    \n",
        "    INFOdf = INFOdf.append(New,ignore_index=True) # Ajout de la ligne correspondant à la colonne\n",
        "      \n",
        "  return INFOdf \n",
        "  print(INFOdf.to_markdown())\n",
        "  print(lineSeparator1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX7l8x9Er_O6"
      },
      "outputs": [],
      "source": [
        "def ExploreQualitative(df):\n",
        " # Explore discrete data\n",
        " # dtypes('object' & 'category')\n",
        "\n",
        " liste = df.select_dtypes('object').columns.append(df.select_dtypes('category').columns)\n",
        " print(pcolors.GREEN+\"Liste des variables qualitatives (object & category) :\"+pcolors.RESET,liste.values)\n",
        " print(\"Afficher les graphiques avec de chaque colonne avec le code suivant :\")\n",
        " print(\"DistributionHist(df,col)  - DistributionBar(df,col)  - OutlierBoxplot(df,col) \\n\")\n",
        "\n",
        "\n",
        " for col in liste:\n",
        "  print()\n",
        "  listeDiscrete = df[col].sort_values(ascending=True,na_position='first').unique()\n",
        "  if len(listeDiscrete) > 6: \n",
        "    listeBig = str(len(listeDiscrete)) + \" modalitées : de \"+ listeDiscrete[0]+ \" à \"+ listeDiscrete[-1]\n",
        "    print(f'{col :-<50} {listeBig}')\n",
        "  else :\n",
        "    print(f'{col :-<50} {listeDiscrete}')\n",
        "  print()\n",
        "  # DistributionBar(df,col)\n",
        "  DistributionHist(df,col)\n",
        "\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwr8OyjgbT1C"
      },
      "outputs": [],
      "source": [
        "def ExploreQuantitative(df):\n",
        " #  get_df_name(df)\n",
        " # Explore  Hist of quantitative data\n",
        " # dtypes('float' & 'int')\n",
        " liste = df.select_dtypes('float').columns.append(df.select_dtypes('int').columns)\n",
        " print(pcolors.GREEN+\"Liste des variables quantitatives (int & float) :\"+pcolors.RESET,liste.values)\n",
        " print(\"Afficher les graphiques avec de chaque colonne avec le code suivant :\")\n",
        " print(\"DistributionHist(df,col)  - DistributionBar(df,col)  - OutlierBoxplot(df,col) \\n\")\n",
        "\n",
        " for col in liste:\n",
        "  print()\n",
        "  DistributionHist(df,col)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PlotZero(df):\n",
        "\n",
        " print(\"Heatmap : Plot Zero\")\n",
        " IsZero = df.eq(0).any(1).sum()\n",
        " if IsZero >= 1 :\n",
        "    print(f\"{pcolors.WARNING}Heatmap des Zéros {pcolors.RESET} {get_df_name(df)} {df.shape}\")\n",
        "    Ploty_title_comment = f\"({df.eq(0).any(1).sum()} Zéros)\"\n",
        " else :\n",
        "     print(f\"{pcolors.OK}Heatmap des Zéros :{pcolors.RESET} Aucun Zéro dans {get_df_name(df)} {df.shape}\")\n",
        "     Ploty_title_comment = \" : Aucun NaN dans le dataframe \"\n",
        " \n",
        " #Ploty_title_comment = \"Plot des Zero \"\n",
        "\n",
        " fig = px.imshow(df.eq(0),\n",
        "                  text_auto=True, \n",
        "                  title=f\"Heatmap des Zéro {get_df_name(df)} {Ploty_title_comment}\",\n",
        "                  color_continuous_scale= 'blues',\n",
        "                  color_continuous_midpoint=1, \n",
        "                  zmin=0.5, #zmax=1,\n",
        "                  )\n",
        " \n",
        " fig.update_layout(\n",
        "    yaxis_title=\"index\",)\n",
        " fig.show()\n",
        "  \n",
        " # Export Plotly graph to .html\n",
        " #\n",
        " col_Name_String =  \"_\"\n",
        " file_name =f\"PlotZero.html\"\n",
        " ExportPlotly(fig,file_name)\n",
        " ExportPloltyColab(df,col_Name_String,fig,file_name)\n",
        "\n",
        "\"\"\"\n",
        "  The 'colorscale' property is a colorscale and may be\n",
        "    specified as:\n",
        "      - A list of colors that will be spaced evenly to create the colorscale.\n",
        "        Many predefined colorscale lists are included in the sequential, diverging,\n",
        "        and cyclical modules in the plotly.colors package.\n",
        "      - A list of 2-element lists where the first element is the\n",
        "        normalized color level value (starting at 0 and ending at 1),\n",
        "        and the second item is a valid color string.\n",
        "        (e.g. [[0, 'green'], [0.5, 'red'], [1.0, 'rgb(0, 0, 255)']])\n",
        "      - One of the following named colorscales:\n",
        "            ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n",
        "             'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n",
        "             'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n",
        "             'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n",
        "             'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n",
        "             'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n",
        "             'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n",
        "             'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl',\n",
        "             'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn',\n",
        "             'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu',\n",
        "             'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar',\n",
        "             'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn',\n",
        "             'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid',\n",
        "             'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr',\n",
        "             'ylorrd'].\n",
        "        Appending '_r' to a named colorscale reverses it.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "APltRMB95ObY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "47d43e1c-8699-4c3f-d83c-b69e23911e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  The 'colorscale' property is a colorscale and may be\\n    specified as:\\n      - A list of colors that will be spaced evenly to create the colorscale.\\n        Many predefined colorscale lists are included in the sequential, diverging,\\n        and cyclical modules in the plotly.colors package.\\n      - A list of 2-element lists where the first element is the\\n        normalized color level value (starting at 0 and ending at 1),\\n        and the second item is a valid color string.\\n        (e.g. [[0, 'green'], [0.5, 'red'], [1.0, 'rgb(0, 0, 255)']])\\n      - One of the following named colorscales:\\n            ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\\n             'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\\n             'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\\n             'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\\n             'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\\n             'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\\n             'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\\n             'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl',\\n             'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn',\\n             'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu',\\n             'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar',\\n             'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn',\\n             'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid',\\n             'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr',\\n             'ylorrd'].\\n        Appending '_r' to a named colorscale reverses it.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PlotNaN(df):\n",
        "  # print(\"Heatmap : Plot NaN\")\n",
        "  # plt.figure(figsize=(20,10))\n",
        "  # sns.heatmap(df.isna(), cbar=False)\n",
        "\n",
        " IsNan = df.isnull().values.any()\n",
        " if IsNan == True :\n",
        "   print(f\"{pcolors.WARNING}Heatmap des NaN{pcolors.RESET} {get_df_name(df)} {df.shape}\")\n",
        "   Ploty_title_comment = f\"{df.isnull().sum().sum()}\"\n",
        " else :\n",
        "    print(f\"{pcolors.OK}Heatmap des Nan :{pcolors.RESET} Aucun NaN dans {get_df_name(df)} {df.shape}\")\n",
        "    Ploty_title_comment = \" : Aucun NaN dans le dataframe \"\n",
        "\n",
        "\n",
        " fig = px.imshow(df.isna(),\n",
        "                  text_auto=True, \n",
        "                  title=f\"Heatmap des NaN {get_df_name(df)} {Ploty_title_comment}\",\n",
        "                  color_continuous_scale=\"greys\",\n",
        "                  color_continuous_midpoint=1, \n",
        "                  zmin=0.5, #zmax=1,\n",
        "                  )\n",
        " \n",
        " fig.update_layout(\n",
        "    yaxis_title=\"index\",)\n",
        " fig.show()\n",
        "  \n",
        " # Export Plotly graph to .html\n",
        " # ExportPlotly(fig,file_name)\n",
        " col_Name_String =  \"_\"\n",
        " file_name =f\"PlotNaN.html\"\n",
        " ExportPloltyColab(df,col_Name_String,fig,file_name)"
      ],
      "metadata": {
        "id": "a3dkBVjuMHah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2tn28f4x4F3"
      },
      "outputs": [],
      "source": [
        "def PlotNaNV0(df):\n",
        "  # print(\"Heatmap : Plot NaN\")\n",
        "  # plt.figure(figsize=(20,10))\n",
        "  # sns.heatmap(df.isna(), cbar=False)\n",
        "  \n",
        "  \n",
        "\n",
        "  fig = px.imshow(df.isna(), text_auto=True, title=\"Plot : Heatmap des NaN\",color_continuous_scale=\"viridis\")\n",
        "  \n",
        "  \n",
        "  fig.show()\n",
        "  \n",
        "  file_name =f\"PlotNaN\"\n",
        "  col =\"_\"\n",
        "  ExportPlotly_dfPATH(df,col,fig,file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFCLQldtx6gF"
      },
      "outputs": [],
      "source": [
        "def ExploreV0(df):\n",
        "  \"\"\" Quick df basic Exploration :  Original version \"\"\"\n",
        "  print(pcolors.WARNING+\"DataFrame head():\"+pcolors.RESET)\n",
        "  print(df.head(5).to_markdown())\n",
        "  INFOS(df)\n",
        "  print()\n",
        "  PlotNaN(df)\n",
        "  print() \n",
        "  print(pcolors.WARNING+\"Analyse univariée:\"+pcolors.RESET)\n",
        "  print() \n",
        "  ExploreQualitative(df)\n",
        "  print()\n",
        "  ExploreQuantitative(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Explore(df):\n",
        " \"\"\" Quick df basic Exploration :  Remix Explore VO 24/09/2022 \"\"\"\n",
        " \n",
        " global df_Name_String \n",
        " df_Name_String = f\"{get_df_name(df)}\" # Save dataframe name as a string \n",
        " df.name =  df_Name_String\n",
        " print( df_Name_String,type( df_Name_String))\n",
        " \n",
        "  # Widget Tab Configuration\n",
        "\n",
        " import ipywidgets as widgets #https://ipywidgets.readthedocs.io/en/stable/ \n",
        " tab0 = widgets.Output()\n",
        " tab1 = widgets.Output()\n",
        " tab2 = widgets.Output()\n",
        " tab3 = widgets.Output()\n",
        " tab4 = widgets.Output()\n",
        " tab5 = widgets.Output()\n",
        " tab6 = widgets.Output()\n",
        " tab7 = widgets.Output()\n",
        " tab8 = widgets.Output()\n",
        " tab9 = widgets.Output()\n",
        " \n",
        " tab = widgets.Tab(children = [tab0,tab1,tab2,tab3,tab4,tab5,tab6,tab7,tab8,tab9])\n",
        " tab.set_title(0, f'INFOS {df_Name_String} ')\n",
        " tab.set_title(1, 'Head & Tail')\n",
        " tab.set_title(2, 'Plot NaN')\n",
        " tab.set_title(3, 'Duplicate')\n",
        " tab.set_title(4, f'Describe {df_Name_String}')\n",
        " tab.set_title(5, 'Qualitatives:')\n",
        " tab.set_title(6, 'Quantitatives')\n",
        " tab.set_title(7, 'Corr Spearman')\n",
        " tab.set_title(8, f'Liste graph. {df_Name_String}')\n",
        " tab.set_title(9, 'Explore Fonctions')\n",
        " display(tab)\n",
        "\n",
        " with tab0:\n",
        "  print(f\"{pcolors.WARNING}Dataframe {df_Name_String} INFOS :{pcolors.RESET}\")\n",
        "  INFOS(df)\n",
        "\n",
        " with tab1:\n",
        "  print(pcolors.WARNING+\"DataFrame head():\"+pcolors.RESET,\"\\n\")\n",
        "  print(df.head(5).to_markdown())\n",
        "  print()\n",
        "  print(\".\"*50)\n",
        "  print()\n",
        "  print(df.tail(5).to_markdown())  \n",
        "\n",
        " with tab2:\n",
        "  print(pcolors.WARNING+\"Plot NaN :\"+pcolors.RESET)\n",
        "  PlotNaN(df)\n",
        "\n",
        " with tab3:\n",
        "   print(f\"{pcolors.WARNING} Duplicated :{pcolors.RESET} {df.duplicated().sum()} ligne(s)\")\n",
        "   #print(df.duplicated().sum())\n",
        "\n",
        "   global data_duplicated  # define data_duplicated  as gloabal variable do visualize it \n",
        "   data_duplicated = df[df.duplicated()]\n",
        "        \n",
        "   print(data_duplicated.to_markdown())\n",
        "   print(\"\\n data_duplicated defined as gloabal variable\")\n",
        "\n",
        " with tab4:\n",
        "  print(pcolors.WARNING+\"describes:\"+pcolors.RESET,\"\\n\")\n",
        "  print(df.describe().to_markdown()) \n",
        "  \n",
        " with tab5:\n",
        "  print(pcolors.WARNING+\"Analyse univariée Variables Qualitatives:\"+pcolors.RESET,\"\\n\")\n",
        "  print(\"ExploreQualitative(df)\")\n",
        "  ExploreQualitative(df)\n",
        "\n",
        "\n",
        " with tab6:\n",
        "  print(pcolors.WARNING+\"Analyse univariée Variables Quantitatives:\"+pcolors.RESET,\"\\n\")\n",
        "  print(\"ExploreQuantitative(df)\")\n",
        "  ExploreQuantitative(df)\n",
        "\n",
        " with tab7:\n",
        "  print(f\"{pcolors.WARNING} Matrice Correlation de pearson des variables quantitatives {pcolors.RESET}\\n\")\n",
        "\n",
        "  FeatureCorrelationMatrix(df,\"pearson\")\n",
        "  print('\\nFeatureCorrelationMatrix(df,Selected_Method_Name)  Selected_Method_Name [\"pearson\", \"kendall\", \"spearman\"]')\n",
        "  print(\"Note : Confirmer un correlation de deux variables avec un test statistique en intrepretant une Pvalue\")\n",
        "\n",
        "\n",
        " with tab8:\n",
        "  print(f\"{pcolors.WARNING}{df_Name_String} Liste des graphiques :{pcolors.RESET}\\n\")\n",
        "  import os\n",
        " \n",
        "  dir_path = f\"/content/Exports/{df_Name_String}\"\n",
        "  Path_File_List = os.listdir(dir_path)\n",
        "  for i in Path_File_List:\n",
        "   print(f\"{dir_path}/{i}\")\n",
        "\n",
        " with tab9:\n",
        "  print(f\"{pcolors.WARNING}Aide : Explore fonctions {pcolors.RESET}\")\n",
        "  print(\"Aide()\")\n",
        "  print(\"\\nINFOS(df)  &  INFOSdf(df)\\n\")\n",
        "  \n",
        "  print(\"Explore(df)\")\n",
        "  print(\" PlotNaN(df)\")\n",
        "  print(\" PlotZero(df)\")\n",
        "  print(\" ExploreQuantitative(df)\")\n",
        "  print(\" ExploreQualitative(df)\\n\")\n",
        "  \n",
        "\n",
        "  print(\"DistributionHist(df,col)  & DistributionBar(df,col)\")\n",
        "  print(\"OutlierBoxplot(df,col)\")\n",
        "  print(\"AnalyseBivarHist(df,critereA,critereB)\\n\")\n",
        "\n",
        "  print(\"AnalyseBivarOLS(df,critereX,critereY,critereC)    CritereC : Color  \" ) \n",
        "  print(\"AnalyseBivarHist(df,critereA,critereB)\")\n",
        "  print(\"AnalyseBivarOLS(df,critereX,critereY,critereC)    CritereC : Color  \\n\")\n",
        "\n",
        "  print(f\"{pcolors.WARNING}\\nExports {pcolors.RESET}\")\n",
        "  print(\"Export plotly graph in html : ExportPlotly(fig,file_name)\")\n",
        "  print(f\"Export DataFrame Excel/CSV  : ExportDfColab(df,file_name  Export Format Excel : .xls & .xlsx   {pcolors.BLUE}or{pcolors.RESET}  Format CSV .csv (utF8 - separateur ;)  If required include path in the FileName '../data/my_new_file.csv' \\n\")\n",
        "\n",
        "# Explore(df)   # debug test"
      ],
      "metadata": {
        "id": "SFsU6l6tZ4VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "TTeIRv7pQR1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML ACP (Analyse Composantes Principales) fonctions"
      ],
      "metadata": {
        "id": "Df57GIpg1kvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_ML(df,features_list,standardization=True):\n",
        " \"\"\"\n",
        "  ###### ML ACP : Pre processing (Scalling) et entrainement d'une ACP \n",
        " ACP_ML(df,features,standardization=True)\n",
        " df : dataframe source \n",
        " featureslist = [\"Variable_1\",\"Variable_2\",\"Variable_3\",\"Variable_n)]  Liste des Variables sélectionées pour l'ACP\n",
        " standardization = True   Prametres Scalling : standardization = True  Si unités features  differérentes - False possible si toutes les features la même unité\n",
        " \"\"\"\n",
        " #################### Variables fonctions ####################\n",
        "\n",
        "\n",
        " ################################ TEMP to Dell later ############################\n",
        " #df = dataRAW.copy()  # DataFrame  \n",
        " #features = ['100m', 'Long.jump', 'Shot.put', 'High.jump', '400m', '110m.hurdle','Discus', 'Pole.vault', 'Javeline', '1500m']  # features selection\n",
        " #ColorCol = 'Rank' # 'Rank', 'Points', 'Competition' \n",
        "\n",
        " #  PC1x = 1  ; PC1y = 2    # Selection Composantes Principales x & y  graphe 1 \n",
        " #  PC2x = 3  ; PC2y = 4    # Selection Composantes Principales x & y  graphe 2 \n",
        " ################################ TEMP to Dell later ############################\n",
        " \n",
        "\n",
        "  \n",
        " #### Function Global Variable  :  ACP_ML(df,features_list,standardization=True) \n",
        " # Can be used Inside (local) & outside the function !\n",
        " #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        " global df_Source             # Source DataFrame  (avant feaure Sélection)\n",
        " global features              # Selected features\n",
        " global pca_fited             # modele PCA fited (entrainé)\n",
        " global PCA_exp_var           # liste Explained Variance des Composantes Principales\n",
        " global PCA_exp_var_percent   # Explained Variance des CP en %\n",
        " global eigenvalues           # eigenvalues : Les valeurs propres en fonction des facteurs\n",
        " global PCA_component         # Projection  X sur les Composantes Principales\n",
        " global X_projected           # Projection  X sur les Composantes Principales \n",
        " global pcs                   # Table des  Composantes Principales \n",
        " global listePCName           # liste Nom des Composantes Principales\n",
        " global df_Source             # Garder en memoire le non du df initial\n",
        " global df_Source_Name_String # Garder en memoire le non du df initial sous forme de string\n",
        " #####################################################################################\n",
        " \n",
        "    \n",
        " #### Selection des Features\n",
        " df_Source = df.copy()        # Garder en memoire le non du df initial\n",
        " df_Source_Name_String = f\"{get_df_name(df)}\"\n",
        " features = features_list\n",
        " data = df_Source.loc[:,features] ; data\n",
        " print(pcolors.OK +\"Feature Selection : \"+pcolors.RESET)\n",
        " print(features) \n",
        "\n",
        " # Not Selected Features ( différence  betwen two arrays \n",
        " diff = np.setdiff1d(features,df.columns, assume_unique=False) # https://numpy.org/doc/stable/reference/generated/numpy.setdiff1d.html\n",
        " print(f\"Excluded features from the PCA aanlisys : {diff}\")\n",
        "      \n",
        " #### Preproscessing Standard Scalling \n",
        " print(f\"{pcolors.OK}\\nPreprocessing : Standard Scalling {pcolors.RESET}\")\n",
        " from sklearn.preprocessing import StandardScaler  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        " \n",
        " if standardization == False:\n",
        "   scaler = StandardScaler(with_std=False)   # Standardization = False  toutes les données ont la même unitée\n",
        " else : \n",
        "  scaler = StandardScaler(with_std=True)  # Standardization = True  standardization pour ramener sur un même eéchalles des unitées différentes\n",
        " \n",
        " print(pcolors.WARNING +\"Scaling :\"+pcolors.RESET,scaler)\n",
        " data_scaled = scaler.fit_transform(data)   # Data  Fit entrainement du mmodèle & Transformed => StandardScal\n",
        "\n",
        " #### ACP  : Analyse en Composantes Principales\n",
        " print(pcolors.OK +\"\\nACP : Analyse en Composantes Principales\"+pcolors.RESET)\n",
        " from sklearn.decomposition import PCA # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA\n",
        " X_scaled = data_scaled     # utlisation des données scaled \n",
        "\n",
        " n_components = X_scaled.shape[1]      # Calcul du nombre de composantes principales : 1 par feature(variable)\n",
        " x_list = range(1, n_components+1) \n",
        " x_list = list(x_list)   # variable avec la liste de nos composantes 1 à n ( numéro)\n",
        " print(\"x_list Composantes Principales Numéro :\",list(x_list),\"\\n\")\n",
        "\n",
        " pca = PCA(n_components = n_components)  # On instancie notre ACP\n",
        " pca_fited = pca.fit(X_scaled)   # Fit ACP  : entrainemenent de l' ACP \n",
        " \n",
        " print(pcolors.WARNING +\"pca.explained_variance_ratio : \"+pcolors.RESET, pca.explained_variance_ratio_) # Variance des composantes issues de l'ACP\n",
        " PCA_exp_var = pca.explained_variance_ratio_   \n",
        " PCA_exp_var_percent = (PCA_exp_var *100).round(2)  \n",
        " eigenvalues = (n_components-1)/n_components*pca.explained_variance_  # eigenvalues : Les valeurs propres en fonction des facteurs\n",
        " PCA_component = pca.components_\n",
        " listePCName = [f'PC{i}' for i in range(1, PCA_exp_var.shape[0]+1)] # liste Nom des Composantes Principales\n",
        " PCA_NoiseVariance = pca.components_\n",
        " X_projected = pca.transform(X_scaled)  # projeter X sur les composantes principales\n",
        "\n",
        " # DataFrame pcs  Table des Composantes principales\n",
        " pcs = pd.DataFrame(PCA_component)\n",
        " pcs.columns = listePCName\n",
        " pcs[\"features\"] = features\n",
        " pcs = ColMove(pcs,\"features\",0)\n",
        " # pcs.set_index(\"features\", inplace = True)\n",
        " return pcs\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "# https://sites.google.com/view/aide-python/statistiques/machine-learning-en-python/analyses-en-composantes-principale\n",
        "# https://blog.statoscop.fr/acp-python.html "
      ],
      "metadata": {
        "id": "asDs9TMQ0uXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BatonsBrises():\n",
        " print( f\"\\n{pcolors.OK}Méthode des bâtons brisés {pcolors.RESET} : {pca_fited}\")\n",
        " # Methode des batons brisés : Calcul du seuil theorique de l'axe \n",
        " # Ne conserver que les axes sont l'inertie  est >= au seuil théorique de l'axe \n",
        "\n",
        " n_components = len(features)\n",
        "\n",
        " b_i = 1/np.arange(n_components,0,-1)\n",
        " cumul_b_i = np.cumsum(b_i)\n",
        " cumul_b_i = cumul_b_i[::-1]\n",
        " BatonsBrise = pd.DataFrame({'ComposantePrincipale':listePCName ,'Val_Propre_eigen':eigenvalues,'Seuil':cumul_b_i})\n",
        " BatonsBrise[\"BatonBrise\"] = np.where(BatonsBrise['Val_Propre_eigen']>=BatonsBrise['Seuil'], 'Axe_interessant ', 'Axe_Secondaire')\n",
        " print(\"Axe intéressant  : Valeur Propre Axe eigen>=Seuil  - Axe secondaire  : eigen<Seuil théorique\")\n",
        " print(\"\")\n",
        " print(BatonsBrise.to_markdown())\n",
        " #return BatonsBrise\n"
      ],
      "metadata": {
        "id": "uYrsH6b0aQmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eboulis des valeurs propres  : Explained variance & scree plot => Plotly version\n",
        "def ACP_EboulisPlot(): \n",
        " \"\"\"\n",
        " ###### ML ACP :Eboulis des valeurs propres (Srcree plot)\n",
        " ACP_EboulisPlotVtemp()\n",
        "\n",
        " ----------------------------------------\n",
        " Fonction Allocation variables globales :  ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " pca_fited  : modele PCA fited (entrainé) (pca_fited = pca.fit(X_scaled))\n",
        " PCA_exp_var : Explained Variance des Composantes Principales  ( PCA_exp_var = pca.explained_variance_ratio)\n",
        " X_projected : Projection  X sur les composantes principales ( X_projected = pca.transform(X_scaled) )\n",
        "\n",
        " \"\"\"\n",
        " #### Debug VAR \n",
        " #  print(\"PCA_exp_va: \",PCA_exp_var)\n",
        " #  print(\"PCA_exp_var_percent :\",PCA_exp_var_percent)\n",
        " #  print()\n",
        "\n",
        " # Graphe  Données projetées sur les PC \n",
        "\n",
        " #  # Recuperer l'inertie expliquée par chaque Composante Principale et les cumuler \n",
        " #  Cumul_inertiePC = (PCA_exp_var_percent[x]+PCA_exp_var_percent[y]).round(2)\n",
        "\n",
        " #  labelx =\"PC\" + str(x+1) + \" (\" + str(PCA_exp_var_percent[x])+\"%)\"\n",
        " #  labely =\"PC\" + str(y+1) + \" (\"+ str(PCA_exp_var_percent[y])+\"%)\"\n",
        " #  TitleText = \"Projections des données sur PC\" + str(x+1) + \" & PC\"+ str(y+1) + \" (Cumul inertie \" + str(Cumul_inertiePC) + \"%)\"\n",
        "\n",
        " print( f\"\\n{pcolors.OK}Eboulis des valeurs Propres {pcolors.RESET} : {pca_fited}\")\n",
        "\n",
        " # Nb Composantes principales analyses  ACP\n",
        " n_components = PCA_exp_var.shape[0] # Compte Nb de Composantes principales analyses  PCA \n",
        " x_list = list(range(1, n_components+1) ) # variable avec la liste de nos composantes 1 à n\n",
        " \n",
        " print(f\"Liste Composantes Principales (n = {n_components}): {x_list}\")\n",
        " print(f\"% Variance Expliqué par chaque Composante Principale : {PCA_exp_var_percent.round(3)}\") # alias eigenValues\n",
        " \n",
        " # Cumul Inerties Expliquée par Composantes principales issues ACP \n",
        " exp_var_cumul = (np.cumsum(PCA_exp_var_percent)).round(2) # # Variance des composantes cumulées\n",
        " print(f\"Cumul % Variance Expliqués des Composantes Principales : {exp_var_cumul}\\n\")\n",
        "\n",
        " # graphique Plotlty Eboulis des valeurs propres \n",
        "\n",
        " fig = px.bar( x= x_list, y=PCA_exp_var_percent,\n",
        "            barmode= 'group', # 'group', 'overlay' or 'relative'\n",
        "            orientation= \"v\",\n",
        "            opacity= 1 , # Opcity  0-1 \n",
        "            labels= dict(x=\"Composante\", y=\"Variance % Inertie\"),\n",
        "            text_auto= True, # True '.1f',\n",
        "            title= \"Eboulis des valeurs propres\",\n",
        "            width= 1000 , height= 500)\n",
        "\n",
        "\n",
        " fig0 = px.line(x= x_list, y= exp_var_cumul,\n",
        "              labels=dict(x=\"Composante\", y=\"Variance Cumulés % inertie\"),              \n",
        "              title='Variance cumulées',             \n",
        "              markers= True,  )           \n",
        " fig0.update_traces(line_color='red',fillcolor='rgba(255, 0, 0, 0.1)',line_width=3,mode='lines+markers',marker_size=15)\n",
        "\n",
        "\n",
        " # add_trace\n",
        " fig.add_trace(fig0.data[0])\n",
        "\n",
        " # Marqueurs \n",
        " fig.add_hline(y=100, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
        " fig.add_hline(y=80, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
        " # fig.add_hline(y=50, line_width=1, line_dash=\"dash\", line_color=\"green\")\n",
        "\n",
        "\n",
        " # legend \n",
        " fig.update_traces(showlegend=False)\n",
        "\n",
        "\n",
        " # custum axes\n",
        " fig.update_layout(\n",
        "    xaxis_title=\"Composantes principales\",\n",
        "    yaxis_title=\"Pourcentage d'inertie expliquée\",\n",
        "    yaxis = dict( tickmode = 'linear',    tick0 = 0,    dtick =10),)\n",
        "\n",
        " fig.update_yaxes(ticksuffix=\"%\", showgrid=True)\n",
        "\n",
        " fig.update_xaxes(tickprefix=\"PC\", showgrid=True,type='category')\n",
        " fig.update_layout(hovermode=\"x\")\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"ACP_Eboulis_valeurs_Propres\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        "# Calcul Critere de Kaiser\n",
        " kaiser=(100/n_components)/100\n",
        " print(f\"\\n{pcolors.OK}Critère de Kaiser :{pcolors.RESET} Les axes dont l'inertie expliquée est < {kaiser}% sont considérés comme Non important(s) \")\n",
        "\n",
        "\n",
        "# ACP_EboulisPlot()"
      ],
      "metadata": {
        "id": "JcRR1z1I08-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eboulis des valeurs propres  : Explained variance & scree plot => Plotly version\n",
        "def ACP_EboulisPlotV0(): \n",
        " \"\"\"\n",
        " ###### ML ACP :Eboulis des valeurs propres (Srcree plot)\n",
        " ACP_EboulisPlotVtemp()\n",
        "\n",
        " ----------------------------------------\n",
        " Fonction Allocation variables globales :  ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " pca_fited  : modele PCA fited (entrainé) (pca_fited = pca.fit(X_scaled))\n",
        " PCA_exp_var : Explained Variance des Composantes Principales  ( PCA_exp_var = pca.explained_variance_ratio)\n",
        " X_projected : Projection  X sur les composantes principales ( X_projected = pca.transform(X_scaled) )\n",
        "\n",
        " \"\"\"\n",
        " #### Debug VAR \n",
        " #  print(\"PCA_exp_va: \",PCA_exp_var)\n",
        " #  print(\"PCA_exp_var_percent :\",PCA_exp_var_percent)\n",
        " #  print()\n",
        "\n",
        " # Graphe  Données projetées sur les PC \n",
        "\n",
        " #  # Recuperer l'inertie expliquée par chaque Composante Principale et les cumuler \n",
        " #  Cumul_inertiePC = (PCA_exp_var_percent[x]+PCA_exp_var_percent[y]).round(2)\n",
        "\n",
        " #  labelx =\"PC\" + str(x+1) + \" (\" + str(PCA_exp_var_percent[x])+\"%)\"\n",
        " #  labely =\"PC\" + str(y+1) + \" (\"+ str(PCA_exp_var_percent[y])+\"%)\"\n",
        " #  TitleText = \"Projections des données sur PC\" + str(x+1) + \" & PC\"+ str(y+1) + \" (Cumul inertie \" + str(Cumul_inertiePC) + \"%)\"\n",
        "\n",
        " print( f\"\\n{pcolors.OK}Eboulis des valeurs Propres {pcolors.RESET} : {pca_fited}\")\n",
        "\n",
        " # Nb Composantes principales analyses  ACP\n",
        " n_components = PCA_exp_var.shape[0] # Compte Nb de Composantes principales analyses  PCA \n",
        " x_list = list(range(1, n_components+1) ) # variable avec la liste de nos composantes 1 à n\n",
        " \n",
        " print(f\"Liste Composantes Principales (n = {n_components}): {x_list}\")\n",
        " print(f\"% Variance Expliqué par chaque Composante Principale : {PCA_exp_var_percent}\")\n",
        " \n",
        " # Cumul Inerties Expliquée par Composantes principales issues ACP \n",
        " exp_var_cumul = (np.cumsum(PCA_exp_var_percent)).round(2) # # Variance des composantes cumulées\n",
        " print(f\"Cumul % Variance Expliqués des Composantes Principales : {exp_var_cumul}\\n\")\n",
        "\n",
        "\n",
        "\n",
        " # graphique Plotlty Eboulis des valeurs propres \n",
        "\n",
        " fig = px.bar( x= x_list, y=PCA_exp_var_percent,\n",
        "            barmode= 'group', # 'group', 'overlay' or 'relative'\n",
        "            orientation= \"v\",\n",
        "            opacity= 1 , # Opcity  0-1 \n",
        "            labels= dict(x=\"Composante\", y=\"Variance % Inertie\"),\n",
        "            text_auto= True, # True '.1f',\n",
        "            title= \"Eboulis des valeurs propres\",\n",
        "            width= 1000 , height= 500)\n",
        " \n",
        " fig0 = px.line(x= x_list, y= exp_var_cumul,\n",
        "              labels=dict(x=\"Composante\", y=\"Variance Cumulés % inertie\"),              \n",
        "              title='Variance cumulées',             \n",
        "              markers= True,  )           \n",
        " fig0.update_traces(line_color='red',fillcolor='rgba(255, 0, 0, 0.1)',line_width=3,mode='lines+markers',marker_size=15)\n",
        "\n",
        "\n",
        " # add_trace\n",
        " fig.add_trace(fig0.data[0])\n",
        " fig.add_trace(fig2.data[0])\n",
        "\n",
        " # Marqueurs \n",
        " fig.add_hline(y=100, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
        " fig.add_hline(y=80, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
        " # fig.add_hline(y=50, line_width=1, line_dash=\"dash\", line_color=\"green\")\n",
        "\n",
        "\n",
        " # legend \n",
        " fig.update_traces(showlegend=False)\n",
        "\n",
        "\n",
        " # custum axes\n",
        " fig.update_layout(\n",
        "    xaxis_title=\"Composantes principales\",\n",
        "    yaxis_title=\"Pourcentage d'inertie expliquée\",\n",
        "    yaxis = dict( tickmode = 'linear',    tick0 = 0,    dtick =10),)\n",
        "\n",
        " fig.update_yaxes(ticksuffix=\"%\", showgrid=True)\n",
        "\n",
        " fig.update_xaxes(tickprefix=\"PC\", showgrid=True,type='category')\n",
        " fig.update_layout(hovermode=\"x\")\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"ACP_Eboulis_valeurs_Propres\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "# ACP_EboulisPlot()"
      ],
      "metadata": {
        "id": "sIFGTJb3cynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_projections(ColorCol,PCx,PCy):\n",
        " \"\"\"\n",
        " ###### ML ACP : Projections des données sur les Composantes Principales\n",
        " ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " Param. affichage graphique : \n",
        " df : dataframe source \n",
        " ColorCol = 'Variable_Non_Incluse_Features_ACP'  pour colorer les point projetés \n",
        "\n",
        " PCx : Num Composantes principales à afficher sur l'axe des ordonnées  1 = PC1\n",
        " PCy : Num Composantes principales à afficher sur l'axe des abcisses 2 = PC2\n",
        "\n",
        " ----------------------------------------\n",
        " Fonction Allocation variables globales :  ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " pca_fited  : modele PCA fited (entrainé) (pca_fited = pca.fit(X_scaled))\n",
        " PCA_exp_var : Explained Variance des Composantes Principales  ( PCA_exp_var = pca.explained_variance_ratio)\n",
        " PCA_exp_var_percent  : Explained Variance des CP en %  PCA_exp_var_percent = (PCA_exp_var *100).round(2)\n",
        " X_projected : Projection  X sur les composantes principales ( X_projected = pca.transform(X_scaled) )\n",
        "  \n",
        " \"\"\"\n",
        " ###### Projections des données sur les Composantes Principales ######\n",
        "\n",
        " # Graphe  Données projetées sur les PC \n",
        "\n",
        " # Selection des Composantes Principales  à afficher \n",
        " x , y  = PCx - 1 , PCy - 1 # (-1 car liste Python commence à Zero )\n",
        "\n",
        " # Recuperer l'inertie expliquée par chaque Composante Principale et les cumuler \n",
        " Cumul_inertiePC = (PCA_exp_var_percent[x]+PCA_exp_var_percent[y]).round(2)\n",
        " \n",
        " labelx = f\"PC{x+1} ({PCA_exp_var_percent[x]}%)\"\n",
        " labely = f\"PC {y+1} ({PCA_exp_var_percent[y]}%)\"\n",
        " TitleText = f\"Projections des données sur PC {x+1} & PC {y+1} (Cumul inertie expliquée {Cumul_inertiePC}%)\"\n",
        "\n",
        " fig = px.scatter(df_Source,\n",
        "                 x=X_projected[:,x], y=X_projected[:, y],\n",
        "                 color= ColorCol,color_continuous_scale='earth',  # _r : reverse color \n",
        "                 labels= dict(x=labelx, y=labely),\n",
        "                 title = TitleText,\n",
        "                 width=1000 , height=500)\n",
        " fig.show()\n",
        " print()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"_Projections\"\n",
        " col=f\"_ACP_PC{PCx}_PC{PCy}\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        "# ACP_projections(ColorCol,PCx,PCy)"
      ],
      "metadata": {
        "id": "-ropZZhnudgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_MatrixPlot_Projections(ColorCol,width,height,Nombre_Max_PC_graphique):\n",
        " \"\"\" \n",
        " ###### ML ACP : Matrice des Projections des individus sur les plans factoriels\n",
        " ACP_MatrixPlot_Projections_ModeleExistant(df,ColorCol,PCA_exp_var,X_projected,width,height,Nombre_Max_PC_graphique)\n",
        "\n",
        " df : dataframe source \n",
        " ColorCol = 'Variable_Non_Incluse_Features_ACP'  pour colorer les point projetés \n",
        "\n",
        " Param. affichage graphque : \n",
        " width,height,  : Largeur et hauteur du graphique en pixel \n",
        " Nombre_Max_PC_graphique  : Nombre Max de Composantes principales à afficher\n",
        "\n",
        " ----------------------------------------\n",
        " Fonction Allocation variables globales :  ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " pca_fited  : modele PCA fited (entrainé) (pca_fited = pca.fit(X_scaled))\n",
        " PCA_exp_var : Explained Variance des Composantes Principales  ( PCA_exp_var = pca.explained_variance_ratio)\n",
        " X_projected : Projection  X sur les composantes principales ( X_projected = pca.transform(X_scaled) )\n",
        "\n",
        " \"\"\"\n",
        " ###### Matrice des Projections des individus sur les plans factoriels  ###### \n",
        " \n",
        "\n",
        " # Variables pour contruire le graphique \n",
        " if Nombre_Max_PC_graphique <= PCA_exp_var.shape[0]: # Tester si  Nombre de PC à afficher dans la matrice est valide\n",
        "   Nombre_Max_PC = Nombre_Max_PC_graphique\n",
        " else :\n",
        "   print(pcolors.FAIL + \"\\nErreur : Nombre_Max_PC_graphique > au nombre total de Composantes Principales (\"+str(PCA_exp_var.shape[0])+\")\\n\" + pcolors.RESET)\n",
        "   Nombre_Max_PC = PCA_exp_var.shape[0]\n",
        "\n",
        " CumulInertie = (PCA_exp_var[:Nombre_Max_PC].sum()*100).round(2)\n",
        " TitleText = f\"Matrice des Projections sur les {Nombre_Max_PC} premières Composantes Principales (Cumul intertie : {CumulInertie}%)\"\n",
        "\n",
        " labels = {\n",
        "    str(i): f\"PC{i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(PCA_exp_var * 100) }  # Label des PC avec % intertie\n",
        "\n",
        " labels_y = {\n",
        "    str(i): f\"PC{i+1})\"\n",
        "    for i, var in enumerate(PCA_exp_var * 100) }  # Label des PC avec % intertie\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " labels[\"color\"] = ColorCol   # Append Legende pour la Couleur\n",
        "\n",
        " # Graphique plotly \n",
        " fig = px.scatter_matrix(\n",
        "    X_projected ,\n",
        "    labels=labels,\n",
        "    dimensions=range(Nombre_Max_PC), # Nombre de PC à afficher dans la matrice \n",
        "    color=df_Source[ColorCol], \n",
        "    title=TitleText,\n",
        "    width=width,height=height)\n",
        " fig.update_traces(diagonal_visible=True)\n",
        "\n",
        " fig.update_layout(yaxis_title= labels_y)\n",
        "\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"ACP_Matrice_Projections\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "# Remix Code : https://plotly.com/python/pca-visualization/\n",
        "# Scatterplot Matrix  : https://plotly.com/python/splom/"
      ],
      "metadata": {
        "id": "yZtXUNPYHJwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_MatrixPlot_Projections(ColorCol,width,height,Nombre_Max_PC_graphique,):\n",
        " \"\"\" \n",
        " ###### ML ACP : Matrice des Projections des individus sur les plans factoriels\n",
        " ACP_MatrixPlot_Projections_ModeleExistant(df,ColorCol,PCA_exp_var,X_projected,width,height,Nombre_Max_PC_graphique)\n",
        "\n",
        " df : dataframe source \n",
        " ColorCol = 'Variable_Non_Incluse_Features_ACP'  pour colorer les point projetés \n",
        "\n",
        " Param. affichage graphque : \n",
        " width,height,  : Largeur et hauteur du graphique en pixel \n",
        " Nombre_Max_PC_graphique  : Nombre Max de Composantes principales à afficher\n",
        "\n",
        " ----------------------------------------\n",
        " Fonction Allocation variables globales :  ACP_projections(pca_fited,X_projected,ColorCol,PCx,PCy)\n",
        "\n",
        " pca_fited  : modele PCA fited (entrainé) (pca_fited = pca.fit(X_scaled))\n",
        " PCA_exp_var : Explained Variance des Composantes Principales  ( PCA_exp_var = pca.explained_variance_ratio)\n",
        " X_projected : Projection  X sur les composantes principales ( X_projected = pca.transform(X_scaled) )\n",
        "\n",
        " \"\"\"\n",
        " ###### Matrice des Projections des individus sur les plans factoriels  ###### \n",
        " \n",
        "\n",
        " # Variables pour contruire le graphique \n",
        " if Nombre_Max_PC_graphique <= PCA_exp_var.shape[0]: # Tester si  Nombre de PC à afficher dans la matrice est valide\n",
        "   Nombre_Max_PC = Nombre_Max_PC_graphique\n",
        " else :\n",
        "   print(pcolors.FAIL + \"\\nErreur : Nombre_Max_PC_graphique > au nombre total de Composantes Principales (\"+str(PCA_exp_var.shape[0])+\")\\n\" + pcolors.RESET)\n",
        "   Nombre_Max_PC = PCA_exp_var.shape[0]\n",
        "\n",
        " CumulInertie = (PCA_exp_var[:Nombre_Max_PC].sum()*100).round(2)\n",
        " TitleText = f\"Matrice des Projections sur les {Nombre_Max_PC} premières Composantes Principales (Cumul intertie : {CumulInertie}%)\"\n",
        "\n",
        " labels = {\n",
        "    str(i): f\"PC{i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(PCA_exp_var * 100) }  # Label des PC avec % intertie\n",
        " labels[\"color\"] = ColorCol   # Append Legende pour la Couleur\n",
        "\n",
        " # Graphique plotly \n",
        " fig = px.scatter_matrix(\n",
        "    X_projected ,\n",
        "    labels=labels,\n",
        "    dimensions=range(Nombre_Max_PC), # Nombre de PC à afficher dans la matrice \n",
        "    color=df_Source[ColorCol], \n",
        "    title=TitleText,\n",
        "    width=width,height=height)\n",
        " fig.update_traces(diagonal_visible=True)\n",
        " \n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"ACP_Matrice_Projections\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "# Remix Code : https://plotly.com/python/pca-visualization/\n",
        "# Scatterplot Matrix  : https://plotly.com/python/splom/"
      ],
      "metadata": {
        "id": "_9d0Cg4MHVOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACP Cercles des correlations \n",
        "def ACP_CercleCorrelationsPlotNolabel(PCx,PCy): \n",
        " \"\"\"\n",
        " Cercle des correlation \n",
        "  PCA_exp_var :  pca.explained_variance_ratio =>  Variance des composantes issues de l'ACP\n",
        "  x  : Numéro composante principale à affciher sur l'axes des abscisses\n",
        "  y : Numéro composante principale à affciher sur l'axes des abscisses\n",
        "  Numéro composante ( calcul avec - 1 car liste python commence à 0)\n",
        " \n",
        "  PCA_exp_var :  Variance des composantes issues de l'ACP\n",
        " \n",
        "\n",
        " PCA_component  \n",
        " \"\"\"\n",
        " \n",
        "\n",
        " # Selection des Composantes Principales  à afficher   (Issues APC)\n",
        " x = PCx - 1  # (pas de -1 car liste python  )\n",
        " y = PCy- 1 \n",
        " \n",
        " dataPlot = pcs\n",
        " fig = px.scatter( pcs.round(2),\n",
        "                  x= pcs.iloc[:,x+1],\n",
        "                  y= pcs.iloc[:,y+1],\n",
        "                  color = \"features\",\n",
        "                  hover_name = \"features\",\n",
        "                  hover_data = pcs.columns,\n",
        "                  #text = \"features\",                                                 \n",
        "                  labels= dict(x= \"x_\" + pcs.columns[x+1], y=\"y_\" + pcs.columns[y+1]),\n",
        "                  )\n",
        " \n",
        " fig.update_traces(marker_size=10,)\n",
        " #fig.update_traces(textposition='top center')\n",
        "\n",
        "\n",
        " \n",
        " for i in range(0,pcs.shape[0]):\n",
        "   fig.add_annotation(\n",
        "     x= pcs.iloc[i,PCx],  # arrows' head\n",
        "     y= pcs.iloc[i,PCy],  # arrows' head\n",
        "     ax=0,  # arrows' tail\n",
        "     ay=0,  # arrows' tail\n",
        "     xref='x',\n",
        "     yref='y',\n",
        "     axref='x',\n",
        "     ayref='y',\n",
        "     text=\"\",  #'' if you want only the arrow\n",
        "     showarrow=True,\n",
        "     arrowhead=1,\n",
        "     arrowsize=1,\n",
        "     arrowwidth=1,\n",
        "     arrowcolor='grey')\n",
        "\n",
        " \n",
        " \n",
        " # Add circles # https://plotly.com/python/shapes/\n",
        " fig.add_shape(type=\"circle\",\n",
        "   xref=\"x\", yref=\"y\",\n",
        "   x0= -1, y0= -1, x1= 1, y1= 1,\n",
        "   line_color=\"Lightgrey\", )   #\"LightSeaGreen\"\n",
        "\n",
        " rayon_inner_circle = 0.4\n",
        " fig.add_shape(type=\"circle\",\n",
        "    xref=\"x\", yref=\"y\",\n",
        "    fillcolor=\"yellow\",\n",
        "    opacity=0.06,\n",
        "    x0=-rayon_inner_circle, y0= -rayon_inner_circle, x1= rayon_inner_circle, y1= rayon_inner_circle,\n",
        "    line_color=\"yellow\",)\n",
        "\n",
        " # Marqueurs \n",
        " fig.add_hline(y=0, line_width=1, line_dash=None, line_color=\"grey\")\n",
        " fig.add_vline(x=0, line_width=1, line_dash=None, line_color=\"grey\")\n",
        " # Axes \n",
        " fig.update_xaxes(range=[-1.1, 1.1])\n",
        " fig.update_yaxes(range=[-1.1, 1.1])\n",
        "\n",
        "\n",
        " # Variable costum axes\n",
        " # nom des axes, avec le pourcentage d'inertie expliqué\n",
        " xAxis_Label = 'PC{} ({}%)'.format(x+1, round(100*PCA_exp_var[x],1))\n",
        " yAxis_Label = 'PC{} ({}%)'.format(y+1, round(100*PCA_exp_var[y],1))\n",
        "\n",
        " # Variable titre graphique \n",
        " CumulInertie =  ((PCA_exp_var[x]+PCA_exp_var[y])*100).round(1)\n",
        " TilteText = \"Cercles des corrélations  (\"+ str(CumulInertie) + \"% intertie expliquée)\"\n",
        "\n",
        " print(TilteText)\n",
        " \n",
        " fig.update_layout(width=1600, height=700,\n",
        "    xaxis_title= xAxis_Label,\n",
        "    yaxis_title= yAxis_Label,\n",
        "    xaxis = dict( tickmode = 'linear',    tick0 = -1.1,    dtick =0.25),\n",
        "    yaxis = dict( tickmode = 'linear',    tick0 = -1.1,    dtick =0.25),\n",
        "    title={\n",
        "        'text': TilteText,\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'}) \n",
        " \n",
        " fig.update_yaxes(scaleanchor = \"x\", scaleratio = 1,  )\n",
        "\n",
        " fig.show()\n",
        " \n",
        " # Export Plotly graph to .html\n",
        " file_name =\"Cercle_Correlations\"\n",
        " col=f\"_ACP_PC{PCx}_PC{PCy}_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        " return pcs\n",
        "# ACP_CercleCorrelationsPlot(1,2)"
      ],
      "metadata": {
        "id": "_VPH61JwSkRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACP Cercles des correlations \n",
        "def ACP_CercleCorrelationsPlot(PCx,PCy): \n",
        " \"\"\"\n",
        " Cercle des correlation \n",
        "  PCA_exp_var :  pca.explained_variance_ratio =>  Variance des composantes issues de l'ACP\n",
        "  x  : Numéro composante principale à affciher sur l'axes des abscisses\n",
        "  y : Numéro composante principale à affciher sur l'axes des abscisses\n",
        "  Numéro composante ( calcul avec - 1 car liste python commence à 0)\n",
        " \n",
        "  PCA_exp_var :  Variance des composantes issues de l'ACP\n",
        " \n",
        "\n",
        " PCA_component  \n",
        " \"\"\"\n",
        " \n",
        "\n",
        " # Selection des Composantes Principales  à afficher   (Issues APC)\n",
        " x = PCx - 1  # (pas de -1 car liste python  )\n",
        " y = PCy- 1 \n",
        " \n",
        " dataPlot = pcs\n",
        " fig = px.scatter( pcs,\n",
        "                  x= pcs.iloc[:,x+1],\n",
        "                  y= pcs.iloc[:,y+1],\n",
        "                  color = \"features\",\n",
        "                  text = \"features\",                                                 \n",
        "                  labels= dict(x= \"x_\" + pcs.columns[x+1], y=\"y_\" + pcs.columns[y+1]),\n",
        "                  )\n",
        " \n",
        " fig.update_traces(marker_size=15,)\n",
        " fig.update_traces(textposition='top center')\n",
        "\n",
        "\n",
        " \n",
        " for i in range(0,pcs.shape[0]):\n",
        "   fig.add_annotation(\n",
        "     x= pcs.iloc[i,PCx],  # arrows' head\n",
        "     y= pcs.iloc[i,PCy],  # arrows' head\n",
        "     ax=0,  # arrows' tail\n",
        "     ay=0,  # arrows' tail\n",
        "     xref='x',\n",
        "     yref='y',\n",
        "     axref='x',\n",
        "     ayref='y',\n",
        "     text=\"\",  #'' if you want only the arrow\n",
        "     showarrow=True,\n",
        "     arrowhead=1,\n",
        "     arrowsize=1,\n",
        "     arrowwidth=1,\n",
        "     arrowcolor='grey')\n",
        "\n",
        " \n",
        " \n",
        " # Add circles # https://plotly.com/python/shapes/\n",
        " fig.add_shape(type=\"circle\",\n",
        "   xref=\"x\", yref=\"y\",\n",
        "   x0= -1, y0= -1, x1= 1, y1= 1,\n",
        "   line_color=\"Lightgrey\", )   #\"LightSeaGreen\"\n",
        "\n",
        " rayon_inner_circle = 0.4\n",
        " fig.add_shape(type=\"circle\",\n",
        "    xref=\"x\", yref=\"y\",\n",
        "    fillcolor=\"yellow\",\n",
        "    opacity=0.06,\n",
        "    x0=-rayon_inner_circle, y0= -rayon_inner_circle, x1= rayon_inner_circle, y1= rayon_inner_circle,\n",
        "    line_color=\"yellow\",)\n",
        "\n",
        " # Marqueurs \n",
        " fig.add_hline(y=0, line_width=1, line_dash=None, line_color=\"grey\")\n",
        " fig.add_vline(x=0, line_width=1, line_dash=None, line_color=\"grey\")\n",
        " # Axes \n",
        " fig.update_xaxes(range=[-1.1, 1.1])\n",
        " fig.update_yaxes(range=[-1.1, 1.1])\n",
        "\n",
        "\n",
        " # Variable costum axes\n",
        " # nom des axes, avec le pourcentage d'inertie expliqué\n",
        " xAxis_Label = 'PC{} ({}%)'.format(x+1, round(100*PCA_exp_var[x],1))\n",
        " yAxis_Label = 'PC{} ({}%)'.format(y+1, round(100*PCA_exp_var[y],1))\n",
        "\n",
        " # Variable titre graphique \n",
        " CumulInertie =  ((PCA_exp_var[x]+PCA_exp_var[y])*100).round(1)\n",
        " TilteText = \"Cercles des corrélations  (\"+ str(CumulInertie) + \"% intertie expliquée)\"\n",
        "\n",
        " print(TilteText)\n",
        " \n",
        " fig.update_layout( width=800, height=500,\n",
        "    xaxis_title= xAxis_Label,\n",
        "    yaxis_title= yAxis_Label,\n",
        "    xaxis = dict( tickmode = 'linear',    tick0 = -1.1,    dtick =0.25),\n",
        "    yaxis = dict( tickmode = 'linear',    tick0 = -1.1,    dtick =0.25),\n",
        "    title={\n",
        "        'text': TilteText,\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'}) \n",
        " \n",
        " fig.update_yaxes(scaleanchor = \"x\", scaleratio = 1,  )\n",
        "\n",
        " fig.show()\n",
        " \n",
        " # Export Plotly graph to .html\n",
        " file_name =\"Cercle_Correlations\"\n",
        " col=f\"_ACP_PC{PCx}_PC{PCy}_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        " return pcs\n",
        "# ACP_CercleCorrelationsPlot(1,2)"
      ],
      "metadata": {
        "id": "TC2zuiwjdrTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_heatmapPC():\n",
        " # Heatmap Composantes (Components) & features Version Plotly \n",
        " print(pcolors.WARNING +\"\\nHeatmap : Details Composantes principales \"+pcolors.RESET)\n",
        "  \n",
        " pcs_heatmap = pcs.copy()\n",
        " pcs_heatmap.set_index('features', inplace=True)\n",
        " pcs_heatmap\n",
        "\n",
        " fig = px.imshow( pcs_heatmap, \n",
        "                 text_auto='.3f',\n",
        "                 labels=dict(x=\"Composantes\", y=\"Features (variables)\"),\n",
        "                 color_continuous_scale=\"prgn\",   # \"tropic_r\",\n",
        "                 color_continuous_midpoint=0, zmin=-1, zmax=1,\n",
        "                 title=\" Heatmap contibution des features aux Composantes Principales\",\n",
        "                 width=1200 ,height=800)\n",
        " fig.show()\n",
        " \n",
        " # Export Plotly graph to .html\n",
        " file_name =\"ACP_HeatMap_ComposantesPrincipales\"\n",
        " col = \"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        "# ACP_heatmapPC()"
      ],
      "metadata": {
        "id": "AoYPBkxr4mm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FeatureCorrelationMatrix(df,Selected_Method_Name):\n",
        " # heatmap Correlation of a data frame  features Version Plotly \n",
        " #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
        " print(f\"\\n{pcolors.WARNING}Heatmap : Matrice correlations variables (features) {pcolors.RESET}\")\n",
        "  \n",
        " #pcs_heatmap = pcs.copy()\n",
        " #pcs_heatmap.set_index('features', inplace=True)\n",
        " #pcs_heatmap\n",
        " \n",
        " df_Name_String = f\"{get_df_name(df)}\"\n",
        " list_Of_features = df.columns\n",
        " Selected_Method = Selected_Method_Name   # choice  {‘pearson’, ‘kendall’, ‘spearman’}\n",
        " Liste_methodes = [\"pearson\", \"kendall\", \"spearman\"]\n",
        "\n",
        " if Selected_Method not in Liste_methodes:\n",
        "  print(f\"{pcolors.FAIL}Erreur de saisie Selected_Method :{pcolors.RESET} choisir un des méthode de cette liste {Liste_methodes}\")\n",
        "  print(\"                                  ..... pearson appliquée par défaut\" )\n",
        "  Selected_Method = \"pearson\" \n",
        "    \n",
        "\n",
        " corr_matrix = df.corr(method=Selected_Method) # Dataframe Matrice de Correlation\n",
        " \n",
        " # graphique de Corrélations Plotly \n",
        " fig = px.imshow(corr_matrix, \n",
        "                 text_auto='.3f',\n",
        "                 labels=dict(color=f'Coef Corrélation {Selected_Method}'),\n",
        "                 color_continuous_scale=\"prgn\",   # \"tropic_r\",\n",
        "                 color_continuous_midpoint=0, zmin=-1, zmax=1,\n",
        "                 title= f\"Heatmap Martrice Corrélations {Selected_Method} des variables {df_Name_String}\",\n",
        "                 width=1400 ,height=700)\n",
        " fig.show()\n",
        "\n",
        "\n",
        " \n",
        " # Export Plotly graph to .html\n",
        " file_name =f\"Features_CorrelationMatrix_{Selected_Method}\"\n",
        " col = \"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Name_String,col,fig,file_name)\n",
        "\n",
        "\n",
        " return corr_matrix\n"
      ],
      "metadata": {
        "id": "fSmd1p59dAI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ACP_global(df,features_list,ColorCol,FC1x ,FC1y,FC2x,FC2y,standardization=True):\n",
        " \"\"\"\n",
        "\n",
        " \"\"\"\n",
        " \n",
        " import os # https://docs.python.org/fr/3/library/os.html\n",
        " ExportdfPATH = f\"/content/Exports/{get_df_name(df)}/\"\n",
        " os.makedirs(ExportdfPATH,exist_ok=True)\n",
        " print(f\"Repertoire : {ExportdfPATH}\")\n",
        " DataFrameStringName = get_df_name(df) ; print(DataFrameStringName )\n",
        "\n",
        " # Charger l'ACP \n",
        " ACP_ML(df,features_list,standardization=True)\n",
        "\n",
        " #### Configuration du widget \n",
        " import ipywidgets as widgets #https://ipywidgets.readthedocs.io/en/stable/ \n",
        " tab0 = widgets.Output()\n",
        " tab1 = widgets.Output()\n",
        " tab2 = widgets.Output()\n",
        " tab3 = widgets.Output()\n",
        " tab4 = widgets.Output()\n",
        " tab5 = widgets.Output()\n",
        " tab6 = widgets.Output()\n",
        " tab7 = widgets.Output()\n",
        " tab8 = widgets.Output()\n",
        " tab9 = widgets.Output()\n",
        " \n",
        "\n",
        " #import ipywidgets as widgets #https://ipywidgets.readthedocs.io/en/stable/ \n",
        " #  for i in range(0,6):\n",
        " #   exec(\"tab\"+str(i)+\" = widgets.Output()\")\n",
        "\n",
        " tab = widgets.Tab(children = [tab0,tab1,tab2,tab3,tab4,tab5,tab6,tab7,tab8,tab9])\n",
        " tab.set_title(0, 'Eboulis Valeurs Propres')\n",
        " tab.set_title(1, 'Bâtons Brisés')\n",
        " tab.set_title(2, 'Heatmap Composantes & features')\n",
        " tab.set_title(3, f'Cercle corr. PC {FC1x} & {FC1y}')\n",
        " tab.set_title(4, f'Cercle  corr. PC {FC2x} & {FC2y}')\n",
        " tab.set_title(5, f'Projection PC {FC1x} & {FC1y}')\n",
        " tab.set_title(6, f'Projection PC {FC2x} & {FC2y}')\n",
        " tab.set_title(7, f'Tables Composantes Princip. ')\n",
        " tab.set_title(8, 'Matrice corr. features')\n",
        " tab.set_title(9, 'Liste des fct APC')\n",
        "\n",
        " display(tab)\n",
        "\n",
        " with tab0:\n",
        "  # Afficher Graphique boulis des valeurs propres  \n",
        "  ACP_EboulisPlot()\n",
        "\n",
        " with tab1:\n",
        "  # AfficherTableau Batons brisés  \n",
        "  BatonsBrises()\n",
        "\n",
        " with tab2:\n",
        "  # Heatmap Composantes (Components) & features Version Plotly \n",
        "  print(f\"\\n{pcolors.WARNING}Details Composantes principales{pcolors.RESET}\")\n",
        "  ACP_heatmapPC()\n",
        "  pcs\n",
        " \n",
        " with tab3:\n",
        "  # Afficher Cercles Correlations 1  \n",
        "  #ACP_CercleCorrelationsPlot(FC1x,FC1y)\n",
        "  ACP_CercleCorrelationsPlotNolabel(FC1x,FC1y)\n",
        "  \n",
        "\n",
        " with tab4:\n",
        "  # Afficher Cercles Correlations 2  \n",
        "  #ACP_CercleCorrelationsPlot(FC2x,FC2y)\n",
        "  ACP_CercleCorrelationsPlotNolabel(FC2x,FC2y)\n",
        "\n",
        " with tab5:\n",
        "  # Afficher Plan fatoriel  1 \n",
        "  ACP_projections(ColorCol,FC1x,FC1y)\n",
        "  ACP_projections(ColorCol,FC2x,FC2y)  \n",
        "\n",
        " with tab6:\n",
        "  # Afficher Plan fatoriel  1 \n",
        "  ACP_projections(ColorCol,FC2x,FC2y)\n",
        "  ACP_projections(ColorCol,FC1x,FC1y)\n",
        "  \n",
        " with tab7:\n",
        "  # Afficher tables des composantes\n",
        "  print(f\"{pcolors.WARNING}Table Composantes principales{pcolors.RESET} (pcs)\\n\")\n",
        "  print(pcs.to_markdown())\n",
        "\n",
        " with tab8:\n",
        "  # Afficher Matrice Correlation des variables data set initial\n",
        "  print(f\"{pcolors.OK}Matrice des correlations des features {pcolors.RESET}\")\n",
        "  FeatureCorrelationMatrix(df,\"pearson\")\n",
        "\n",
        "\n",
        " with tab9:\n",
        "  print(f\"{pcolors.OK}Liste Fonctions ACP Analyse des Composantes Principales  : reduction features {pcolors.RESET}\")\n",
        "  print(\" ACP_global(df,features_list,ColorCol,FC1x ,FC1y,FC2x ,FC2y,standardization=True)\")\n",
        "  print(\"  ACP_EboulisPlot()\")\n",
        "  print(\"  ACP_heatmapPC()\")\n",
        "  print(\"  ACP_CercleCorrelationsPlot(FCx,FCy)  or  ACP_CercleCorrelationsPlotNolabel(PCx,PCy)  \")\n",
        "  print(\"  ACP_projections(ColorCol,FCx,FCy)\")\n",
        "  print(\"  ACP_MatrixPlot_Projections(ColorCol,width,height,Nombre_Max_PC_graphique)\")\n",
        "  print(\"      Color Choice  color_continuous_scale='earth',  # _r : reverse color    viridis deep bluered inferno magma rainbow\")\n",
        "  print(\"      https://plotly.com/python/colorscales/\")\n",
        "  print(\"  DataFrame des Composantes Principales : cps)\")\n",
        "\n",
        "  print(\"\\n scikit-learn : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\")\n",
        " # Matrices projection  10 max \n",
        " \n",
        " nbr_features = len(features)\n",
        " ACP_MatrixPlot_Projections(ColorCol,1800,600,nbr_features)\n",
        "\n",
        "# iris = iris.copy()\n",
        "# features_list = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n",
        "# ColorCol = \"species\"\n",
        "# FC1x ,FC1y = 1,2\n",
        "# FC2x ,FC2y = 3,4\n",
        "# ACP_global(iris,features_list,ColorCol,FC1x ,FC1y,FC2x ,FC2y,standardization=True)"
      ],
      "metadata": {
        "id": "cgC3pi9YRE5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML ACP Classification KMean"
      ],
      "metadata": {
        "id": "kAULtBIYS3qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Scaling_ML(df,features_list,LabelColString,standardization=True): \n",
        " \"\"\"\n",
        " Machine learning : Preprocessing  StandardScaler \n",
        " \n",
        " (df,features_list,standardization=True)\n",
        "  \n",
        "  df : Source data frame \n",
        "  features_list : list of selected features\n",
        "\n",
        " Output Globals Variable : \n",
        "   global df_Source             # Source DataFrame  (avant feaure Sélection)\n",
        "   global features              # Selected features\n",
        "   global df_Source             # Garder en memoire le non du df initial\n",
        "   global df_Source_Name_String # Garder en memoire le non du df initial sous forme de string\n",
        "   \n",
        "   data                      \n",
        "   ==> data_scaled                  # data standardized \n",
        "\n",
        " \"\"\"\n",
        " #################### Variables fonctions ####################\n",
        "\n",
        "   \n",
        " #### Function Global Variable  :  ACP_ML(df,features_list,standardization=True) \n",
        " # Can be used Inside (local) & outside the function !\n",
        " #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        " global df_Source             # Source DataFrame  (before feaure Selection)\n",
        " global features              # Selected features\n",
        " global df_Source             # Garder en memoire le non du df initial\n",
        " global df_Source_Name_String # Garder en memoire le non du df initial sous forme de string\n",
        " global data                  # Dataframe for analysis\n",
        " global data_scaled           # Scaled dataframe with sklearn.preprocessing StandardScaler\n",
        " global LabelValues                # Label Columns not fited ! \n",
        " global LabelColName           # Label Columns name as a string\n",
        " global data_unscaled         # data_scaled inverse_transformed \n",
        " #############################################################\n",
        "\n",
        " #### Selection des Features\n",
        " df_Source = df.copy()        # Garder en memoire le non du df initial\n",
        " df_Source_Name_String = f\"{get_df_name(df)}\"\n",
        " features = features_list\n",
        " data = df_Source.loc[:,features] ; data\n",
        " print(pcolors.OK +\"Features Selection : \"+pcolors.RESET)\n",
        " print(f\"DataFrame source : {df_Source_Name_String}   (alias df_Source )\")\n",
        " print(f\"                   {len(features)} Seleted features : {features} \")\n",
        " \n",
        " LabelColName = LabelColString  # Label Columns name as a string\n",
        " LabelValues = df_Source[LabelColString] ; print(f\"                   Label : {LabelColString} not scaled not fited\")\n",
        "\n",
        " # Not Selected Features (différence)  betwen two arrays \n",
        " diff = np.setdiff1d(df_Source.columns, features ,assume_unique=False) # https://numpy.org/doc/stable/reference/generated/numpy.setdiff1d.html\n",
        " print(f\"                   Excluded features from analysis : {diff}\")\n",
        "      \n",
        " #### Preproscessing Standard Scalling \n",
        " print(f\"{pcolors.OK}\\nPreprocessing : Standard Scalling {pcolors.RESET}\")\n",
        " from sklearn.preprocessing import StandardScaler  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        " \n",
        " if standardization == False:\n",
        "   print(f\"{pcolors.FAIL}Attention{pcolors.RESET} : StandardScaler(with_std=False) : verifier que toutes les features ont la même unité\")\n",
        "   scaler = StandardScaler(with_std=False)   # Standardization = False  toutes les données ont la même unitée\n",
        " else : \n",
        "  scaler = StandardScaler(with_std=True)  # Standardization = True  standardization pour ramener sur un même eéchalles des unitées différentes\n",
        " \n",
        " print(f\"{pcolors.WARNING}Scaling :{pcolors.RESET} {scaler}\")\n",
        " data_scaled = scaler.fit_transform(data)   # Data  Fit entrainement du mmodèle & Transformed => StandardScal\n",
        "\n",
        " print(f\"  {df_Source_Name_String} {df_Source.shape} scaled into data_scaled {data_scaled.shape}\")\n",
        " \n",
        " print(f\"    {pcolors.WARNING}Globals Variables :{pcolors.RESET}\")\n",
        " print(f\"    {df_Source_Name_String} => df_Source &  df_Source_Name_String :  Source DataFrame  (before feature Selection) \")\n",
        " print(f\"    {LabelColString} {LabelValues[:1]} ... {LabelValues[:-1]}:  Label Columns not fited !  \")\n",
        " print(f\"    features : Selected features {features}\")\n",
        " print(f\"    data : Dataframe with the selected features  \")\n",
        " print(f\"    data_scaled : Scaled dataframe with sklearn.preprocessing StandardScaler  \")\n",
        "\n",
        " \n",
        "\n",
        "# ######### DEBUG test  ###############################################################\n",
        "# iris = px.data.iris()\n",
        "# features_list = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]  \n",
        "# Scaling_ML(iris,features_list,standardization=True)\n",
        "# #####################################################################################"
      ],
      "metadata": {
        "id": "R5FuoOXHTFT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Scaling_MLVO(df,features_list,LabelColString,standardization=True): \n",
        " \"\"\"\n",
        " Machine learning : Preprocessing  StandardScaler \n",
        " \n",
        " (df,features_list,standardization=True)\n",
        "  \n",
        "  df : Source data frame \n",
        "  features_list : list of selected features\n",
        "\n",
        " Output Globals Variable : \n",
        "   global df_Source             # Source DataFrame  (avant feaure Sélection)\n",
        "   global features              # Selected features\n",
        "   global df_Source             # Garder en memoire le non du df initial\n",
        "   global df_Source_Name_String # Garder en memoire le non du df initial sous forme de string\n",
        "   \n",
        "   data                      \n",
        "   ==> data_scaled                  # data standardized \n",
        "\n",
        " \"\"\"\n",
        " #################### Variables fonctions ####################\n",
        "\n",
        "   \n",
        " #### Function Global Variable  :  ACP_ML(df,features_list,standardization=True) \n",
        " # Can be used Inside (local) & outside the function !\n",
        " #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        " global df_Source             # Source DataFrame  (before feaure Selection)\n",
        " global features              # Selected features\n",
        " global df_Source             # Garder en memoire le non du df initial\n",
        " global df_Source_Name_String # Garder en memoire le non du df initial sous forme de string\n",
        " global data                  # Dataframe for analysis\n",
        " global data_scaled           # Scaled dataframe with sklearn.preprocessing StandardScaler\n",
        " global LabelValues                # Label Columns not fited ! \n",
        " global LabelColName           # Label Columns name as a string\n",
        " #############################################################\n",
        "\n",
        " #### Selection des Features\n",
        " df_Source = df.copy()        # Garder en memoire le non du df initial\n",
        " df_Source_Name_String = f\"{get_df_name(df)}\"\n",
        " features = features_list\n",
        " data = df_Source.loc[:,features] ; data\n",
        " print(pcolors.OK +\"Features Selection : \"+pcolors.RESET)\n",
        " print(f\"DataFrame source : {df_Source_Name_String}   (alias df_Source )\")\n",
        " print(f\"                   {len(features)} Seleted features : {features} \")\n",
        " \n",
        " LabelColName = LabelColString \n",
        " LabelValues = df_Source[LabelColString] ; print(f\"                   Label : {LabelColString} not scaled not fited\")\n",
        "\n",
        " # Not Selected Features (différence)  betwen two arrays \n",
        " diff = np.setdiff1d(df_Source.columns, features ,assume_unique=False) # https://numpy.org/doc/stable/reference/generated/numpy.setdiff1d.html\n",
        " print(f\"                   Excluded features from analysis : {diff}\")\n",
        "      \n",
        " #### Preproscessing Standard Scalling \n",
        " print(f\"{pcolors.OK}\\nPreprocessing : Standard Scalling {pcolors.RESET}\")\n",
        " from sklearn.preprocessing import StandardScaler  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        " \n",
        " if standardization == False:\n",
        "   print(f\"{pcolors.FAIL}Attention{pcolors.RESET} : StandardScaler(with_std=False) : verifier que toutes les features ont la même unité\")\n",
        "   scaler = StandardScaler(with_std=False)   # Standardization = False  toutes les données ont la même unitée\n",
        " else : \n",
        "  scaler = StandardScaler(with_std=True)  # Standardization = True  standardization pour ramener sur un même eéchalles des unitées différentes\n",
        " \n",
        " print(f\"{pcolors.WARNING}Scaling :{pcolors.RESET} {scaler}\")\n",
        " data_scaled = scaler.fit_transform(data)   # Data  Fit entrainement du mmodèle & Transformed => StandardScal\n",
        "\n",
        " print(f\"  {df_Source_Name_String} {df_Source.shape} scaled into data_scaled {data_scaled.shape}\")\n",
        " \n",
        " print(f\"    {pcolors.WARNING}Globals Variables :{pcolors.RESET}\")\n",
        " print(f\"    {df_Source_Name_String} => df_Source &  df_Source_Name_String :  Source DataFrame  (before feature Selection) \")\n",
        " print(f\"    {LabelValues} :  Label Columns not fited !  \")\n",
        " print(f\"    features : Selected features {features}\")\n",
        " print(f\"    data : Dataframe with the selected features  \")\n",
        " print(f\"    data_scaled : Scaled dataframe with sklearn.preprocessing StandardScaler  \")\n",
        "\n",
        "# ######### DEBUG test  ###############################################################\n",
        "# iris = px.data.iris()\n",
        "# features_list = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]  \n",
        "# Scaling_ML(iris,features_list,standardization=True)\n",
        "# #####################################################################################"
      ],
      "metadata": {
        "id": "aYMpLcxd0Cdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ClusterKmeanCoudeplot(nbr_Max_Cluster_envisage_Int):\n",
        " # On extrait X de data_scaled\n",
        " X = data_scaled\n",
        " # On peut le transformer en DataFrame : \n",
        " X = pd.DataFrame(X)\n",
        " #return X\n",
        " \n",
        " # Une liste vide pour enregistrer les inerties :  \n",
        " intertia_list = [ ]\n",
        "\n",
        " # Pour chaque nombre de clusters : \n",
        " from sklearn.cluster import KMeans\n",
        " k_max = nbr_Max_Cluster_envisage_Int +1\n",
        " k_list = range(1, k_max)   # Notre liste de nombres de clusters : \n",
        "  \n",
        " print(f\"{pcolors.OK}\\nKmeans test k (nombre de cluster) {pcolors.RESET}\")\n",
        " print(f\" Kmean liste des {len(k_list)} nombre de clusters testés :  {list(k_list)}\")\n",
        "\n",
        " for k in k_list : \n",
        "    \n",
        "    # On instancie un k-means pour k clusters\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "    #print(kmeans)\n",
        "    \n",
        "    # On entraine\n",
        "    kmeans.fit(X)\n",
        "    \n",
        "    # On enregistre l'inertie obtenue : \n",
        "    intertia_list.append(kmeans.inertia_)\n",
        "\n",
        " #print(len(intertia_list))\n",
        " #print(intertia_list)\n",
        "\n",
        " KmeansInteriaCluster = pd.DataFrame({\"n_cluster\":k_list,\"Inertie\":intertia_list}) # DataFrame Inertie en fonction du nombre de cluster\n",
        "\n",
        " fig = px.line(KmeansInteriaCluster,\n",
        "               x= \"n_cluster\", y= \"Inertie\",\n",
        "              labels=KmeansInteriaCluster.columns,              \n",
        "              title='Diagramme du Coude : Inertie en fonction du nombre de clusters Kmean',             \n",
        "              markers= True,  )           \n",
        " fig.update_traces(line_color='red',fillcolor='rgba(255, 0, 0, 0.1)',line_width=3,mode='lines+markers',marker_size=15)\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =\"Kmean_Diagramme_du_Coude_Selection_Nb_Cluster.html\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        "\n",
        " return KmeansInteriaCluster\n",
        "#ClusterKmeanCoudeplot(15)"
      ],
      "metadata": {
        "id": "IbXA1m0DTHpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KmeanCluster(nbr_de_Cluster):\n",
        " global data_KmeanClustered          # Dataframe  data with cluster label\n",
        " global centroid                     # data frame out of ndarray of shape (n_clusters, n_features) with centroid coordinates\n",
        "\n",
        " # On extrait X de data_scaled\n",
        " X = data_scaled\n",
        " # On peut le transformer en DataFrame : \n",
        " X = pd.DataFrame(X)\n",
        " X.columns = features\n",
        " print(f\"X.shape : {X.shape}\")\n",
        " \n",
        " \n",
        " from sklearn.cluster import KMeans\n",
        "\n",
        " # On instancie un k-means pour k clusters\n",
        " kmeans = KMeans(n_clusters = nbr_de_Cluster,verbose = 0)\n",
        " print(f\"{pcolors.OK}\\nKmeans {pcolors.RESET} : {kmeans}\")\n",
        "\n",
        " # On entraine\n",
        " kmeans.fit(X)\n",
        " print(f\"kmeans.labels_ : {len(kmeans.labels_)}\")   #Kmeans labels  variable catégorielle ordinale  {kmeans.labels_}\n",
        " print(f\"Fit kmeans.n_iter_ : {kmeans.n_iter_}\")\n",
        "\n",
        " # On entraine\n",
        " #kmeans.fit(X)\n",
        " #print(f\"kmeans.labels_ : {len(kmeans.labels_)}\")   #Kmeans lables  variable catégorielle ordinale\n",
        " print(f\"kmeans.n_iter_ : {kmeans.n_iter_}\")\n",
        "\n",
        " # Creation d'un data frame avec les cluster labelisés \n",
        " data_KmeanClustered = X\n",
        "\n",
        " Liste_Cluster_Name = [f'Cluster_{i}' for i in range(0, nbr_de_Cluster)]  # liste des cluster  Variable Texte\n",
        " Liste_Cluster_Name = [f'{i}' for i in range(0, nbr_de_Cluster)]                      # liste des cluster  Variable int\n",
        " #print(Liste_Cluster_Name)\n",
        " dicoClusters = {i:j for i,j in enumerate(list(Liste_Cluster_Name))}\n",
        " print(dicoClusters)\n",
        " #labels = [dicoCluster[i] for i in kmeans.labels_]\n",
        " data_KmeanClustered[\"Cluster\"] = kmeans.labels_\n",
        " #data_KmeanClustered[\"Cluster\"] = [dicoClusters[i] for i in kmeans.labels_]\n",
        "\n",
        "\n",
        " data_KmeanClustered[LabelColString] = LabelValues # Ajout colonne label \n",
        "\n",
        " # Creation dataframe Centroides \n",
        " centroid = pd.DataFrame(kmeans.cluster_centers_,columns=features) #; print(centroid)\n",
        " centroid[\"Cluster\"] = Liste_Cluster_Name\n",
        " ColMove(centroid ,\"Cluster\",0)\n",
        " print(\"DataFrame centroïdes : centroid\")\n",
        "\n",
        "\n",
        " #DEBUG \n",
        " #inversed = scaler.inverse_transform(scaled)\n",
        "\n",
        "\n",
        " # Scatter matric plotly \n",
        " # https://plotly.com/python/splom/\n",
        " sns.pairplot(data_KmeanClustered, hue=\"Cluster\")\n",
        "\n",
        " fig = px.scatter_matrix(data_KmeanClustered,\n",
        "    dimensions= data_KmeanClustered.columns[:-1],  \n",
        "    color=\"Cluster\",\n",
        "    title = f\"Matrices des {nbr_de_Cluster} clusters identifiés par Kmean dans {df_Source_Name_String}\",\n",
        "    height = 800)\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =f\"Kmean_matrice_{nbr_de_Cluster}_clusters.html\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        " \n",
        " print(f\"{pcolors.OK}Dataframe avec les cluster :{pcolors.RESET} data_KmeanClustered\")\n",
        " return data_KmeanClustered\n",
        "#KmeanCluster(3)\n"
      ],
      "metadata": {
        "id": "6koYBPK3E6MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KmeanClusterVO(nbr_de_Cluster):\n",
        " global data_KmeanClustered          # Dataframe  data with cluster label\n",
        " global centroid                     # data frame out of ndarray of shape (n_clusters, n_features) with centroid coordinates\n",
        "\n",
        " # On extrait X de data_scaled\n",
        " X = data_scaled\n",
        " # On peut le transformer en DataFrame : \n",
        " X = pd.DataFrame(X)\n",
        " X.columns = features\n",
        " #return X\n",
        " \n",
        " \n",
        " from sklearn.cluster import KMeans\n",
        "\n",
        " # On instancie un k-means pour k clusters\n",
        " kmeans = KMeans(n_clusters = nbr_de_Cluster,verbose = 0)\n",
        " print(f\"{pcolors.OK}\\nKmeans {pcolors.RESET} : {kmeans}\")\n",
        "\n",
        " # On entraine\n",
        " kmeans.fit(X)\n",
        " #print(f\"kmeans.labels_ : {len(kmeans.labels_)}\")   #Kmeans lables  variable catégorielle ordinale\n",
        " print(f\"kmeans.n_iter_ : {kmeans.n_iter_}\")\n",
        "\n",
        " # Creation d'un data frame avec les cluster labelisés \n",
        " data_KmeanClustered = X\n",
        "\n",
        " #Liste_Cluster_Name = [f'Cluster_{i}' for i in range(0, nbr_de_Cluster)]  # liste des cluster  Variable Texte\n",
        " Liste_Cluster_Name = [range(0, nbr_de_Cluster)]                         # liste des cluster  Variable int\n",
        " #print(Liste_Cluster_Name)\n",
        " dicoClusters = {i:j for i,j in enumerate(list(Liste_Cluster_Name))}\n",
        " print(dicoClusters)\n",
        " #labels = [dicoCluster[i] for i in kmeans.labels_]\n",
        " #data_KmeanClustered[\"Cluster\"] = kmeans.labels_\n",
        " data_KmeanClustered[\"Cluster\"] = [dicoClusters[i] for i in kmeans.labels_]\n",
        "\n",
        "\n",
        " data_KmeanClustered[LabelColString] = LabelValues # Ajout colonne label \n",
        "\n",
        " # Creation dataframe Centroides \n",
        " centroid = pd.DataFrame(kmeans.cluster_centers_,columns=features) #; print(centroid)\n",
        " centroid[\"Cluster\"] = Liste_Cluster_Name\n",
        " ColMove(centroid ,\"Cluster\",0)\n",
        " print(\"DataFrame centroïdes : centroid\")\n",
        "\n",
        "\n",
        " #DEBUG \n",
        " #inversed = scaler.inverse_transform(scaled)\n",
        "\n",
        "\n",
        " # Scatter matric plotly \n",
        " # https://plotly.com/python/splom/\n",
        " sns.pairplot(data_KmeanClustered, hue=\"Cluster\")\n",
        "\n",
        " fig = px.scatter_matrix(data_KmeanClustered,\n",
        "    dimensions= data_KmeanClustered.columns[:-1],  \n",
        "    color=\"Cluster\",\n",
        "    title = f\"Matrices des {nbr_de_Cluster} clusters identifiés par Kmean dans {df_Source_Name_String}\",\n",
        "    height = 800)\n",
        " fig.show()\n",
        "\n",
        " # Export Plotly graph to .html\n",
        " file_name =f\"Kmean_matrice_{nbr_de_Cluster}_clusters.html\"\n",
        " col=\"_\"\n",
        " #ExportPlotly_dfPATH(df_Source,col,fig,file_name)\n",
        " ExportPlotly_df_Name_StringPATH(df_Source_Name_String,col,fig,file_name)\n",
        " \n",
        " print(f\"{pcolors.OK}Dataframe avec les cluster :{pcolors.RESET} data_KmeanClustered\")\n",
        " return data_KmeanClustered\n",
        "#KmeanCluster(3)"
      ],
      "metadata": {
        "id": "PA4yHg_yTVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML ACP Classification hierarchy"
      ],
      "metadata": {
        "id": "NjwZl64ZT7DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HierarchyClusterScipy(df,features_list,standardization=True):\n",
        " \n",
        " # On extrait X de data_scaled\n",
        " X = Scaling_ML(df,features_list,standardization=True)\n",
        " # On peut le transformer en DataFrame : \n",
        " X = pd.DataFrame(X)\n",
        " X.columns = features\n",
        " #return X\n",
        "\n",
        " #Calcul distance Z  avec librairie scipy.cluster.hierarchy \n",
        " # https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n",
        " # https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage \n",
        " from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        " Z = linkage(X, method=\"ward\")   # methode de Ward \n",
        "\n",
        " data_Clustered_Hierarchy = pd.DataFrame(Z)\n",
        " data_Clustered_Hierarchy.columns = features\n",
        "\n",
        " # Dendrogram Plotly \n",
        " # https://plotly.com/python/dendrogram/\n",
        " #https://plotly.github.io/plotly.py-docs/generated/plotly.figure_factory.create_dendrogram.html\n",
        " from plotly.figure_factory import create_dendrogram\n",
        "\n",
        " import plotly.figure_factory as ff\n",
        "\n",
        " fig = ff.create_dendrogram(Z,                 # X ((ndarray)) – Matrix of observations as array of arrays\n",
        "                           color_threshold=200,     #Value at which the separation of clusters will be made\n",
        "                           orientation ='bottom',  # \"top\", \"right\", \"bottom\", or \"left\"\n",
        "                           )\n",
        " # custum axes\n",
        " fig.update_layout(\n",
        "    xaxis_title=\"Nombre de points par neuds\",\n",
        "    yaxis_title=\"Distance (Ward)\",)\n",
        "\n",
        " fig.update_layout( title = \"Dendrograme Clustering hierarchique avec Sipcy\",\n",
        "                  width=800, height=500)\n",
        " fig.show()\n",
        "\n",
        " # Figure MathPlotlib\n",
        "\n",
        " fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        " _ = dendrogram(Z, p=10, truncate_mode=\"lastp\", ax=ax)\n",
        " plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        " plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        " plt.ylabel(\"Distance.\")\n",
        " plt.show()\n",
        " \n",
        " #  Liste_Cluster_Name = [f'Cluster_{i}' for i in range(0, nbr_de_Cluster)]  # liste des cluster \n",
        " #  #print(Liste_Cluster_Name)\n",
        " #  dicoClusters = {i:j for i,j in enumerate(list(Liste_Cluster_Name))}\n",
        " #  print(dicoClusters)\n",
        " #  #labels = [dicoCluster[i] for i in kmeans.labels_]\n",
        " #  #data_KmeanClustered[\"Cluster\"] = kmeans.labels_\n",
        " #  data_KmeanClustered[\"Cluster\"] = [dicoClusters[i] for i in kmeans.labels_]\n",
        "\n",
        " k=3\n",
        " clusters = fcluster(Z, k, criterion='maxclust')\n",
        " print(clusters)\n",
        "\n",
        " #  nbr_de_Cluste = k\n",
        "\n",
        " #  Liste_Cluster_Name = [f'Cluster_{i}' for i in range(0, nbr_de_Cluster)]  # liste des cluster \n",
        " #  #print(Liste_Cluster_Name)\n",
        " #  dicoClusters = {i:j for i,j in enumerate(list(Liste_Cluster_Name))}\n",
        " #  print(dicoClusters)\n",
        " #  #labels = [dicoCluster[i] for i in kmeans.labels_]\n",
        " #  #data_KmeanClustered[\"Cluster\"] = kmeans.labels_\n",
        " #  data_Clustered_Hierarchy[\"Cluster\"] = [dicoClusters[i] for i in kmeans.labels_]\n",
        "\n",
        "\n",
        " return data_Clustered_Hierarchy"
      ],
      "metadata": {
        "id": "RvvB69SdUOBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}